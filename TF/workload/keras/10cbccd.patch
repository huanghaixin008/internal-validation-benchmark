diff --git a/examples/audio/speaker_recognition_using_cnn.py b/examples/audio/speaker_recognition_using_cnn.py
index 9c608ff..9aa52f7 100644
--- a/examples/audio/speaker_recognition_using_cnn.py
+++ b/examples/audio/speaker_recognition_using_cnn.py
@@ -47,9 +47,39 @@ from tensorflow import keras
 from pathlib import Path
 from IPython.display import display, Audio
 
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset/16000_pcm_speeches', help="float32, int8 or bfloat16")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 # Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/download
 # and save it to the 'Downloads' folder in your HOME directory
-DATASET_ROOT = os.path.join(os.path.expanduser("~"), "Downloads/16000_pcm_speeches")
+DATASET_ROOT = args.data_dir
+model_save_filename = "model_speaker_recognition.h5"
+weights_save_filename = "weights_speaker_recognition.h5"
 
 # The folders in which we will put the audio samples and the noise samples
 AUDIO_SUBFOLDER = "audio"
@@ -76,8 +106,8 @@ SAMPLING_RATE = 16000
 #      where prop = sample_amplitude / noise_amplitude
 SCALE = 0.5
 
-BATCH_SIZE = 128
-EPOCHS = 100
+BATCH_SIZE = args.batch_size
+EPOCHS = args.epochs
 
 
 """
@@ -327,7 +357,9 @@ train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch
 )
 
 valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)
-valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)
+valid_ds = valid_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(
+    BATCH_SIZE
+)
 
 
 # Add noise to the training set
@@ -378,7 +410,9 @@ def build_model(input_shape, num_classes):
     x = keras.layers.Dense(256, activation="relu")(x)
     x = keras.layers.Dense(128, activation="relu")(x)
 
-    outputs = keras.layers.Dense(num_classes, activation="softmax", name="output")(x)
+    #outputs = keras.layers.Dense(num_classes, activation="softmax", name="output")(x)
+    x = keras.layers.Dense(num_classes)(x)
+    outputs = keras.layers.Activation("softmax", name="output")(x)
 
     return keras.models.Model(inputs=inputs, outputs=outputs)
 
@@ -395,7 +429,6 @@ model.compile(
 # Add callbacks:
 # 'EarlyStopping' to stop training when the model is not enhancing anymore
 # 'ModelCheckPoint' to always keep the model that has the best val_accuracy
-model_save_filename = "model.h5"
 
 earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
 mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(
@@ -405,19 +438,62 @@ mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(
 """
 ## Training
 """
+if args.train:
+    print("## Training Start:")
+    start_time = time.time()
+    history = model.fit(
+        train_ds,
+        batch_size=args.batch_size,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # validation_data=valid_ds,
+        # callbacks=[earlystopping_cb, mdlcheckpoint_cb],
+    )
+    end_time = time.time()
 
-history = model.fit(
-    train_ds,
-    epochs=EPOCHS,
-    validation_data=valid_ds,
-    callbacks=[earlystopping_cb, mdlcheckpoint_cb],
-)
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+    model.save_weights(weights_save_filename)
 
 """
-## Evaluation
+## Train with FP32, Inference with BF16
 """
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = build_model((SAMPLING_RATE // 2, 1), len(class_names))
+    loaded_model.compile(
+        optimizer="Adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
+    )
+    loaded_model.load_weights(weights_save_filename)
+    model = loaded_model
 
-print(model.evaluate(valid_ds))
+"""
+## Evaluation
+"""
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(valid_ds) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(valid_ds, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 """
 We get ~ 98% validation accuracy.
diff --git a/examples/audio/transformer_asr.py b/examples/audio/transformer_asr.py
index 11a5eb6..235a48c 100644
--- a/examples/audio/transformer_asr.py
+++ b/examples/audio/transformer_asr.py
@@ -36,6 +36,36 @@ import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset/LJSpeech-1.1', help="data_dir")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+weights_save_filename = "weights_transformer_asr.h5"
 
 """
 ## Define the Transformer Input Layer
@@ -215,6 +245,7 @@ class Transformer(keras.Model):
                 TransformerDecoder(num_hid, num_head, num_feed_forward),
             )
 
+        #self.classifier = layers.Dense(num_classes)
         self.classifier = layers.Dense(num_classes)
 
     def decode(self, enc_out, target):
@@ -286,20 +317,19 @@ Note: This requires ~3.6 GB of disk space and
 takes ~5 minutes for the extraction of files.
 """
 
-keras.utils.get_file(
-    os.path.join(os.getcwd(), "data.tar.gz"),
-    "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2",
-    extract=True,
-    archive_format="tar",
-    cache_dir=".",
-)
+#keras.utils.get_file(
+#    os.path.join(os.getcwd(), "data.tar.gz"),
+#    "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2",
+#    extract=True,
+#    archive_format="tar",
+#    cache_dir=".",
+#)
 
 
-saveto = "./datasets/LJSpeech-1.1"
-wavs = glob("{}/**/*.wav".format(saveto), recursive=True)
+wavs = glob("{}/**/*.wav".format(args.data_dir), recursive=True)
 
 id_to_text = {}
-with open(os.path.join(saveto, "metadata.csv"), encoding="utf-8") as f:
+with open(os.path.join(args.data_dir, "metadata.csv"), encoding="utf-8") as f:
     for line in f:
         id = line.strip().split("|")[0]
         text = line.strip().split("|")[2]
@@ -385,7 +415,7 @@ def create_audio_ds(data):
     return audio_ds
 
 
-def create_tf_dataset(data, bs=4):
+def create_tf_dataset(data, bs=args.batch_size):
     audio_ds = create_audio_ds(data)
     text_ds = create_text_ds(data)
     ds = tf.data.Dataset.zip((audio_ds, text_ds))
@@ -398,8 +428,8 @@ def create_tf_dataset(data, bs=4):
 split = int(len(data) * 0.99)
 train_data = data[:split]
 test_data = data[split:]
-ds = create_tf_dataset(train_data, bs=64)
-val_ds = create_tf_dataset(test_data, bs=4)
+ds = create_tf_dataset(train_data, bs=args.batch_size)
+val_ds = create_tf_dataset(test_data, bs=args.batch_size)
 
 """
 ## Callbacks to display predictions
@@ -456,6 +486,7 @@ class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):
         warmup_epochs=15,
         decay_epochs=85,
         steps_per_epoch=203,
+        #steps_per_epoch=10,
     ):
         super().__init__()
         self.init_lr = init_lr
@@ -521,7 +552,75 @@ learning_rate = CustomSchedule(
 optimizer = keras.optimizers.Adam(learning_rate)
 model.compile(optimizer=optimizer, loss=loss_fn)
 
-history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)
+if args.train:
+    print("## Start to fit")
+    start_time = time.time()
+    history = model.fit(
+        ds,
+        # validation_data=val_ds,
+        # callbacks=[display_cb],
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+    model.save_weights(weights_save_filename)
+
+"""
+## Train with FP32, Inference with BF16
+"""
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = Transformer(
+        num_hid=200,
+        num_head=2,
+        num_feed_forward=400,
+        target_maxlen=max_target_len,
+        num_layers_enc=4,
+        num_layers_dec=1,
+        num_classes=34,
+    )
+    loaded_model.compile(optimizer=optimizer, loss=loss_fn)
+    loaded_history = loaded_model.fit(
+        ds,
+        validation_data=val_ds,
+        callbacks=[display_cb],
+        epochs=args.epochs,
+        batch_size=args.batch_size
+    )
+    loaded_model.load_weights(weights_save_filename)
+    model = loaded_model
+
+"""
+## Evaluation
+"""
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(val_ds) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(val_ds, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 """
 In practice, you should train for around 100 epochs or more.
@@ -535,3 +634,4 @@ target:     <under the entry for may one, nineteen sixty,>
 prediction: <under the introus for may monee, nin the sixty,>
 ```
 """
+
diff --git a/examples/generative/cyclegan.py b/examples/generative/cyclegan.py
index 517d95b..8b2d230 100644
--- a/examples/generative/cyclegan.py
+++ b/examples/generative/cyclegan.py
@@ -35,6 +35,36 @@ from tensorflow.keras import layers
 import tensorflow_addons as tfa
 import tensorflow_datasets as tfds
 
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset', help="data_dir")
+parser.add_argument("--model_dir", type=str, default='./model', help="model_dir")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 tfds.disable_progress_bar()
 autotune = tf.data.experimental.AUTOTUNE
 
@@ -48,7 +78,7 @@ dataset.
 """
 
 # Load the horse-zebra dataset using tensorflow-datasets.
-dataset, _ = tfds.load("cycle_gan/horse2zebra", with_info=True, as_supervised=True)
+dataset, _ = tfds.load("cycle_gan/horse2zebra", data_dir=args.data_dir, with_info=True, as_supervised=True)
 train_horses, train_zebras = dataset["trainA"], dataset["trainB"]
 test_horses, test_zebras = dataset["testA"], dataset["testB"]
 
@@ -62,7 +92,7 @@ kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)
 gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)
 
 buffer_size = 256
-batch_size = 1
+batch_size = args.batch_size
 
 
 def normalize_img(img):
@@ -606,11 +636,24 @@ model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
 
 # Here we will train the model for just one epoch as each epoch takes around
 # 7 minutes on a single P100 backed machine.
-cycle_gan_model.fit(
-    tf.data.Dataset.zip((train_horses, train_zebras)),
-    epochs=1,
-    callbacks=[plotter, model_checkpoint_callback],
-)
+if args.train:
+    print("## Start to fit")
+    start_time = time.time()
+    cycle_gan_model.fit(
+        tf.data.Dataset.zip((train_horses, train_zebras)),
+        epochs=args.epochs,
+        # callbacks=[plotter],
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    print("## Fit finished")
 
 """
 Test the performance of the model.
@@ -625,28 +668,55 @@ Test the performance of the model.
 curl -LO https://github.com/AakashKumarNain/CycleGAN_TF2/releases/download/v1.0/saved_checkpoints.zip
 unzip -qq saved_checkpoints.zip
 """
-
-
-# Load the checkpoints
-weight_file = "./saved_checkpoints/cyclegan_checkpoints.090"
-cycle_gan_model.load_weights(weight_file).expect_partial()
-print("Weights loaded successfully")
-
-_, ax = plt.subplots(4, 2, figsize=(10, 15))
-for i, img in enumerate(test_horses.take(4)):
-    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()
-    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
-    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
-
-    ax[i, 0].imshow(img)
-    ax[i, 1].imshow(prediction)
-    ax[i, 0].set_title("Input image")
-    ax[i, 0].set_title("Input image")
-    ax[i, 1].set_title("Translated image")
-    ax[i, 0].axis("off")
-    ax[i, 1].axis("off")
-
-    prediction = keras.preprocessing.image.array_to_img(prediction)
-    prediction.save("predicted_img_{i}.png".format(i=i))
-plt.tight_layout()
-plt.show()
+if args.evaluate:
+    # Load the checkpoints
+    print("## Load weights")
+    weight_file = args.model_dir + "/cyclegan/saved_checkpoints/cyclegan_checkpoints.090"
+    cycle_gan_model.load_weights(weight_file).expect_partial()
+    print("Weights loaded successfully")
+
+    _, ax = plt.subplots(4, 2, figsize=(10, 15))
+
+    print("## Evaluate Start:")
+    # iter num and warmup
+    if args.num_iter != 0 and args.num_iter < len(test_horses.take(4)):
+        num_iter = args.num_iter
+    else:
+        num_iter = len(test_horses.take(4))
+    num_warmup = int(num_iter / 3)
+
+    total_time = 0
+    total_sample = 0
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    for i, img in enumerate(test_horses.take(4)):
+        if i >= num_iter:
+            break
+        start_time = time.time()
+        prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()
+        end_time = time.time()
+        if i >= num_warmup:
+            total_time += end_time - start_time
+            total_sample += batch_size
+        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
+        img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)
+
+        ax[i, 0].imshow(img)
+        ax[i, 1].imshow(prediction)
+        ax[i, 0].set_title("Input image")
+        ax[i, 0].set_title("Input image")
+        ax[i, 1].set_title("Translated image")
+        ax[i, 0].axis("off")
+        ax[i, 1].axis("off")
+
+        prediction = keras.preprocessing.image.array_to_img(prediction)
+        prediction.save("predicted_img_{i}.png".format(i=i))
+    if args.profile:
+        tf.profiler.experimental.stop()
+    plt.tight_layout()
+    plt.show()
+
+    latency = total_time / total_sample
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/generative/neural_style_transfer.py b/examples/generative/neural_style_transfer.py
index 56e4146..85e8f0b 100644
--- a/examples/generative/neural_style_transfer.py
+++ b/examples/generative/neural_style_transfer.py
@@ -42,10 +42,41 @@ import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras.applications import vgg19
 
-base_image_path = keras.utils.get_file("paris.jpg", "https://i.imgur.com/F28w3Ac.jpg")
-style_reference_image_path = keras.utils.get_file(
-    "starry_night.jpg", "https://i.imgur.com/9ooB60I.jpg"
-)
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 200")
+parser.add_argument("-w","--num_warmup", type=int, default=20, 
+            help="numbers of inference iteration, default is 20")
+parser.add_argument("--data_dir", type=str, default='./dataset/neural_style_transfer', help="float32, int8 or bfloat16")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+#
+base_image_path = args.data_dir + "/paris.jpg"
+style_reference_image_path = args.data_dir + "/starry_night.jpg"
 result_prefix = "paris_generated"
 
 # Weights of the different loss components
@@ -58,6 +89,7 @@ width, height = keras.preprocessing.image.load_img(base_image_path).size
 img_nrows = 400
 img_ncols = int(width * img_nrows / height)
 
+
 """
 ## Let's take a look at our base (content) image and our style reference image
 """
@@ -252,20 +284,59 @@ base_image = preprocess_image(base_image_path)
 style_reference_image = preprocess_image(style_reference_image_path)
 combination_image = tf.Variable(preprocess_image(base_image_path))
 
-iterations = 4000
-for i in range(1, iterations + 1):
-    loss, grads = compute_loss_and_grads(
-        combination_image, base_image, style_reference_image
-    )
-    optimizer.apply_gradients([(grads, combination_image)])
-    if i % 100 == 0:
-        print("Iteration %d: loss=%.2f" % (i, loss))
-        img = deprocess_image(combination_image.numpy())
-        fname = result_prefix + "_at_iteration_%d.png" % i
-        keras.preprocessing.image.save_img(fname, img)
-
-"""
-After 4000 iterations, you get the following result:
-"""
+if args.train:
+    total_time = 0.0
+    total_sample = 0
+    for i in range(args.num_iter):
+        start_time = time.time()
+        loss, grads = compute_loss_and_grads(
+            combination_image, base_image, style_reference_image
+        )
+        optimizer.apply_gradients([(grads, combination_image)])
+        end_time = time.time()
+        if i >= args.num_warmup:
+            total_time += (end_time - start_time)
+            total_sample += 1
+
+        if i % 1 == 0:
+            print("Iteration %d, loss=%.2f, time: %.6f sec" % (i, loss, end_time - start_time))
+            img = deprocess_image(combination_image.numpy())
+            fname = result_prefix + "_at_iteration_%d.png" % i
+            keras.preprocessing.image.save_img(fname, img)
+
+    """
+    After 4000 iterations, you get the following result:
+    """
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    # display(Image(result_prefix + "_at_iteration_" + str(args.num_iter) + ".png"))
+
+model.compile(
+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
+    optimizer=keras.optimizers.RMSprop(),
+    metrics=[keras.metrics.SparseCategoricalAccuracy()],
+)
 
-display(Image(result_prefix + "_at_iteration_4000.png"))
+# Test the model
+if args.evaluate:
+    print("## Evaluate Start:")
+    input_tensor = tf.concat(
+        [base_image, style_reference_image, combination_image], axis=0
+    )
+    #
+    num_iter = int(len(input_tensor) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(input_tensor, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/graph/node2vec_movielens.py b/examples/graph/node2vec_movielens.py
index b805c05..3ab9464 100644
--- a/examples/graph/node2vec_movielens.py
+++ b/examples/graph/node2vec_movielens.py
@@ -64,6 +64,35 @@ from tensorflow import keras
 from tensorflow.keras import layers
 import matplotlib.pyplot as plt
 
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset/ml-latest-small', help="float32, int8 or bfloat16")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Download the MovieLens dataset and prepare the data
 
@@ -75,22 +104,18 @@ three data files: `users.csv`, `movies.csv`, and `ratings.csv`. In this example,
 we will only need the `movies.dat`, and `ratings.dat` data files.
 """
 
-urlretrieve(
-    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip", "movielens.zip"
-)
-ZipFile("movielens.zip", "r").extractall()
 
 """
 Then, we load the data into a Pandas DataFrame and perform some basic preprocessing.
 """
 
 # Load movies to a DataFrame.
-movies = pd.read_csv("ml-latest-small/movies.csv")
+movies = pd.read_csv(args.data_dir + "/movies.csv")
 # Create a `movieId` string.
 movies["movieId"] = movies["movieId"].apply(lambda x: f"movie_{x}")
 
 # Load ratings to a DataFrame.
-ratings = pd.read_csv("ml-latest-small/ratings.csv")
+ratings = pd.read_csv(args.data_dir + "/ratings.csv")
 # Convert the `ratings` to floating point
 ratings["rating"] = ratings["rating"].apply(lambda x: float(x))
 # Create the `movie_id` string.
@@ -392,7 +417,7 @@ print(f"Weights shape: {weights.shape}")
 ### Convert the data into `tf.data.Dataset` objects
 """
 
-batch_size = 1024
+batch_size = args.batch_size
 
 
 def create_dataset(targets, contexts, labels, weights, batch_size):
@@ -429,7 +454,6 @@ Our skip-gram is a simple binary classification model that works as follows:
 
 learning_rate = 0.001
 embedding_dim = 50
-num_epochs = 10
 
 """
 ### Implement the model
@@ -477,111 +501,75 @@ model.compile(
     loss=keras.losses.BinaryCrossentropy(from_logits=True),
 )
 
-"""
-Let's plot the model.
-"""
-
-keras.utils.plot_model(
-    model, show_shapes=True, show_dtype=True, show_layer_names=True,
-)
-
 """
 Now we train the model on the `dataset`.
 """
-
-history = model.fit(dataset, epochs=num_epochs)
-
-"""
-Finally we plot the learning history.
-"""
-
-plt.plot(history.history["loss"])
-plt.ylabel("loss")
-plt.xlabel("epoch")
-plt.show()
-
-"""
-## Analyze the learnt embeddings.
-"""
-
-movie_embeddings = model.get_layer("item_embeddings").get_weights()[0]
-print("Embeddings shape:", movie_embeddings.shape)
-
-"""
-### Find related movies
-
-Define a list with some movies called `query_movies`.
-"""
-
-query_movies = [
-    "Matrix, The (1999)",
-    "Star Wars: Episode IV - A New Hope (1977)",
-    "Lion King, The (1994)",
-    "Terminator 2: Judgment Day (1991)",
-    "Godfather, The (1972)",
-]
-
-"""
-Get the embeddings of the movies in `query_movies`.
-"""
-
-query_embeddings = []
-
-for movie_title in query_movies:
-    movieId = get_movie_id_by_title(movie_title)
-    token_id = vocabulary_lookup[movieId]
-    movie_embedding = movie_embeddings[token_id]
-    query_embeddings.append(movie_embedding)
-
-query_embeddings = np.array(query_embeddings)
-
-"""
-Compute the [consine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the embeddings of `query_movies`
-and all the other movies, then pick the top k for each.
-"""
-
-similarities = tf.linalg.matmul(
-    tf.math.l2_normalize(query_embeddings),
-    tf.math.l2_normalize(movie_embeddings),
-    transpose_b=True,
-)
-
-_, indices = tf.math.top_k(similarities, k=5)
-indices = indices.numpy().tolist()
-
-"""
-Display the top related movies in `query_movies`.
-"""
-
-for idx, title in enumerate(query_movies):
-    print(title)
-    print("".rjust(len(title), "-"))
-    similar_tokens = indices[idx]
-    for token in similar_tokens:
-        similar_movieId = vocabulary[token]
-        similar_title = get_movie_title_by_id(similar_movieId)
-        print(f"- {similar_title}")
-    print()
-
-"""
-### Visualize the embeddings using the Embedding Projector
-"""
-
-import io
-
-out_v = io.open("embeddings.tsv", "w", encoding="utf-8")
-out_m = io.open("metadata.tsv", "w", encoding="utf-8")
-
-for idx, movie_id in enumerate(vocabulary[1:]):
-    movie_title = list(movies[movies.movieId == movie_id].title)[0]
-    vector = movie_embeddings[idx]
-    out_v.write("\t".join([str(x) for x in vector]) + "\n")
-    out_m.write(movie_title + "\n")
-
-out_v.close()
-out_m.close()
-
-"""
-Download the `embeddings.tsv` and `metadata.tsv` to analyze the obtained embeddings
-in the [Embedding Projector](https://projector.tensorflow.org/).
-"""
+if args.train:
+    print("## Training Start:")
+    start_time = time.time()
+    history = model.fit(
+        dataset,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+    """
+    ## Analyze the learnt embeddings.
+    """
+
+    movie_embeddings = model.get_layer("item_embeddings").get_weights()[0]
+    print("Embeddings shape:", movie_embeddings.shape)
+
+    """
+    ### Find related movies
+
+    Define a list with some movies called `query_movies`.
+    """
+
+    query_movies = [
+        "Matrix, The (1999)",
+        "Star Wars: Episode IV - A New Hope (1977)",
+        "Lion King, The (1994)",
+        "Terminator 2: Judgment Day (1991)",
+        "Godfather, The (1972)",
+    ]
+
+    """
+    Get the embeddings of the movies in `query_movies`.
+    """
+
+    query_embeddings = []
+
+    for movie_title in query_movies:
+        movieId = get_movie_id_by_title(movie_title)
+        token_id = vocabulary_lookup[movieId]
+        movie_embedding = movie_embeddings[token_id]
+        query_embeddings.append(movie_embedding)
+
+    query_embeddings = np.array(query_embeddings)
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(dataset) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(dataset, steps=num_iter, batch_size=batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/keras_recipes/antirectifier.py b/examples/keras_recipes/antirectifier.py
index 4798e2d..82355d8 100644
--- a/examples/keras_recipes/antirectifier.py
+++ b/examples/keras_recipes/antirectifier.py
@@ -25,6 +25,35 @@ import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## The Antirectifier layer
 """
@@ -64,9 +93,7 @@ class Antirectifier(layers.Layer):
 """
 
 # Training parameters
-batch_size = 128
 num_classes = 10
-epochs = 20
 
 # The data, split between train and test sets
 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
@@ -81,17 +108,21 @@ print(x_train.shape[0], "train samples")
 print(x_test.shape[0], "test samples")
 
 # Build the model
-model = keras.Sequential(
-    [
-        keras.Input(shape=(784,)),
-        layers.Dense(256),
-        Antirectifier(),
-        layers.Dense(256),
-        Antirectifier(),
-        layers.Dropout(0.5),
-        layers.Dense(10),
-    ]
-)
+def get_model():
+    model = keras.Sequential(
+        [
+            keras.Input(shape=(784,)),
+            layers.Dense(256),
+            Antirectifier(),
+            layers.Dense(256),
+            Antirectifier(),
+            layers.Dropout(0.5),
+            layers.Dense(10),
+        ]
+    )
+    return model
+
+model = get_model()
 
 # Compile the model
 model.compile(
@@ -101,7 +132,53 @@ model.compile(
 )
 
 # Train the model
-model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.15)
+if args.train:
+    start_time = time.time()
+    model.fit(
+        x_train,
+        y_train,
+        batch_size=args.batch_size,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # validation_split=0.15
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+ 
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = get_model()
+    loaded_model.compile(
+        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
+        optimizer=keras.optimizers.RMSprop(),
+        metrics=[keras.metrics.SparseCategoricalAccuracy()],
+    )
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
 
 # Test the model
-model.evaluate(x_test, y_test)
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_test) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(x_test, y_test, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/keras_recipes/bayesian_neural_networks.py b/examples/keras_recipes/bayesian_neural_networks.py
index a7107c1..165c45f 100644
--- a/examples/keras_recipes/bayesian_neural_networks.py
+++ b/examples/keras_recipes/bayesian_neural_networks.py
@@ -49,7 +49,6 @@ pip install tensorflow-datasets
 """
 ## Setup
 """
-
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
@@ -57,6 +56,35 @@ from tensorflow.keras import layers
 import tensorflow_datasets as tfds
 import tensorflow_probability as tfp
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Create training and evaluation datasets
 
@@ -100,17 +128,45 @@ def run_experiment(model, loss, train_dataset, test_dataset):
         loss=loss,
         metrics=[keras.metrics.RootMeanSquaredError()],
     )
+    total_sample = 0
+    if args.train:
+        print("Start training the model...")
+        start_time = time.time()
+        model.fit(
+            train_dataset,
+            epochs=args.epochs,
+            # validation_data=test_dataset,
+            steps_per_epoch=args.num_iter,
+            batch_size=args.batch_size
+        )
+        end_time = time.time()
+        print("Model training finished.")
+        total_sample = args.num_iter * args.batch_size * args.epochs
+    # _, rmse = model.evaluate(train_dataset, verbose=0)
+    # print(f"Train RMSE: {round(rmse, 3)}")
+    if args.evaluate:
+        num_iter = args.num_iter if args.num_iter > 0 else None
+        print("Evaluating model performance...")
+        start_time = time.time()
+        if args.profile:
+            tf.profiler.experimental.start(timeline_dir)
+        _, rmse = model.evaluate(
+            test_dataset,
+            verbose=0,
+            steps=num_iter,
+            batch_size=args.batch_size
+        )
+        if args.profile:
+            tf.profiler.experimental.stop()
+        end_time = time.time()
+        print(f"Test RMSE: {round(rmse, 3)}")
+        total_sample = (args.num_iter if args.num_iter > 0 else len(test_dataset)) * args.batch_size
 
-    print("Start training the model...")
-    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)
-    print("Model training finished.")
-    _, rmse = model.evaluate(train_dataset, verbose=0)
-    print(f"Train RMSE: {round(rmse, 3)}")
-
-    print("Evaluating model performance...")
-    _, rmse = model.evaluate(test_dataset, verbose=0)
-    print(f"Test RMSE: {round(rmse, 3)}")
-
+    total_time = end_time - start_time
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 """
 ## Create model inputs
@@ -169,7 +225,7 @@ the examples, respectively.
 """
 
 dataset_size = 4898
-batch_size = 256
+batch_size = args.batch_size
 train_size = int(dataset_size * 0.85)
 train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)
 
@@ -181,7 +237,7 @@ as the loss function.
 num_epochs = 100
 mse_loss = keras.losses.MeanSquaredError()
 baseline_model = create_baseline_model()
-run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)
+# run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)
 
 """
 We take a sample from the test set use the model to obtain predictions for them.
@@ -292,7 +348,7 @@ train_sample_size = int(train_size * 0.3)
 small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)
 
 bnn_model_small = create_bnn_model(train_sample_size)
-run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)
+# run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)
 
 """
 Since we have trained a BNN model, the model produces a different output each time
@@ -332,7 +388,7 @@ compute_predictions(bnn_model_small)
 
 num_epochs = 500
 bnn_model_full = create_bnn_model(train_size)
-run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)
+# run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)
 
 compute_predictions(bnn_model_full)
 
diff --git a/examples/keras_recipes/quasi_svm.py b/examples/keras_recipes/quasi_svm.py
index 17e6d40..6061683 100644
--- a/examples/keras_recipes/quasi_svm.py
+++ b/examples/keras_recipes/quasi_svm.py
@@ -27,29 +27,61 @@ In our case, we approximate SVM using a hinge loss.
 """
 ## Setup
 """
-
+import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras.layers.experimental import RandomFourierFeatures
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Build the model
 """
-
-model = keras.Sequential(
-    [
-        keras.Input(shape=(784,)),
-        RandomFourierFeatures(
-            output_dim=4096, scale=10.0, kernel_initializer="gaussian"
-        ),
-        layers.Dense(units=10),
-    ]
-)
-model.compile(
-    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
-    loss=keras.losses.hinge,
-    metrics=[keras.metrics.CategoricalAccuracy(name="acc")],
-)
+def get_model():
+    model = keras.Sequential(
+        [
+            keras.Input(shape=(784,)),
+            RandomFourierFeatures(
+                output_dim=4096, scale=10.0, kernel_initializer="gaussian"
+            ),
+            layers.Dense(units=10),
+        ]
+    )
+    return model
+
+def compile_model(model):
+    model.compile(
+        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
+        loss=keras.losses.hinge,
+        metrics=[keras.metrics.CategoricalAccuracy(name="acc")],
+    )
 
 """
 ## Prepare the data
@@ -70,7 +102,55 @@ y_test = keras.utils.to_categorical(y_test)
 ## Train the model
 """
 
-model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2)
+model = get_model()
+compile_model(model)
+if args.train:
+    print("## Training Start:")
+    start_time = time.time()
+    model.fit(
+        x_train,
+        y_train,
+        epochs=args.epochs,
+        batch_size=args.batch_size,
+        steps_per_epoch=args.num_iter,
+        # validation_split=0.2
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = get_model()
+    compile_model(loaded_model)
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+ 
+# Test the model
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_test) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(x_test, y_test, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 """
 I can't say that it works well or that it is indeed a good idea, but you can probably
diff --git a/examples/nlp/lstm_seq2seq.py b/examples/nlp/lstm_seq2seq.py
index 7128bfe..21f76b3 100644
--- a/examples/nlp/lstm_seq2seq.py
+++ b/examples/nlp/lstm_seq2seq.py
@@ -49,6 +49,34 @@ import numpy as np
 import tensorflow as tf
 from tensorflow import keras
 
+import os
+import sys
+import time
+import argparse
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+
 """
 ## Download the data
 """
@@ -62,12 +90,12 @@ from tensorflow import keras
 ## Configuration
 """
 
-batch_size = 64  # Batch size for training.
-epochs = 100  # Number of epochs to train for.
+batch_size = args.batch_size  # Batch size for training.
+epochs = args.epochs  # Number of epochs to train for.
 latent_dim = 256  # Latent dimensionality of the encoding space.
 num_samples = 10000  # Number of samples to train on.
 # Path to the data txt file on disk.
-data_path = "fra.txt"
+data_path = "dataset/lstm_seq2seq/fra.txt"
 
 """
 ## Prepare the data
@@ -168,15 +196,27 @@ model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
 model.compile(
     optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"]
 )
-model.fit(
-    [encoder_input_data, decoder_input_data],
-    decoder_target_data,
-    batch_size=batch_size,
-    epochs=epochs,
-    validation_split=0.2,
-)
-# Save model
-model.save("s2s")
+if args.train:
+    print("---- Start to train")
+    start_time = time.time()
+    model.fit(
+        [encoder_input_data, decoder_input_data],
+        decoder_target_data,
+        batch_size=batch_size,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # validation_split=0.2,
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    # Save model
+    # model.save("s2s")
 
 """
 ## Run inference (sampling)
@@ -190,7 +230,7 @@ Output will be the next target token.
 
 # Define sampling models
 # Restore the model and construct the encoder and decoder.
-model = keras.models.load_model("s2s")
+model = keras.models.load_model("dataset/lstm_seq2seq/s2s")
 
 encoder_inputs = model.input[0]  # input_1
 encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1
@@ -256,12 +296,30 @@ def decode_sequence(input_seq):
 """
 You can now generate decoded sentences as such:
 """
-
-for seq_index in range(20):
-    # Take one sequence (part of the training set)
-    # for trying out decoding.
-    input_seq = encoder_input_data[seq_index : seq_index + 1]
-    decoded_sentence = decode_sequence(input_seq)
-    print("-")
-    print("Input sentence:", input_texts[seq_index])
-    print("Decoded sentence:", decoded_sentence)
+if args.evaluate:
+    total_sample = 0
+    total_time = 0.0
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    for seq_index in range(args.num_iter):
+        # Take one sequence (part of the training set)
+        # for trying out decoding.
+        input_seq = encoder_input_data[seq_index : seq_index + 1]
+
+        start_time = time.time()
+        decoded_sentence = decode_sequence(input_seq)
+        end_time = time.time()
+        print("Iteration: {}, inference time: {} sec".format(seq_index, (end_time - start_time)), flush=True)
+        total_time += (end_time - start_time)
+        total_sample += args.batch_size
+
+        print("-")
+        print("Input sentence:", input_texts[seq_index])
+        print("Decoded sentence:", decoded_sentence)
+    if args.profile:
+        tf.profiler.experimental.stop()
+
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/nlp/masked_language_modeling.py b/examples/nlp/masked_language_modeling.py
index 9b4ebc0..7c9b519 100644
--- a/examples/nlp/masked_language_modeling.py
+++ b/examples/nlp/masked_language_modeling.py
@@ -52,6 +52,34 @@ import glob
 import re
 from pprint import pprint
 
+import os
+import sys
+import time
+import argparse
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+
 """
 ## Set-up Configuration
 """
@@ -60,7 +88,7 @@ from pprint import pprint
 @dataclass
 class Config:
     MAX_LEN = 256
-    BATCH_SIZE = 32
+    BATCH_SIZE = args.batch_size
     LR = 0.001
     VOCAB_SIZE = 30000
     EMBED_DIM = 128
@@ -94,9 +122,9 @@ def get_text_list_from_files(files):
 
 def get_data_from_text_files(folder_name):
 
-    pos_files = glob.glob("aclImdb/" + folder_name + "/pos/*.txt")
+    pos_files = glob.glob("dataset/masked_language_modeling/aclImdb/" + folder_name + "/pos/*.txt")
     pos_texts = get_text_list_from_files(pos_files)
-    neg_files = glob.glob("aclImdb/" + folder_name + "/neg/*.txt")
+    neg_files = glob.glob("dataset/masked_language_modeling/aclImdb/" + folder_name + "/neg/*.txt")
     neg_texts = get_text_list_from_files(neg_files)
     df = pd.DataFrame(
         {
@@ -418,34 +446,38 @@ generator_callback = MaskedTextGenerator(sample_tokens.numpy())
 bert_masked_model = create_masked_language_bert_model()
 bert_masked_model.summary()
 
+
 """
 ## Train and Save
 """
-
-bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])
-bert_masked_model.save("bert_mlm_imdb.h5")
+if args.train:
+    print("---- Start to train")
+    start_time = time.time()
+    bert_masked_model.fit(
+        mlm_ds,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # callbacks=[generator_callback]
+    )
+    end_time = time.time()
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    # model save
+    # bert_masked_model.save("dataset/masked_language_modeling/bert_mlm_imdb")
 
 """
 ## Fine-tune a sentiment classification model
 
 We will fine-tune our self-supervised model on a downstream task of sentiment classification.
 To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the
-pretrained BERT features.
 
+pretrained BERT features.
 """
 
-# Load pretrained bert model
-mlm_model = keras.models.load_model(
-    "bert_mlm_imdb.h5", custom_objects={"MaskedLanguageModel": MaskedLanguageModel}
-)
-pretrained_bert_model = tf.keras.Model(
-    mlm_model.input, mlm_model.get_layer("encoder_0/ffn_layernormalization").output
-)
-
-# Freeze it
-pretrained_bert_model.trainable = False
-
-
 def create_classifier_bert_model():
     inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)
     sequence_output = pretrained_bert_model(inputs)
@@ -459,24 +491,38 @@ def create_classifier_bert_model():
     )
     return classifer_model
 
+if args.train and args.evaluate:
+    # Load pretrained bert model
+    mlm_model = keras.models.load_model(
+        "dataset/masked_language_modeling/bert_mlm_imdb", custom_objects={"MaskedLanguageModel": MaskedLanguageModel}
+    )
+    pretrained_bert_model = tf.keras.Model(
+        mlm_model.input, mlm_model.get_layer("encoder_0/ffn_layernormalization").output
+    )
 
-classifer_model = create_classifier_bert_model()
-classifer_model.summary()
+    # Freeze it
+    pretrained_bert_model.trainable = False
 
-# Train the classifier with frozen BERT stage
-classifer_model.fit(
-    train_classifier_ds, epochs=5, validation_data=test_classifier_ds,
-)
 
-# Unfreeze the BERT model for fine-tuning
-pretrained_bert_model.trainable = True
-optimizer = keras.optimizers.Adam()
-classifer_model.compile(
-    optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy"]
-)
-classifer_model.fit(
-    train_classifier_ds, epochs=5, validation_data=test_classifier_ds,
-)
+    classifer_model = create_classifier_bert_model()
+    classifer_model.summary()
+
+    # Train the classifier with frozen BERT stage
+    classifer_model.fit(
+        train_classifier_ds, epochs=args.epochs, validation_data=test_classifier_ds,
+    )
+
+    # Unfreeze the BERT model for fine-tuning
+    pretrained_bert_model.trainable = True
+    optimizer = keras.optimizers.Adam()
+    classifer_model.compile(
+        optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy"]
+    )
+    classifer_model.fit(
+        train_classifier_ds, epochs=args.epochs, validation_data=test_classifier_ds,
+    )
+    # save model
+    # classifer_model.save("dataset/masked_language_modeling/classifer_model")
 
 """
 ## Create an end-to-end model and evaluate it
@@ -488,7 +534,6 @@ the `TextVectorization` layer, and let's evaluate. Our model will accept raw str
 as input.
 """
 
-
 def get_end_to_end(model):
     inputs_string = keras.Input(shape=(1,), dtype="string")
     indices = vectorize_layer(inputs_string)
@@ -500,6 +545,23 @@ def get_end_to_end(model):
     )
     return end_to_end_model
 
-
-end_to_end_classification_model = get_end_to_end(classifer_model)
-end_to_end_classification_model.evaluate(test_raw_classifier_ds)
+if args.evaluate:
+    print("## Evaluate Start:")
+    classifer_model = keras.models.load_model("dataset/masked_language_modeling/classifer_model")
+    end_to_end_classification_model = get_end_to_end(classifer_model)
+
+    num_iter = len(test_raw_classifier_ds)
+    if args.num_iter > 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    end_to_end_classification_model.evaluate(
+            test_raw_classifier_ds, steps=num_iter, batch_size=config.BATCH_SIZE)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * config.BATCH_SIZE) * 1000
+    throughput = (num_iter * config.BATCH_SIZE) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/nlp/nl_image_search.py b/examples/nlp/nl_image_search.py
index 2fd715d..4d68e43 100644
--- a/examples/nlp/nl_image_search.py
+++ b/examples/nlp/nl_image_search.py
@@ -32,7 +32,6 @@ pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons
 ## Setup
 """
 
-import os
 import collections
 import json
 import numpy as np
@@ -46,6 +45,34 @@ import matplotlib.pyplot as plt
 import matplotlib.image as mpimg
 from tqdm import tqdm
 
+import os
+import sys
+import time
+import argparse
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+
 # Suppressing tf.hub warnings
 tf.get_logger().setLevel("ERROR")
 
@@ -67,7 +94,7 @@ one with images, and the other—with associated image captions.
 Note that the compressed images folder is 13GB in size.
 """
 
-root_dir = "datasets"
+root_dir = "dataset/nl_image_search/"
 annotations_dir = os.path.join(root_dir, "annotations")
 images_dir = os.path.join(root_dir, "train2014")
 tfrecords_dir = os.path.join(root_dir, "tfrecords")
@@ -396,8 +423,8 @@ In this experiment, we freeze the base encoders for text and images, and make on
 the projection head trainable.
 """
 
-num_epochs = 5  # In practice, train for at least 30 epochs
-batch_size = 256
+num_epochs = args.epochs  # In practice, train for at least 30 epochs
+batch_size = args.batch_size
 
 vision_encoder = create_vision_encoder(
     num_projection_layers=1, projection_dims=256, dropout_rate=0.1
@@ -430,27 +457,43 @@ reduce_lr = keras.callbacks.ReduceLROnPlateau(
 early_stopping = tf.keras.callbacks.EarlyStopping(
     monitor="val_loss", patience=5, restore_best_weights=True
 )
-history = dual_encoder.fit(
-    train_dataset,
-    epochs=num_epochs,
-    validation_data=valid_dataset,
-    callbacks=[reduce_lr, early_stopping],
-)
-print("Training completed. Saving vision and text encoders...")
-vision_encoder.save("vision_encoder")
-text_encoder.save("text_encoder")
-print("Models are saved.")
+if args.train:
+    print("---- Start to train")
+    start_time = time.time()
+    history = dual_encoder.fit(
+        train_dataset,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # validation_data=valid_dataset,
+        # callbacks=[reduce_lr, early_stopping],
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+    print("Training completed. Saving vision and text encoders...")
+    vision_encoder.save(root_dir + "/vision_encoder")
+    text_encoder.save(root_dir + "/text_encoder")
+    print("Models are saved.")
 
-"""
-Plotting the training loss:
-"""
 
-plt.plot(history.history["loss"])
-plt.plot(history.history["val_loss"])
-plt.ylabel("Loss")
-plt.xlabel("Epoch")
-plt.legend(["train", "valid"], loc="upper right")
-plt.show()
+    """
+    Plotting the training loss:
+    """
+
+    plt.plot(history.history["loss"])
+    plt.plot(history.history["val_loss"])
+    plt.ylabel("Loss")
+    plt.xlabel("Epoch")
+    plt.legend(["train", "valid"], loc="upper right")
+    plt.show()
+
+    exit(0)
 
 """
 ## Search for images using natural language queries
@@ -477,8 +520,8 @@ such as [Apache Spark](https://spark.apache.org) or [Apache Beam](https://beam.a
 Generating the image embeddings may take several minutes.
 """
 print("Loading vision and text encoders...")
-vision_encoder = keras.models.load_model("vision_encoder")
-text_encoder = keras.models.load_model("text_encoder")
+vision_encoder = keras.models.load_model(root_dir + "/vision_encoder")
+text_encoder = keras.models.load_model(root_dir + "/text_encoder")
 print("Models are loaded.")
 
 
@@ -488,11 +531,28 @@ def read_image(image_path):
 
 
 print(f"Generating embeddings for {len(image_paths)} images...")
-image_embeddings = vision_encoder.predict(
-    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),
-    verbose=1,
-)
-print(f"Image embeddings shape: {image_embeddings.shape}.")
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = len(tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size))
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    image_embeddings = vision_encoder.predict(
+        tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),
+        verbose=1,
+        steps=num_iter,
+    )
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    print(f"Image embeddings shape: {image_embeddings.shape}.")
+    exit(0)
 
 """
 ### Retrieve relevant images
diff --git a/examples/nlp/semantic_similarity_with_bert.py b/examples/nlp/semantic_similarity_with_bert.py
index 37e72a2..645b175 100644
--- a/examples/nlp/semantic_similarity_with_bert.py
+++ b/examples/nlp/semantic_similarity_with_bert.py
@@ -31,13 +31,45 @@ import pandas as pd
 import tensorflow as tf
 import transformers
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("--max_length", type=int, default=128, help="max_length")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset/SNLI_Corpus', help="float32, int8 or bfloat16")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Configuration
 """
 
-max_length = 128  # Maximum length of input sentence to the model.
-batch_size = 32
-epochs = 2
+max_length = args.max_length  # Maximum length of input sentence to the model.
+batch_size = args.batch_size
+epochs = args.epochs
+steps_per_epoch = 10
 
 # Labels in our dataset.
 labels = ["contradiction", "entailment", "neutral"]
@@ -51,9 +83,9 @@ curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.
 tar -xvzf data.tar.gz
 """
 # There are more than 550k samples in total; we will use 100k for this example.
-train_df = pd.read_csv("SNLI_Corpus/snli_1.0_train.csv", nrows=100000)
-valid_df = pd.read_csv("SNLI_Corpus/snli_1.0_dev.csv")
-test_df = pd.read_csv("SNLI_Corpus/snli_1.0_test.csv")
+train_df = pd.read_csv(args.data_dir + "/snli_1.0_train.csv", nrows=100000)
+valid_df = pd.read_csv(args.data_dir + "/snli_1.0_dev.csv")
+test_df = pd.read_csv(args.data_dir + "/snli_1.0_test.csv")
 
 # Shape of the data
 print(f"Total train samples : {train_df.shape[0]}")
@@ -223,7 +255,11 @@ class BertSemanticDataGenerator(tf.keras.utils.Sequence):
 # Create the model under a distribution strategy scope.
 strategy = tf.distribute.MirroredStrategy()
 
-with strategy.scope():
+# Loading pretrained BERT model.
+bert_model = transformers.TFBertModel.from_pretrained("bert-base-uncased")
+
+#with strategy.scope():
+def build_model():
     # Encoded token ids from BERT tokenizer.
     input_ids = tf.keras.layers.Input(
         shape=(max_length,), dtype=tf.int32, name="input_ids"
@@ -236,14 +272,20 @@ with strategy.scope():
     token_type_ids = tf.keras.layers.Input(
         shape=(max_length,), dtype=tf.int32, name="token_type_ids"
     )
-    # Loading pretrained BERT model.
-    bert_model = transformers.TFBertModel.from_pretrained("bert-base-uncased")
+
+    if (len(sys.argv) > 1 and sys.argv[1] == 'bf16') :
+        from tensorflow.keras import mixed_precision
+        policy = mixed_precision.Policy('mixed_bfloat16')
+        mixed_precision.set_global_policy(policy)
+
     # Freeze the BERT model to reuse the pretrained features without modifying them.
     bert_model.trainable = False
 
-    sequence_output, pooled_output = bert_model(
+    bert_output = bert_model(
         input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids
     )
+    sequence_output = bert_output.last_hidden_state
+    pooled_output = bert_output.pooler_output
     # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.
     bi_lstm = tf.keras.layers.Bidirectional(
         tf.keras.layers.LSTM(64, return_sequences=True)
@@ -263,10 +305,11 @@ with strategy.scope():
         loss="categorical_crossentropy",
         metrics=["acc"],
     )
+    return model
 
 
 print(f"Strategy: {strategy}")
-model.summary()
+model = build_model()
 
 """
 Create train and validation data generators
@@ -290,45 +333,59 @@ valid_data = BertSemanticDataGenerator(
 Training is done only for the top layers to perform "feature extraction",
 which will allow the model to use the representations of the pretrained model.
 """
-history = model.fit(
-    train_data,
-    validation_data=valid_data,
-    epochs=epochs,
-    use_multiprocessing=True,
-    workers=-1,
-)
-
-"""
-## Fine-tuning
+if args.train:
+    print("## Start to train the model for top layers to perform feature extraction")
+    start_time = time.time()
+    history = model.fit(
+        train_data,
+        # validation_data=valid_data,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        use_multiprocessing=True,
+        # workers=-1,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    sys.exit()
+    """
+    ## Fine-tuning
 
-This step must only be performed after the feature extraction model has
-been trained to convergence on the new data.
+    This step must only be performed after the feature extraction model has
+    been trained to convergence on the new data.
 
-This is an optional last step where `bert_model` is unfreezed and retrained
-with a very low learning rate. This can deliver meaningful improvement by
-incrementally adapting the pretrained features to the new data.
-"""
+    This is an optional last step where `bert_model` is unfreezed and retrained
+    with a very low learning rate. This can deliver meaningful improvement by
+    incrementally adapting the pretrained features to the new data.
+    """
 
-# Unfreeze the bert_model.
-bert_model.trainable = True
-# Recompile the model to make the change effective.
-model.compile(
-    optimizer=tf.keras.optimizers.Adam(1e-5),
-    loss="categorical_crossentropy",
-    metrics=["accuracy"],
-)
-model.summary()
+    # Unfreeze the bert_model.
+    bert_model.trainable = True
+    # Recompile the model to make the change effective.
+    model.compile(
+        optimizer=tf.keras.optimizers.Adam(1e-5),
+        loss="categorical_crossentropy",
+        metrics=["accuracy"],
+    )
 
-"""
-## Train the entire model end-to-end
-"""
-history = model.fit(
-    train_data,
-    validation_data=valid_data,
-    epochs=epochs,
-    use_multiprocessing=True,
-    workers=-1,
-)
+    """
+    ## Train the entire model end-to-end
+    """
+    print("## Start to train the entire model")
+    history = model.fit(
+        train_data,
+        validation_data=valid_data,
+        epochs=args.epochs,
+        use_multiprocessing=True,
+        workers=-1,
+        steps_per_epoch=steps_per_epoch,
+    )
 
 """
 ## Evaluate model on the test set
@@ -339,42 +396,34 @@ test_data = BertSemanticDataGenerator(
     batch_size=batch_size,
     shuffle=False,
 )
-model.evaluate(test_data, verbose=1)
 
-"""
-## Inference on custom sentences
-"""
-
-
-def check_similarity(sentence1, sentence2):
-    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])
-    test_data = BertSemanticDataGenerator(
-        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = build_model()
+    loaded_model.compile(
+        optimizer=tf.keras.optimizers.Adam(1e-5),
+        loss="categorical_crossentropy",
+        metrics=["accuracy"],
     )
-
-    proba = model.predict(test_data)[0]
-    idx = np.argmax(proba)
-    proba = f"{proba[idx]: .2f}%"
-    pred = labels[idx]
-    return pred, proba
-
-
-"""
-Check results on some example sentence pairs.
-"""
-sentence1 = "Two women are observing something together."
-sentence2 = "Two women are standing with their eyes closed."
-check_similarity(sentence1, sentence2)
-"""
-Check results on some example sentence pairs.
-"""
-sentence1 = "A smiling costumed woman is holding an umbrella"
-sentence2 = "A happy woman in a fairy costume holds an umbrella"
-check_similarity(sentence1, sentence2)
-
-"""
-Check results on some example sentence pairs
-"""
-sentence1 = "A soccer game with multiple males playing"
-sentence2 = "Some men are playing a sport"
-check_similarity(sentence1, sentence2)
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(test_data) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(test_data, steps=num_iter, batch_size=batch_size, verbose=1)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/nlp/text_extraction_with_bert.py b/examples/nlp/text_extraction_with_bert.py
index 0143994..4bab3e7 100644
--- a/examples/nlp/text_extraction_with_bert.py
+++ b/examples/nlp/text_extraction_with_bert.py
@@ -46,8 +46,37 @@ from tensorflow import keras
 from tensorflow.keras import layers
 from tokenizers import BertWordPieceTokenizer
 from transformers import BertTokenizer, TFBertModel, BertConfig
+import time
+import sys
+import argparse
 
-max_len = 384
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("--max_len", type=int, default=384, help="max_len")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--model_dir", type=str, default='./model', help="data_dir")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+max_len = args.max_len
 configuration = BertConfig()  # default parameters and configuration for BERT
 
 """
@@ -55,13 +84,13 @@ configuration = BertConfig()  # default parameters and configuration for BERT
 """
 # Save the slow pretrained tokenizer
 slow_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
-save_path = "bert_base_uncased/"
+save_path = args.model_dir + "/bert-base-uncased/"
 if not os.path.exists(save_path):
     os.makedirs(save_path)
 slow_tokenizer.save_pretrained(save_path)
 
 # Load the fast tokenizer from saved file
-tokenizer = BertWordPieceTokenizer("bert_base_uncased/vocab.txt", lowercase=True)
+tokenizer = BertWordPieceTokenizer(save_path + "vocab.txt", lowercase=True)
 
 """
 ## Load the data
@@ -210,7 +239,7 @@ x_train, y_train = create_inputs_targets(train_squad_examples)
 print(f"{len(train_squad_examples)} training points created.")
 
 eval_squad_examples = create_squad_examples(raw_eval_data)
-x_eval, y_eval = create_inputs_targets(eval_squad_examples)
+x_eval, y_eval = create_inputs_targets(eval_squad_examples[:1000])
 print(f"{len(eval_squad_examples)} evaluation points created.")
 
 """
@@ -218,9 +247,13 @@ Create the Question-Answering Model using BERT and Functional API
 """
 
 
+encoder = TFBertModel.from_pretrained("bert-base-uncased")
 def create_model():
     ## BERT encoder
-    encoder = TFBertModel.from_pretrained("bert-base-uncased")
+    if (len(sys.argv) > 1 and sys.argv[1] == 'bf16') :
+        from tensorflow.keras import mixed_precision
+        policy = mixed_precision.Policy('mixed_bfloat16')
+        mixed_precision.set_global_policy(policy)
 
     ## QA Model
     input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
@@ -249,23 +282,7 @@ def create_model():
     return model
 
 
-"""
-This code should preferably be run on Google Colab TPU runtime.
-With Colab TPUs, each epoch will take 5-6 minutes.
-"""
-use_tpu = True
-if use_tpu:
-    # Create distribution strategy
-    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
-    tf.config.experimental_connect_to_cluster(tpu)
-    tf.tpu.experimental.initialize_tpu_system(tpu)
-    strategy = tf.distribute.experimental.TPUStrategy(tpu)
-
-    # Create model
-    with strategy.scope():
-        model = create_model()
-else:
-    model = create_model()
+model = create_model()
 
 model.summary()
 
@@ -336,12 +353,48 @@ class ExactMatch(keras.callbacks.Callback):
 """
 ## Train and Evaluate
 """
-exact_match_callback = ExactMatch(x_eval, y_eval)
-model.fit(
-    x_train,
-    y_train,
-    epochs=1,  # For demonstration, 3 epochs are recommended
-    verbose=2,
-    batch_size=64,
-    callbacks=[exact_match_callback],
-)
+if args.train:
+    exact_match_callback = ExactMatch(x_eval, y_eval)
+    start_time = time.time()
+    model.fit(
+        x_train,
+        y_train,
+        epochs=args.epochs,  # For demonstration, 3 epochs are recommended
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size,
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = create_model()
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_eval) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(x_eval, y_eval, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/rl/ddpg_pendulum.py b/examples/rl/ddpg_pendulum.py
index e3c4bc8..84585ae 100644
--- a/examples/rl/ddpg_pendulum.py
+++ b/examples/rl/ddpg_pendulum.py
@@ -65,12 +65,41 @@ from tensorflow.keras import layers
 import numpy as np
 import matplotlib.pyplot as plt
 
+import os
+import time
+import sys
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 We use [OpenAIGym](http://gym.openai.com/docs) to create the environment.
 We will use the `upper_bound` parameter to scale our actions later.
 """
 
-problem = "Pendulum-v0"
+problem = "Pendulum-v1"
 env = gym.make(problem)
 
 num_states = env.observation_space.shape[0]
@@ -322,13 +351,13 @@ actor_lr = 0.001
 critic_optimizer = tf.keras.optimizers.Adam(critic_lr)
 actor_optimizer = tf.keras.optimizers.Adam(actor_lr)
 
-total_episodes = 100
+total_episodes = args.epochs
 # Discount factor for future rewards
 gamma = 0.99
 # Used to update target networks
 tau = 0.005
 
-buffer = Buffer(50000, 64)
+buffer = Buffer(50000, args.batch_size)
 
 """
 Now we implement our main training loop, and iterate over episodes.
@@ -346,7 +375,10 @@ for ep in range(total_episodes):
 
     prev_state = env.reset()
     episodic_reward = 0
-
+    eval_total_time = 0.0
+    total_time = 0.0
+    total_sample = 0
+    start_time = time.time()
     while True:
         # Uncomment this to see the Actor in action
         # But not in a python notebook.
@@ -360,16 +392,33 @@ for ep in range(total_episodes):
 
         buffer.record((prev_state, action, reward, state))
         episodic_reward += reward
-
-        buffer.learn()
+        if args.evaluate:
+            eval_start_time = time.time()
+            buffer.learn()
+            eval_end_time = time.time()
+            eval_total_time += (eval_end_time - eval_start_time)
+        else:
+            buffer.learn()
         update_target(target_actor.variables, actor_model.variables, tau)
         update_target(target_critic.variables, critic_model.variables, tau)
 
+        total_sample += args.batch_size
         # End this episode when `done` is True
         if done:
             break
 
         prev_state = state
+    end_time = time.time()
+    total_time = end_time - start_time
+
+    if args.train:
+        latency = total_time / total_sample * 1000
+        throughput = total_sample / total_time
+    else:
+        latency = eval_total_time / total_sample * 1000
+        throughput = total_sample / eval_total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
     ep_reward_list.append(episodic_reward)
 
@@ -405,6 +454,7 @@ critic_model.save_weights("pendulum_critic.h5")
 target_actor.save_weights("pendulum_target_actor.h5")
 target_critic.save_weights("pendulum_target_critic.h5")
 
+
 """
 Before Training:
 
diff --git a/examples/rl/deep_q_network_breakout.py b/examples/rl/deep_q_network_breakout.py
index 97e4d50..bc8f30a 100644
--- a/examples/rl/deep_q_network_breakout.py
+++ b/examples/rl/deep_q_network_breakout.py
@@ -51,6 +51,35 @@ import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=2, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 # Configuration paramaters for the whole setup
 seed = 42
 gamma = 0.99  # Discount factor for past rewards
@@ -60,8 +89,8 @@ epsilon_max = 1.0  # Maximum epsilon greedy parameter
 epsilon_interval = (
     epsilon_max - epsilon_min
 )  # Rate at which to reduce chance of random action being taken
-batch_size = 32  # Size of batch taken from replay buffer
-max_steps_per_episode = 10000
+batch_size = args.batch_size  # Size of batch taken from replay buffer
+max_steps_per_episode = 500
 
 # Use the Baseline Atari environment because of Deepmind helper functions
 env = make_atari("BreakoutNoFrameskip-v4")
@@ -111,154 +140,177 @@ model_target = create_q_model()
 """
 ## Train
 """
-# In the Deepmind paper they use RMSProp however then Adam optimizer
-# improves training time
-optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)
-
-# Experience replay buffers
-action_history = []
-state_history = []
-state_next_history = []
-rewards_history = []
-done_history = []
-episode_reward_history = []
-running_reward = 0
-episode_count = 0
-frame_count = 0
-# Number of frames to take random action and observe output
-epsilon_random_frames = 50000
-# Number of frames for exploration
-epsilon_greedy_frames = 1000000.0
-# Maximum replay length
-# Note: The Deepmind paper suggests 1000000 however this causes memory issues
-max_memory_length = 100000
-# Train the model after 4 actions
-update_after_actions = 4
-# How often to update the target network
-update_target_network = 10000
-# Using huber loss for stability
-loss_function = keras.losses.Huber()
-
-while True:  # Run until solved
-    state = np.array(env.reset())
-    episode_reward = 0
-
-    for timestep in range(1, max_steps_per_episode):
-        # env.render(); Adding this line would show the attempts
-        # of the agent in a pop up window.
-        frame_count += 1
-
-        # Use epsilon-greedy for exploration
-        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:
-            # Take random action
-            action = np.random.choice(num_actions)
-        else:
-            # Predict action Q-values
-            # From environment state
-            state_tensor = tf.convert_to_tensor(state)
-            state_tensor = tf.expand_dims(state_tensor, 0)
-            action_probs = model(state_tensor, training=False)
-            # Take best action
-            action = tf.argmax(action_probs[0]).numpy()
-
-        # Decay probability of taking random action
-        epsilon -= epsilon_interval / epsilon_greedy_frames
-        epsilon = max(epsilon, epsilon_min)
-
-        # Apply the sampled action in our environment
-        state_next, reward, done, _ = env.step(action)
-        state_next = np.array(state_next)
-
-        episode_reward += reward
-
-        # Save actions and states in replay buffer
-        action_history.append(action)
-        state_history.append(state)
-        state_next_history.append(state_next)
-        done_history.append(done)
-        rewards_history.append(reward)
-        state = state_next
-
-        # Update every fourth frame and once batch size is over 32
-        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:
-
-            # Get indices of samples for replay buffers
-            indices = np.random.choice(range(len(done_history)), size=batch_size)
-
-            # Using list comprehension to sample from replay buffer
-            state_sample = np.array([state_history[i] for i in indices])
-            state_next_sample = np.array([state_next_history[i] for i in indices])
-            rewards_sample = [rewards_history[i] for i in indices]
-            action_sample = [action_history[i] for i in indices]
-            done_sample = tf.convert_to_tensor(
-                [float(done_history[i]) for i in indices]
-            )
-
-            # Build the updated Q-values for the sampled future states
-            # Use the target model for stability
-            future_rewards = model_target.predict(state_next_sample)
-            # Q value = reward + discount factor * expected future reward
-            updated_q_values = rewards_sample + gamma * tf.reduce_max(
-                future_rewards, axis=1
-            )
-
-            # If final frame set the last value to -1
-            updated_q_values = updated_q_values * (1 - done_sample) - done_sample
-
-            # Create a mask so we only calculate loss on the updated Q-values
-            masks = tf.one_hot(action_sample, num_actions)
-
-            with tf.GradientTape() as tape:
-                # Train the model on the states and updated Q-values
-                q_values = model(state_sample)
-
-                # Apply the masks to the Q-values to get the Q-value for action taken
-                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)
-                # Calculate loss between new Q-value and old Q-value
-                loss = loss_function(updated_q_values, q_action)
-
-            # Backpropagation
-            grads = tape.gradient(loss, model.trainable_variables)
-            optimizer.apply_gradients(zip(grads, model.trainable_variables))
-
-        if frame_count % update_target_network == 0:
-            # update the the target network with new weights
-            model_target.set_weights(model.get_weights())
-            # Log details
-            template = "running reward: {:.2f} at episode {}, frame count {}"
-            print(template.format(running_reward, episode_count, frame_count))
-
-        # Limit the state and reward history
-        if len(rewards_history) > max_memory_length:
-            del rewards_history[:1]
-            del state_history[:1]
-            del state_next_history[:1]
-            del action_history[:1]
-            del done_history[:1]
-
-        if done:
+if args.train or args.evaluate:
+    # In the Deepmind paper they use RMSProp however then Adam optimizer
+    # improves training time
+    optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)
+
+    # Experience replay buffers
+    action_history = []
+    state_history = []
+    state_next_history = []
+    rewards_history = []
+    done_history = []
+    episode_reward_history = []
+    running_reward = 0
+    episode_count = 0
+    frame_count = 0
+    # Number of frames to take random action and observe output
+    epsilon_random_frames = 50000
+    # Number of frames for exploration
+    epsilon_greedy_frames = 1000000.0
+    # Maximum replay length
+    # Note: The Deepmind paper suggests 1000000 however this causes memory issues
+    max_memory_length = 100000
+    # Train the model after 4 actions
+    update_after_actions = 4
+    # How often to update the target network
+    update_target_network = 10000
+    # Using huber loss for stability
+    loss_function = keras.losses.Huber()
+
+    epochs = args.epochs
+    for epoch in range(1, epochs):
+        print("# Epoch {}/{}".format(epoch, epochs), flush=True)
+        state = np.array(env.reset())
+        episode_reward = 0
+        total_sample = 0
+        start_time = time.time()
+        for timestep in range(1, max_steps_per_episode):
+            # env.render(); Adding this line would show the attempts
+            # of the agent in a pop up window.
+            frame_count += 1
+
+            # Use epsilon-greedy for exploration
+            if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:
+                # Take random action
+                action = np.random.choice(num_actions)
+            else:
+                # Predict action Q-values
+                # From environment state
+                state_tensor = tf.convert_to_tensor(state)
+                state_tensor = tf.expand_dims(state_tensor, 0)
+                action_probs = model(state_tensor, training=False)
+                # Take best action
+                action = tf.argmax(action_probs[0]).numpy()
+
+            # Decay probability of taking random action
+            epsilon -= epsilon_interval / epsilon_greedy_frames
+            epsilon = max(epsilon, epsilon_min)
+
+            # Apply the sampled action in our environment
+            state_next, reward, done, _ = env.step(action)
+            state_next = np.array(state_next)
+
+            episode_reward += reward
+
+            # Save actions and states in replay buffer
+            action_history.append(action)
+            state_history.append(state)
+            state_next_history.append(state_next)
+            done_history.append(done)
+            rewards_history.append(reward)
+            state = state_next
+
+            if args.evaluate:
+                continue
+
+            # Update every fourth frame and once batch size is over 32
+            if frame_count % update_after_actions == 0 and len(done_history) > batch_size:
+
+                # Get indices of samples for replay buffers
+                indices = np.random.choice(range(len(done_history)), size=batch_size)
+
+                # Using list comprehension to sample from replay buffer
+                state_sample = np.array([state_history[i] for i in indices])
+                state_next_sample = np.array([state_next_history[i] for i in indices])
+                rewards_sample = [rewards_history[i] for i in indices]
+                action_sample = [action_history[i] for i in indices]
+                done_sample = tf.convert_to_tensor(
+                    [float(done_history[i]) for i in indices]
+                )
+
+                # Build the updated Q-values for the sampled future states
+                # Use the target model for stability
+                future_rewards = model_target.predict(state_next_sample)
+                # Q value = reward + discount factor * expected future reward
+                updated_q_values = rewards_sample + gamma * tf.reduce_max(
+                    future_rewards, axis=1
+                )
+
+                # If final frame set the last value to -1
+                updated_q_values = updated_q_values * (1 - done_sample) - done_sample
+
+                # Create a mask so we only calculate loss on the updated Q-values
+                masks = tf.one_hot(action_sample, num_actions)
+
+                with tf.GradientTape() as tape:
+                    # Train the model on the states and updated Q-values
+                    q_values = model(state_sample)
+
+                    # Apply the masks to the Q-values to get the Q-value for action taken
+                    q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)
+                    # Calculate loss between new Q-value and old Q-value
+                    loss = loss_function(updated_q_values, q_action)
+
+                # Backpropagation
+                grads = tape.gradient(loss, model.trainable_variables)
+                optimizer.apply_gradients(zip(grads, model.trainable_variables))
+
+            if frame_count % update_target_network == 0:
+                # update the the target network with new weights
+                model_target.set_weights(model.get_weights())
+                # Log details
+                template = "running reward: {:.2f} at episode {}, frame count {}"
+                print(template.format(running_reward, episode_count, frame_count))
+
+            # Limit the state and reward history
+            if len(rewards_history) > max_memory_length:
+                del rewards_history[:1]
+                del state_history[:1]
+                del state_next_history[:1]
+                del action_history[:1]
+                del done_history[:1]
+            total_sample += args.batch_size
+            if done:
+                break
+        end_time = time.time()
+        if args.train:
+            total_time = end_time - start_time
+            latency = total_time / total_sample * 1000
+            throughput = total_sample / total_time
+            print("### Latency:: {:.2f} ms".format(latency))
+            print("### Throughput: {:.3f} samples/s".format(throughput))
+
+        # Update running reward to check condition for solving
+        episode_reward_history.append(episode_reward)
+        if len(episode_reward_history) > 100:
+            del episode_reward_history[:1]
+        running_reward = np.mean(episode_reward_history)
+
+        episode_count += 1
+
+        if running_reward > 40:  # Condition to consider the task solved
+            print("Solved at episode {}!".format(episode_count))
             break
 
-    # Update running reward to check condition for solving
-    episode_reward_history.append(episode_reward)
-    if len(episode_reward_history) > 100:
-        del episode_reward_history[:1]
-    running_reward = np.mean(episode_reward_history)
-
-    episode_count += 1
-
-    if running_reward > 40:  # Condition to consider the task solved
-        print("Solved at episode {}!".format(episode_count))
-        break
-
-"""
-## Visualizations
-Before any training:
-![Imgur](https://i.imgur.com/rRxXF4H.gif)
-
-In early stages of training:
-![Imgur](https://i.imgur.com/X8ghdpL.gif)
-
-In later stages of training:
-![Imgur](https://i.imgur.com/Z1K6qBQ.gif)
-"""
+if args.evaluate:
+    print("## Evaluate Start:")
+    #
+    indices = np.random.choice(range(len(done_history)), size=batch_size)
+    state_next_sample = np.array([state_next_history[i] for i in indices])
+    #
+    num_iter = args.num_iter if args.num_iter > 0 else 500
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    for i in range(num_iter):
+        future_rewards = model_target.predict(state_next_sample, steps=1, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/structured_data/collaborative_filtering_movielens.py b/examples/structured_data/collaborative_filtering_movielens.py
index bc72693..0dc3afb 100644
--- a/examples/structured_data/collaborative_filtering_movielens.py
+++ b/examples/structured_data/collaborative_filtering_movielens.py
@@ -37,7 +37,36 @@ import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 from pathlib import Path
-import matplotlib.pyplot as plt
+
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_file", type=str, default='./dataset/ml-latest-small/ratings.csv', help="dataset")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
 
 """
 ## First, load the data and apply preprocessing
@@ -45,25 +74,7 @@ import matplotlib.pyplot as plt
 
 # Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
 # Use the ratings.csv file
-movielens_data_file_url = (
-    "http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
-)
-movielens_zipped_file = keras.utils.get_file(
-    "ml-latest-small.zip", movielens_data_file_url, extract=False
-)
-keras_datasets_path = Path(movielens_zipped_file).parents[0]
-movielens_dir = keras_datasets_path / "ml-latest-small"
-
-# Only extract the data the first time the script is run.
-if not movielens_dir.exists():
-    with ZipFile(movielens_zipped_file, "r") as zip:
-        # Extract files
-        print("Extracting all the files now...")
-        zip.extractall(path=keras_datasets_path)
-        print("Done!")
-
-ratings_file = movielens_dir / "ratings.csv"
-df = pd.read_csv(ratings_file)
+df = pd.read_csv(args.data_file)
 
 """
 First, need to perform some preprocessing to encode users and movies as integer indices.
@@ -159,68 +170,62 @@ model.compile(
 """
 ## Train the model based on the data split
 """
-history = model.fit(
-    x=x_train,
-    y=y_train,
-    batch_size=64,
-    epochs=5,
-    verbose=1,
-    validation_data=(x_val, y_val),
-)
-
-"""
-## Plot training and validation loss
-"""
-plt.plot(history.history["loss"])
-plt.plot(history.history["val_loss"])
-plt.title("model loss")
-plt.ylabel("loss")
-plt.xlabel("epoch")
-plt.legend(["train", "test"], loc="upper left")
-plt.show()
-
-"""
-## Show top 10 movie recommendations to a user
-"""
-
-movie_df = pd.read_csv(movielens_dir / "movies.csv")
-
-# Let us get a user and see the top recommendations.
-user_id = df.userId.sample(1).iloc[0]
-movies_watched_by_user = df[df.userId == user_id]
-movies_not_watched = movie_df[
-    ~movie_df["movieId"].isin(movies_watched_by_user.movieId.values)
-]["movieId"]
-movies_not_watched = list(
-    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))
-)
-movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]
-user_encoder = user2user_encoded.get(user_id)
-user_movie_array = np.hstack(
-    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
-)
-ratings = model.predict(user_movie_array).flatten()
-top_ratings_indices = ratings.argsort()[-10:][::-1]
-recommended_movie_ids = [
-    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
-]
-
-print("Showing recommendations for user: {}".format(user_id))
-print("====" * 9)
-print("Movies with high ratings from user")
-print("----" * 8)
-top_movies_user = (
-    movies_watched_by_user.sort_values(by="rating", ascending=False)
-    .head(5)
-    .movieId.values
-)
-movie_df_rows = movie_df[movie_df["movieId"].isin(top_movies_user)]
-for row in movie_df_rows.itertuples():
-    print(row.title, ":", row.genres)
-
-print("----" * 8)
-print("Top 10 movie recommendations")
-print("----" * 8)
-recommended_movies = movie_df[movie_df["movieId"].isin(recommended_movie_ids)]
-for row in recommended_movies.itertuples():
-    print(row.title, ":", row.genres)
+if args.train:
+    start_time = time.time()
+    history = model.fit(
+        x=x_train,
+        y=y_train,
+        batch_size=args.batch_size,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        verbose=1,
+        # validation_data=(x_val, y_val),
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)
+    loaded_model.compile(
+        loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.001)
+    )
+    loaded_history = loaded_model.fit(
+        x=x_train,
+        y=y_train,
+        batch_size=args.batch_size,
+        epochs=args.epochs,
+        verbose=1,
+        steps_per_epoch=1,
+    )
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+"""
+## Evaluate the model
+"""
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_val) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(x=x_val, y=y_val, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/structured_data/deep_neural_decision_forests.py b/examples/structured_data/deep_neural_decision_forests.py
index bbfe4ee..2b71266 100644
--- a/examples/structured_data/deep_neural_decision_forests.py
+++ b/examples/structured_data/deep_neural_decision_forests.py
@@ -39,6 +39,35 @@ from tensorflow import keras
 from tensorflow.keras import layers
 import math
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+ 
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Prepare the data
 """
@@ -152,14 +181,14 @@ target_label_lookup = StringLookup(
 )
 
 
-def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
+def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=args.batch_size):
     dataset = tf.data.experimental.make_csv_dataset(
         csv_file_path,
         batch_size=batch_size,
         column_names=CSV_HEADER,
         column_defaults=COLUMN_DEFAULTS,
         label_name=TARGET_FEATURE_NAME,
-        num_epochs=1,
+        num_epochs=args.epochs,
         header=False,
         na_value="?",
         shuffle=shuffle,
@@ -345,64 +374,62 @@ Finally, let's set up the code that will train and evaluate the model.
 """
 
 learning_rate = 0.01
-batch_size = 265
-num_epochs = 10
 hidden_units = [64, 64]
 
-
-def run_experiment(model):
-
+def compile_model(model):
     model.compile(
         optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
         loss=keras.losses.SparseCategoricalCrossentropy(),
         metrics=[keras.metrics.SparseCategoricalAccuracy()],
     )
 
+def train_model(model):
     print("Start training the model...")
     train_dataset = get_dataset_from_csv(
-        train_data_file, shuffle=True, batch_size=batch_size
+        train_data_file,
+        shuffle=True,
+        batch_size=args.batch_size
     )
-
-    model.fit(train_dataset, epochs=num_epochs)
+    start_time = time.time()
+    model.fit(
+        train_dataset,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
     print("Model training finished")
 
+def evaluate_model(model):
     print("Evaluating the model on the test data...")
-    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)
-
-    _, accuracy = model.evaluate(test_dataset)
-    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
+    test_dataset = get_dataset_from_csv(test_data_file, batch_size=args.batch_size)
+
+    print("## Evaluate Start:")
+    num_iter = int(len(test_data) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    _, accuracy = model.evaluate(test_dataset, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 
-"""
-## Experiment 1: train a decision tree model
-
-In this experiment, we train a single neural decision tree model
-where we use all input features.
-"""
-
-num_trees = 10
-depth = 10
-used_features_rate = 1.0
 num_classes = len(TARGET_LABELS)
 
-
-def create_tree_model():
-    inputs = create_model_inputs()
-    features = encode_inputs(inputs)
-    features = layers.BatchNormalization()(features)
-    num_features = features.shape[1]
-
-    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)
-
-    outputs = tree(features)
-    model = keras.Model(inputs=inputs, outputs=outputs)
-    return model
-
-
-tree_model = create_tree_model()
-run_experiment(tree_model)
-
-
 """
 ## Experiment 2: train a forest model
 
@@ -416,7 +443,6 @@ num_trees = 25
 depth = 5
 used_features_rate = 0.5
 
-
 def create_forest_model():
     inputs = create_model_inputs()
     features = encode_inputs(inputs)
@@ -433,5 +459,20 @@ def create_forest_model():
 
 
 forest_model = create_forest_model()
-
-run_experiment(forest_model)
+compile_model(forest_model)
+
+if args.train:
+    train_model(forest_model)
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = create_forest_model()
+    compile_model(loaded_model)
+    loaded_model.set_weights(forest_model.get_weights())
+    forest_model = loaded_model
+
+if args.evaluate:
+    evaluate_model(forest_model)
diff --git a/examples/structured_data/movielens_recommendations_transformers.py b/examples/structured_data/movielens_recommendations_transformers.py
index ee37641..9b68943 100644
--- a/examples/structured_data/movielens_recommendations_transformers.py
+++ b/examples/structured_data/movielens_recommendations_transformers.py
@@ -60,6 +60,33 @@ from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras.layers.experimental.preprocessing import StringLookup
 
+import sys
+import time
+import argparse
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
+
 """
 ## Prepare the data
 
@@ -82,16 +109,21 @@ users = pd.read_csv(
     "ml-1m/users.dat",
     sep="::",
     names=["user_id", "sex", "age_group", "occupation", "zip_code"],
+    encoding='latin-1',
 )
 
 ratings = pd.read_csv(
     "ml-1m/ratings.dat",
     sep="::",
     names=["user_id", "movie_id", "rating", "unix_timestamp"],
+    encoding='latin-1',
 )
 
 movies = pd.read_csv(
-    "ml-1m/movies.dat", sep="::", names=["movie_id", "title", "genres"]
+    "ml-1m/movies.dat",
+    sep="::",
+    names=["movie_id", "title", "genres"],
+    encoding='latin-1',
 )
 
 """
@@ -263,7 +295,7 @@ MOVIE_FEATURES = ["genres"]
 """
 
 
-def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
+def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=args.batch_size):
     def process(features):
         movie_ids_string = features["sequence_movie_ids"]
         sequence_movie_ids = tf.strings.split(movie_ids_string, ",").to_tensor()
@@ -537,17 +569,44 @@ model.compile(
 )
 
 # Read the training data.
-train_dataset = get_dataset_from_csv("train_data.csv", shuffle=True, batch_size=265)
+train_dataset = get_dataset_from_csv("train_data.csv", shuffle=True, batch_size=args.batch_size)
 
 # Fit the model with the training data.
-model.fit(train_dataset, epochs=5)
+if args.train:
+    print("---- Start to train")
+    start_time = time.time()
+    history = model.fit(train_dataset, epochs=args.epochs, steps_per_epoch=args.num_iter)
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.epochs * args.num_iter * args.batch_size
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
 
 # Read the test data.
-test_dataset = get_dataset_from_csv("test_data.csv", batch_size=265)
+test_dataset = get_dataset_from_csv("test_data.csv", batch_size=args.batch_size)
 
 # Evaluate the model on the test data.
-_, rmse = model.evaluate(test_dataset, verbose=0)
-print(f"Test MAE: {round(rmse, 3)}")
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = len(test_dataset)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    _, rmse = model.evaluate(test_dataset, steps=num_iter, verbose=0)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    print(f"Test MAE: {round(rmse, 3)}")
 
 """
 You should achieve a Mean Absolute Error (MAE) at or around 0.7 on the test data.
diff --git a/examples/timeseries/timeseries_anomaly_detection.py b/examples/timeseries/timeseries_anomaly_detection.py
index e2a2541..65d2806 100644
--- a/examples/timeseries/timeseries_anomaly_detection.py
+++ b/examples/timeseries/timeseries_anomaly_detection.py
@@ -16,13 +16,42 @@ autoencoder model to detect anomalies in timeseries data.
 """
 ## Setup
 """
-
+import tensorflow as tf
 import numpy as np
 import pandas as pd
 from tensorflow import keras
 from tensorflow.keras import layers
 from matplotlib import pyplot as plt
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=50, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=0, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Load the data
 
@@ -50,34 +79,6 @@ df_daily_jumpsup = pd.read_csv(
     df_daily_jumpsup_url, parse_dates=True, index_col="timestamp"
 )
 
-"""
-## Quick look at the data
-"""
-
-print(df_small_noise.head())
-
-print(df_daily_jumpsup.head())
-
-"""
-## Visualize the data
-### Timeseries data without anomalies
-
-We will use the following data for training.
-"""
-fig, ax = plt.subplots()
-df_small_noise.plot(legend=False, ax=ax)
-plt.show()
-
-"""
-### Timeseries data with anomalies
-
-We will use the following data for testing and see if the sudden jump up in the
-data is detected as an anomaly.
-"""
-fig, ax = plt.subplots()
-df_daily_jumpsup.plot(legend=False, ax=ax)
-plt.show()
-
 """
 ## Prepare training data
 
@@ -124,27 +125,31 @@ output of the same shape. In this case, `sequence_length` is 288 and
 `num_features` is 1.
 """
 
-model = keras.Sequential(
-    [
-        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),
-        layers.Conv1D(
-            filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
-        ),
-        layers.Dropout(rate=0.2),
-        layers.Conv1D(
-            filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
-        ),
-        layers.Conv1DTranspose(
-            filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
-        ),
-        layers.Dropout(rate=0.2),
-        layers.Conv1DTranspose(
-            filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
-        ),
-        layers.Conv1DTranspose(filters=1, kernel_size=7, padding="same"),
-    ]
-)
-model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
+def create_model():
+    model = keras.Sequential(
+        [
+            layers.Input(shape=(x_train.shape[1], x_train.shape[2])),
+            layers.Conv1D(
+                filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
+            ),
+            layers.Dropout(rate=0.2),
+            layers.Conv1D(
+                filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
+            ),
+            layers.Conv1DTranspose(
+                filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
+            ),
+            layers.Dropout(rate=0.2),
+            layers.Conv1DTranspose(
+                filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
+            ),
+            layers.Conv1DTranspose(filters=1, kernel_size=7, padding="same"),
+        ]
+    )
+    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
+    return model
+
+model = create_model()
 model.summary()
 
 """
@@ -153,136 +158,49 @@ model.summary()
 Please note that we are using `x_train` as both the input and the target
 since this is a reconstruction model.
 """
-
-history = model.fit(
-    x_train,
-    x_train,
-    epochs=50,
-    batch_size=128,
-    validation_split=0.1,
-    callbacks=[
-        keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")
-    ],
-)
-
-"""
-Let's plot training and validation loss to see how the training went.
-"""
-
-plt.plot(history.history["loss"], label="Training Loss")
-plt.plot(history.history["val_loss"], label="Validation Loss")
-plt.legend()
-plt.show()
-
-"""
-## Detecting anomalies
-
-We will detect anomalies by determining how well our model can reconstruct
-the input data.
-
-
-1.   Find MAE loss on training samples.
-2.   Find max MAE loss value. This is the worst our model has performed trying
-to reconstruct a sample. We will make this the `threshold` for anomaly
-detection.
-3.   If the reconstruction loss for a sample is greater than this `threshold`
-value then we can infer that the model is seeing a pattern that it isn't
-familiar with. We will label this sample as an `anomaly`.
-
-
-"""
-
-# Get train MAE loss.
-x_train_pred = model.predict(x_train)
-train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)
-
-plt.hist(train_mae_loss, bins=50)
-plt.xlabel("Train MAE loss")
-plt.ylabel("No of samples")
-plt.show()
-
-# Get reconstruction loss threshold.
-threshold = np.max(train_mae_loss)
-print("Reconstruction error threshold: ", threshold)
-
-"""
-### Compare recontruction
-
-Just for fun, let's see how our model has recontructed the first sample.
-This is the 288 timesteps from day 1 of our training dataset.
-"""
-
-# Checking how the first sequence is learnt
-plt.plot(x_train[0])
-plt.plot(x_train_pred[0])
-plt.show()
-
-"""
-### Prepare test data
-"""
-
-
-df_test_value = (df_daily_jumpsup - training_mean) / training_std
-fig, ax = plt.subplots()
-df_test_value.plot(legend=False, ax=ax)
-plt.show()
-
-# Create sequences from test values.
-x_test = create_sequences(df_test_value.values)
-print("Test input shape: ", x_test.shape)
-
-# Get test MAE loss.
-x_test_pred = model.predict(x_test)
-test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)
-test_mae_loss = test_mae_loss.reshape((-1))
-
-plt.hist(test_mae_loss, bins=50)
-plt.xlabel("test MAE loss")
-plt.ylabel("No of samples")
-plt.show()
-
-# Detect all the samples which are anomalies.
-anomalies = test_mae_loss > threshold
-print("Number of anomaly samples: ", np.sum(anomalies))
-print("Indices of anomaly samples: ", np.where(anomalies))
-
-"""
-## Plot anomalies
-
-We now know the samples of the data which are anomalies. With this, we will
-find the corresponding `timestamps` from the original test data. We will be
-using the following method to do that:
-
-Let's say time_steps = 3 and we have 10 training values. Our `x_train` will
-look like this:
-
-- 0, 1, 2
-- 1, 2, 3
-- 2, 3, 4
-- 3, 4, 5
-- 4, 5, 6
-- 5, 6, 7
-- 6, 7, 8
-- 7, 8, 9
-
-All except the initial and the final time_steps-1 data values, will appear in
-`time_steps` number of samples. So, if we know that the samples
-[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point
-5 is an anomaly.
-"""
-
-# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies
-anomalous_data_indices = []
-for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):
-    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):
-        anomalous_data_indices.append(data_idx)
-
-"""
-Let's overlay the anomalies on the original test data plot.
-"""
-
-df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]
-fig, ax = plt.subplots()
-df_daily_jumpsup.plot(legend=False, ax=ax)
-df_subset.plot(legend=False, ax=ax, color="r")
-plt.show()
+if args.train:
+    start_time = time.time()
+    history = model.fit(
+        x_train,
+        x_train,
+        epochs=args.epochs,
+        batch_size=args.batch_size,
+        validation_split=0.1,
+        callbacks=[
+            keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")
+        ],
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = len(x_train) * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = create_model()
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_train) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(x_train, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/timeseries/timeseries_classification_from_scratch.py b/examples/timeseries/timeseries_classification_from_scratch.py
index 804b3c3..bc82f68 100755
--- a/examples/timeseries/timeseries_classification_from_scratch.py
+++ b/examples/timeseries/timeseries_classification_from_scratch.py
@@ -21,7 +21,36 @@ CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset f
 
 from tensorflow import keras
 import numpy as np
-import matplotlib.pyplot as plt
+import tensorflow as tf
+
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=50, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
 
 """
 ## Load the data: the FordA dataset
@@ -66,14 +95,6 @@ Here we visualize one timeseries example for each class in the dataset.
 
 classes = np.unique(np.concatenate((y_train, y_test), axis=0))
 
-plt.figure()
-for c in classes:
-    c_x_train = x_train[y_train == c]
-    plt.plot(c_x_train[0], label="class " + str(c))
-plt.legend(loc="best")
-plt.show()
-plt.close()
-
 """
 ## Standardize the data
 
@@ -150,77 +171,87 @@ def make_model(input_shape):
 
     gap = keras.layers.GlobalAveragePooling1D()(conv3)
 
-    output_layer = keras.layers.Dense(num_classes, activation="softmax")(gap)
+    x = keras.layers.Dense(num_classes)(gap)
+    output_layer = keras.layers.Activation("softmax")(x)
 
     return keras.models.Model(inputs=input_layer, outputs=output_layer)
 
 
 model = make_model(input_shape=x_train.shape[1:])
-keras.utils.plot_model(model, show_shapes=True)
 
 """
 ## Train the model
 
 """
 
-epochs = 500
-batch_size = 32
+epochs = args.epochs
+batch_size = args.batch_size
 
 callbacks = [
-    keras.callbacks.ModelCheckpoint(
-        "best_model.h5", save_best_only=True, monitor="val_loss"
-    ),
     keras.callbacks.ReduceLROnPlateau(
         monitor="val_loss", factor=0.5, patience=20, min_lr=0.0001
     ),
     keras.callbacks.EarlyStopping(monitor="val_loss", patience=50, verbose=1),
 ]
-model.compile(
-    optimizer="adam",
-    loss="sparse_categorical_crossentropy",
-    metrics=["sparse_categorical_accuracy"],
-)
-history = model.fit(
-    x_train,
-    y_train,
-    batch_size=batch_size,
-    epochs=epochs,
-    callbacks=callbacks,
-    validation_split=0.2,
-    verbose=1,
-)
-
-"""
-## Evaluate model on test data
-"""
-
-model = keras.models.load_model("best_model.h5")
-
-test_loss, test_acc = model.evaluate(x_test, y_test)
 
-print("Test accuracy", test_acc)
-print("Test loss", test_loss)
+def compile_model(model):
+    model.compile(
+        optimizer="adam",
+        loss="sparse_categorical_crossentropy",
+        metrics=["sparse_categorical_accuracy"],
+    )
+
+compile_model(model)
+
+if args.train:
+    start_time = time.time()
+    history = model.fit(
+        x_train,
+        y_train,
+        batch_size=batch_size,
+        epochs=args.epochs,
+        # callbacks=callbacks,
+        # validation_split=0.2,
+        steps_per_epoch=args.num_iter,
+        verbose=1,
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = make_model(input_shape=x_train.shape[1:])
+    compile_model(loaded_model)
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
 
 """
-## Plot the model's training and validation loss
-"""
-
-metric = "sparse_categorical_accuracy"
-plt.figure()
-plt.plot(history.history[metric])
-plt.plot(history.history["val_" + metric])
-plt.title("model " + metric)
-plt.ylabel(metric, fontsize="large")
-plt.xlabel("epoch", fontsize="large")
-plt.legend(["train", "val"], loc="best")
-plt.show()
-plt.close()
-
-"""
-We can see how the training accuracy reaches almost 0.95 after 100 epochs.
-However, by observing the validation accuracy we can see how the network still needs
-training until it reaches almost 0.97 for both the validation and the training accuracy
-after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation
-accuracy will start decreasing while the training accuracy will continue on increasing:
-the model starts overfitting.
+## Evaluate model on test data
 """
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_test) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    test_loss, test_acc = model.evaluate(x_test, y_test, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("Test accuracy", test_acc)
+    print("Test loss", test_loss)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/vision/autoencoder.py b/examples/vision/autoencoder.py
index 3e6119e..63b878b 100644
--- a/examples/vision/autoencoder.py
+++ b/examples/vision/autoencoder.py
@@ -22,12 +22,39 @@ by [François Chollet](https://twitter.com/fchollet).
 
 import numpy as np
 import tensorflow as tf
-import matplotlib.pyplot as plt
 
 from tensorflow.keras import layers
 from tensorflow.keras.datasets import mnist
 from tensorflow.keras.models import Model
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
 
 def preprocess(array):
     """
@@ -51,35 +78,6 @@ def noise(array):
 
     return np.clip(noisy_array, 0.0, 1.0)
 
-
-def display(array1, array2):
-    """
-    Displays ten random images from each one of the supplied arrays.
-    """
-
-    n = 10
-
-    indices = np.random.randint(len(array1), size=n)
-    images1 = array1[indices, :]
-    images2 = array2[indices, :]
-
-    plt.figure(figsize=(20, 4))
-    for i, (image1, image2) in enumerate(zip(images1, images2)):
-        ax = plt.subplot(2, n, i + 1)
-        plt.imshow(image1.reshape(28, 28))
-        plt.gray()
-        ax.get_xaxis().set_visible(False)
-        ax.get_yaxis().set_visible(False)
-
-        ax = plt.subplot(2, n, i + 1 + n)
-        plt.imshow(image2.reshape(28, 28))
-        plt.gray()
-        ax.get_xaxis().set_visible(False)
-        ax.get_yaxis().set_visible(False)
-
-    plt.show()
-
-
 """
 ## Prepare the data
 """
@@ -96,31 +94,32 @@ test_data = preprocess(test_data)
 noisy_train_data = noise(train_data)
 noisy_test_data = noise(test_data)
 
-# Display the train data and a version of it with added noise
-display(train_data, noisy_train_data)
-
 """
 ## Build the autoencoder
 
 We are going to use the Functional API to build our convolutional autoencoder.
 """
 
-input = layers.Input(shape=(28, 28, 1))
-
-# Encoder
-x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(input)
-x = layers.MaxPooling2D((2, 2), padding="same")(x)
-x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(x)
-x = layers.MaxPooling2D((2, 2), padding="same")(x)
-
-# Decoder
-x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
-x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
-x = layers.Conv2D(1, (3, 3), activation="sigmoid", padding="same")(x)
-
-# Autoencoder
-autoencoder = Model(input, x)
-autoencoder.compile(optimizer="adam", loss="binary_crossentropy")
+def create_model():
+    input = layers.Input(shape=(28, 28, 1))
+    
+    # Encoder
+    x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(input)
+    x = layers.MaxPooling2D((2, 2), padding="same")(x)
+    x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(x)
+    x = layers.MaxPooling2D((2, 2), padding="same")(x)
+    
+    # Decoder
+    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
+    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
+    x = layers.Conv2D(1, (3, 3), activation="sigmoid", padding="same")(x)
+    
+    # Autoencoder
+    autoencoder = Model(input, x)
+    autoencoder.compile(optimizer="adam", loss="binary_crossentropy")
+    return autoencoder
+
+autoencoder = create_model()
 autoencoder.summary()
 
 """
@@ -128,48 +127,60 @@ Now we can train our autoencoder using `train_data` as both our input data
 and target. Notice we are setting up the validation data using the same
 format.
 """
+if args.train:
+    start_time = time.time()
+    autoencoder.fit(
+        x=train_data,
+        y=train_data,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        batch_size=args.batch_size,
+        shuffle=True,
+        # validation_data=(test_data, test_data),
+    )
+    end_time = time.time()
 
-autoencoder.fit(
-    x=train_data,
-    y=train_data,
-    epochs=50,
-    batch_size=128,
-    shuffle=True,
-    validation_data=(test_data, test_data),
-)
-
-"""
-Let's predict on our test dataset and display the original image together with
-the prediction from our autoencoder.
-
-Notice how the predictions are pretty close to the original images, although
-not quite the same.
-"""
-
-predictions = autoencoder.predict(test_data)
-display(test_data, predictions)
-
-"""
-Now that we know that our autoencoder works, let's retrain it using the noisy
-data as our input and the clean data as our target. We want our autoencoder to
-learn how to denoise the images.
-"""
-
-autoencoder.fit(
-    x=noisy_train_data,
-    y=train_data,
-    epochs=100,
-    batch_size=128,
-    shuffle=True,
-    validation_data=(noisy_test_data, test_data),
-)
-
-"""
-Let's now predict on the noisy data and display the results of our autoencoder.
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
-Notice how the autoencoder does an amazing job at removing the noise from the
-input images.
-"""
+    """
+    Now that we know that our autoencoder works, let's retrain it using the noisy
+    data as our input and the clean data as our target. We want our autoencoder to
+    learn how to denoise the images.
+    """
+    autoencoder.fit(
+        x=noisy_train_data,
+        y=train_data,
+        epochs=args.epochs,
+        batch_size=args.batch_size,
+        shuffle=True,
+        validation_data=(noisy_test_data, test_data),
+    )
 
-predictions = autoencoder.predict(noisy_test_data)
-display(noisy_test_data, predictions)
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = create_model()
+    loaded_model.set_weights(autoencoder.get_weights())
+    autoencoder = loaded_model
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = args.num_iter if args.num_iter > 0 and args.num_iter < len(noisy_test_data) else len(noisy_test_data)
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    predictions = autoencoder.evaluate(noisy_test_data, steps=num_iter, batch_size=args.batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/vision/image_classification_with_vision_transformer.py b/examples/vision/image_classification_with_vision_transformer.py
index 9dfd47e..aad5cc2 100644
--- a/examples/vision/image_classification_with_vision_transformer.py
+++ b/examples/vision/image_classification_with_vision_transformer.py
@@ -34,6 +34,35 @@ from tensorflow import keras
 from tensorflow.keras import layers
 import tensorflow_addons as tfa
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Prepare the data
 """
@@ -53,8 +82,8 @@ print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")
 
 learning_rate = 0.001
 weight_decay = 0.0001
-batch_size = 256
-num_epochs = 100
+batch_size = args.batch_size
+num_epochs = args.epochs
 image_size = 72  # We'll resize input images to this size
 patch_size = 6  # Size of the patches to be extract from the input images
 num_patches = (image_size // patch_size) ** 2
@@ -95,7 +124,8 @@ data_augmentation.layers[0].adapt(x_train)
 
 def mlp(x, hidden_units, dropout_rate):
     for units in hidden_units:
-        x = layers.Dense(units, activation=tf.nn.gelu)(x)
+        x = layers.Dense(units)(x)
+        x = layers.Activation('gelu')(x)
         x = layers.Dropout(dropout_rate)(x)
     return x
 
@@ -123,17 +153,7 @@ class Patches(layers.Layer):
         patches = tf.reshape(patches, [batch_size, -1, patch_dims])
         return patches
 
-
-"""
-Let's display patches for a sample image
-"""
-
-import matplotlib.pyplot as plt
-
-plt.figure(figsize=(4, 4))
 image = x_train[np.random.choice(range(x_train.shape[0]))]
-plt.imshow(image.astype("uint8"))
-plt.axis("off")
 
 resized_image = tf.image.resize(
     tf.convert_to_tensor([image]), size=(image_size, image_size)
@@ -144,14 +164,6 @@ print(f"Patch size: {patch_size} X {patch_size}")
 print(f"Patches per image: {patches.shape[1]}")
 print(f"Elements per patch: {patches.shape[-1]}")
 
-n = int(np.sqrt(patches.shape[1]))
-plt.figure(figsize=(4, 4))
-for i, patch in enumerate(patches[0]):
-    ax = plt.subplot(n, n, i + 1)
-    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))
-    plt.imshow(patch_img.numpy().astype("uint8"))
-    plt.axis("off")
-
 """
 ## Implement the patch encoding layer
 
@@ -239,8 +251,7 @@ def create_vit_classifier():
 ## Compile, train, and evaluate the mode
 """
 
-
-def run_experiment(model):
+def compile_model(model):
     optimizer = tfa.optimizers.AdamW(
         learning_rate=learning_rate, weight_decay=weight_decay
     )
@@ -254,33 +265,63 @@ def run_experiment(model):
         ],
     )
 
-    checkpoint_filepath = "/tmp/checkpoint"
-    checkpoint_callback = keras.callbacks.ModelCheckpoint(
-        checkpoint_filepath,
-        monitor="val_accuracy",
-        save_best_only=True,
-        save_weights_only=True,
-    )
-
+def fit_model(model):
+    start_time = time.time()
     history = model.fit(
         x=x_train,
         y=y_train,
         batch_size=batch_size,
-        epochs=num_epochs,
-        validation_split=0.1,
-        callbacks=[checkpoint_callback],
+        epochs=args.epochs,
+        # validation_split=0.1,
+        steps_per_epoch=args.num_iter,
     )
-
-    model.load_weights(checkpoint_filepath)
-    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+def evaluate_model(model):
+    print("## Evaluate Start:")
+    num_iter = int(len(x_test) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, steps=num_iter, batch_size=batch_size)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
     print(f"Test accuracy: {round(accuracy * 100, 2)}%")
     print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")
-
-    return history
-
-
-vit_classifier = create_vit_classifier()
-history = run_experiment(vit_classifier)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = create_vit_classifier()
+    compile_model(loaded_model)
+    loaded_model.set_weights(model.get_weights())
+    vit_classifier = loaded_model
+else:
+    vit_classifier = create_vit_classifier()
+    compile_model(vit_classifier)
+
+if args.train:
+    fit_model(vit_classifier)
+
+if args.evaluate:
+    evaluate_model(vit_classifier)
 
 
 """
diff --git a/examples/vision/keypoint_detection.py b/examples/vision/keypoint_detection.py
index 0faecca..86f4cdd 100644
--- a/examples/vision/keypoint_detection.py
+++ b/examples/vision/keypoint_detection.py
@@ -71,14 +71,42 @@ import pandas as pd
 import numpy as np
 import json
 import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+parser.add_argument("--data_dir", type=str, default='./dataset/Stanford_extra', help="float32, int8 or bfloat16")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
 
 """
 ## Define hyperparameters
 """
 
 IMG_SIZE = 224
-BATCH_SIZE = 64
-EPOCHS = 5
+BATCH_SIZE =args.batch_size
+EPOCHS = args.epochs
 NUM_KEYPOINTS = 24 * 2  # 24 pairs each having x and y coordinates
 
 """
@@ -89,8 +117,8 @@ keypoints, like color information, animal pose name, etc. We will load this file
 dataframe to extract information for visualization purposes.
 """
 
-IMG_DIR = "Images"
-JSON = "StanfordExtra_V12/StanfordExtra_v12.json"
+IMG_DIR = args.data_dir + "/Images"
+JSON = args.data_dir + "/StanfordExtra_V12/StanfordExtra_v12.json"
 KEYPOINT_DEF = (
     "https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv"
 )
@@ -189,31 +217,6 @@ Now, we write a utility function to visualize the images and their keypoints.
 
 # Parts of this code come from here:
 # https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb
-def visualize_keypoints(images, keypoints):
-    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))
-    [ax.axis("off") for ax in np.ravel(axes)]
-
-    for (ax_orig, ax_all), image, current_keypoint in zip(axes, images, keypoints):
-        ax_orig.imshow(image)
-        ax_all.imshow(image)
-
-        # If the keypoints were formed by `imgaug` then the coordinates need
-        # to be iterated differently.
-        if isinstance(current_keypoint, KeypointsOnImage):
-            for idx, kp in enumerate(current_keypoint.keypoints):
-                ax_all.scatter(
-                    [kp.x], [kp.y], c=colours[idx], marker="x", s=50, linewidths=5
-                )
-        else:
-            current_keypoint = np.array(current_keypoint)
-            # Since the last entry is the visibility flag, we discard it.
-            current_keypoint = current_keypoint[:, :2]
-            for idx, (x, y) in enumerate(current_keypoint):
-                ax_all.scatter([x], [y], c=colours[idx], marker="x", s=50, linewidths=5)
-
-    plt.tight_layout(pad=2.0)
-    plt.show()
-
 
 # Select four samples randomly for visualization.
 samples = list(json_dict.keys())
@@ -230,8 +233,6 @@ for sample in selected_samples:
     images.append(image)
     keypoints.append(keypoint)
 
-visualize_keypoints(images, keypoints)
-
 """
 The plots show that we have images of non-uniform sizes, which is expected in most
 real-world scenarios. However, if we resize these images to have a uniform shape (for
@@ -352,13 +353,6 @@ validation_dataset = KeyPointsDataset(validation_keys, test_aug, train=False)
 print(f"Total batches in training set: {len(train_dataset)}")
 print(f"Total batches in validation set: {len(validation_dataset)}")
 
-sample_images, sample_keypoints = next(iter(train_dataset))
-assert sample_keypoints.max() == 1.0
-assert sample_keypoints.min() == 0.0
-
-sample_keypoints = sample_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE
-visualize_keypoints(sample_images[:4], sample_keypoints)
-
 """
 ## Model building
 
@@ -412,7 +406,54 @@ For this example, we will train the network only for five epochs.
 
 model = get_model()
 model.compile(loss="mse", optimizer=keras.optimizers.Adam(1e-4))
-model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS)
+
+if args.train:
+    start_time = time.time()
+    model.fit(
+        train_dataset,
+        # validation_data=validation_dataset,
+        steps_per_epoch=args.num_iter,
+        epochs=args.epochs,
+        batch_size=args.batch_size
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = get_model()
+    loaded_model.compile(loss="mse", optimizer=keras.optimizers.Adam(1e-4))
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+"""
+## Model evaluate
+"""
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = args.num_iter if args.num_iter > 0 else len(validation_dataset)
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    model.evaluate(validation_dataset, steps=num_iter, batch_size=BATCH_SIZE)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+
 
 """
 ## Make predictions and visualize them
@@ -423,23 +464,3 @@ sample_val_images = sample_val_images[:4]
 sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE
 predictions = model.predict(sample_val_images).reshape(-1, 24, 2) * IMG_SIZE
 
-# Ground-truth
-visualize_keypoints(sample_val_images, sample_val_keypoints)
-
-# Predictions
-visualize_keypoints(sample_val_images, predictions)
-
-"""
-Predictions will likely improve with more training.
-"""
-
-"""
-## Going further
-
-* Try using other augmentation transforms from `imgaug` to investigate how that changes
-the results.
-* Here, we transferred the features from the pre-trained network linearly that is we did
-not [fine-tune](https://keras.io/guides/transfer_learning/) it. You are encouraged to fine-tune it on this task and see if that
-improves the performance. You can also try different architectures and see how they
-affect the final performance.
-"""
diff --git a/examples/vision/mnist_convnet.py b/examples/vision/mnist_convnet.py
index 32d276f..b5d89d0 100644
--- a/examples/vision/mnist_convnet.py
+++ b/examples/vision/mnist_convnet.py
@@ -9,11 +9,40 @@ Description: A simple convnet that achieves ~99% test accuracy on MNIST.
 """
 ## Setup
 """
-
+import tensorflow as tf
 import numpy as np
 from tensorflow import keras
 from tensorflow.keras import layers
 
+import os
+import sys
+import time
+import argparse
+
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 """
 ## Prepare the data
 """
@@ -43,37 +72,81 @@ y_test = keras.utils.to_categorical(y_test, num_classes)
 """
 ## Build the model
 """
-
-model = keras.Sequential(
-    [
-        keras.Input(shape=input_shape),
-        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
-        layers.MaxPooling2D(pool_size=(2, 2)),
-        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
-        layers.MaxPooling2D(pool_size=(2, 2)),
-        layers.Flatten(),
-        layers.Dropout(0.5),
-        layers.Dense(num_classes, activation="softmax"),
-    ]
-)
-
+def get_model():
+    model = keras.Sequential(
+        [
+            keras.Input(shape=input_shape),
+            layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
+            layers.MaxPooling2D(pool_size=(2, 2)),
+            layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
+            layers.MaxPooling2D(pool_size=(2, 2)),
+            layers.Flatten(),
+            layers.Dropout(0.5),
+            layers.Dense(num_classes, activation="softmax"),
+        ]
+    )
+    return model
+
+model = get_model()
 model.summary()
 
 """
 ## Train the model
 """
 
-batch_size = 128
-epochs = 15
+batch_size = args.batch_size
+epochs = args.epochs
 
 model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
 
-model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
+if args.train:
+    start_time = time.time()
+    model.fit(
+        x_train,
+        y_train,
+        batch_size=batch_size,
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # validation_split=0.1
+    )
+    end_time = time.time()
+
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
 
 """
 ## Evaluate the trained model
 """
 
-score = model.evaluate(x_test, y_test, verbose=0)
-print("Test loss:", score[0])
-print("Test accuracy:", score[1])
+if args.precision == 'bf16_with_fp32_weight':
+    print("## Load FP32 weight, evaluate with BF16")
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+    loaded_model = get_model()
+    loaded_model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
+    loaded_model.set_weights(model.get_weights())
+    model = loaded_model
+
+if args.evaluate:
+    print("## Evaluate Start:")
+    num_iter = int(len(x_test) / args.batch_size)
+    if args.num_iter != 0 and args.num_iter < num_iter:
+        num_iter = args.num_iter
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    start_time = time.time()
+    score = model.evaluate(x_test, y_test, steps=num_iter, batch_size=batch_size, verbose=0)
+    end_time = time.time()
+    if args.profile:
+        tf.profiler.experimental.stop()
+    latency = (end_time - start_time) / (num_iter * args.batch_size) * 1000
+    throughput = (num_iter * args.batch_size) / (end_time - start_time)
+    print("Test loss:", score[0])
+    print("Test accuracy:", score[1])
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
diff --git a/examples/vision/retinanet.py b/examples/vision/retinanet.py
index e689f04..db7af96 100644
--- a/examples/vision/retinanet.py
+++ b/examples/vision/retinanet.py
@@ -27,8 +27,8 @@ the problem of the extreme foreground-background class imbalance.
 """
 
 
-import os
 import re
+from tracemalloc import start
 import zipfile
 
 import numpy as np
@@ -38,6 +38,33 @@ from tensorflow import keras
 import matplotlib.pyplot as plt
 import tensorflow_datasets as tfds
 
+import os
+import sys
+import time
+import argparse
+
+## args
+parser = argparse.ArgumentParser()
+parser.add_argument("--train", action='store_true', help="training.")
+parser.add_argument("--evaluate", action='store_true', help="evaluation.")
+parser.add_argument("-b", "--batch_size", type=int, default=1, help="batch size")
+parser.add_argument("--precision", type=str, default='float32', help="float32, int8 or bfloat16")
+parser.add_argument("--epochs", type=int, default=1, help="training epochs")
+parser.add_argument("--profile", action='store_true', help="profile.")
+parser.add_argument("-i", "-n", "--num_iter", type=int, default=200, 
+            help="numbers of inference iteration, default is 500")
+args = parser.parse_args()
+
+
+if args.precision == 'bfloat16' :
+    from tensorflow.keras import mixed_precision
+    policy = mixed_precision.Policy('mixed_bfloat16')
+    mixed_precision.set_global_policy(policy)
+
+## timeline
+import pathlib
+timeline_dir = str(pathlib.Path.cwd()) + '/timeline/' + str(os.getpid())
+
 
 """
 ## Downloading the COCO2017 dataset
@@ -47,13 +74,13 @@ lot of time, hence we will be using a smaller subset of ~500 images for
 training in this example.
 """
 
-url = "https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip"
-filename = os.path.join(os.getcwd(), "data.zip")
-keras.utils.get_file(filename, url)
-
-
-with zipfile.ZipFile("data.zip", "r") as z_fp:
-    z_fp.extractall("./")
+# url = "https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip"
+# filename = os.path.join(os.getcwd(), "data.zip")
+# keras.utils.get_file(filename, url)
+# 
+# 
+# with zipfile.ZipFile("data.zip", "r") as z_fp:
+#     z_fp.extractall("./")
 
 
 """
@@ -827,7 +854,7 @@ model_dir = "retinanet/"
 label_encoder = LabelEncoder()
 
 num_classes = 80
-batch_size = 2
+batch_size = args.batch_size
 
 learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]
 learning_rate_boundaries = [125, 250, 500, 240000, 360000]
@@ -867,7 +894,7 @@ callbacks_list = [
 #  set `data_dir=None` to load the complete dataset
 
 (train_dataset, val_dataset), dataset_info = tfds.load(
-    "coco/2017", split=["train", "validation"], with_info=True, data_dir="data"
+    "coco/2017", split=["train", "validation"], with_info=True, data_dir="dataset/retinanet/data/"
 )
 
 """
@@ -921,14 +948,26 @@ epochs = 1
 
 # Running 100 training and 50 validation steps,
 # remove `.take` when training on the full dataset
+if args.train:
+    print("---- Start to train")
+    start_time = time.time()
+    model.fit(
+        train_dataset,
+        # validation_data=val_dataset.take(50),
+        epochs=args.epochs,
+        steps_per_epoch=args.num_iter,
+        # callbacks=callbacks_list,
+        verbose=1,
+    )
+    end_time = time.time()
 
-model.fit(
-    train_dataset.take(100),
-    validation_data=val_dataset.take(50),
-    epochs=epochs,
-    callbacks=callbacks_list,
-    verbose=1,
-)
+    total_time = end_time - start_time
+    total_sample = args.num_iter * args.batch_size * args.epochs
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
+    exit(0)
 
 """
 ## Loading weights
@@ -940,40 +979,62 @@ weights_dir = "data"
 latest_checkpoint = tf.train.latest_checkpoint(weights_dir)
 model.load_weights(latest_checkpoint)
 
-"""
-## Building inference model
-"""
-
-image = tf.keras.Input(shape=[None, None, 3], name="image")
-predictions = model(image, training=False)
-detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)
-inference_model = tf.keras.Model(inputs=image, outputs=detections)
-
-"""
-## Generating detections
-"""
-
-
 def prepare_image(image):
     image, _, ratio = resize_and_pad_image(image, jitter=None)
     image = tf.keras.applications.resnet.preprocess_input(image)
     return tf.expand_dims(image, axis=0), ratio
 
+if args.evaluate:
+    print("---- Start to inference")
+    """
+    ## Building inference model
+    """
 
-val_dataset = tfds.load("coco/2017", split="validation", data_dir="data")
-int2str = dataset_info.features["objects"]["label"].int2str
+    image = tf.keras.Input(shape=[None, None, 3], name="image")
+    predictions = model(image, training=False)
+    detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)
+    inference_model = tf.keras.Model(inputs=image, outputs=detections)
 
-for sample in val_dataset.take(2):
-    image = tf.cast(sample["image"], dtype=tf.float32)
-    input_image, ratio = prepare_image(image)
-    detections = inference_model.predict(input_image)
-    num_detections = detections.valid_detections[0]
-    class_names = [
-        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]
-    ]
-    visualize_detections(
-        image,
-        detections.nmsed_boxes[0][:num_detections] / ratio,
-        class_names,
-        detections.nmsed_scores[0][:num_detections],
-    )
+    """
+    ## Generating detections
+    """
+
+    val_dataset = tfds.load("coco/2017", split="validation", data_dir="data")
+    int2str = dataset_info.features["objects"]["label"].int2str
+
+    total_time = 0.0
+    total_sample = 0
+    i = 0
+    args.num_iter = args.num_iter if args.num_iter > 0 else len(val_dataset)
+    print("Iteration: ", args.num_iter)
+    if args.profile:
+        tf.profiler.experimental.start(timeline_dir)
+    for sample in val_dataset.take(args.num_iter):
+        image = tf.cast(sample["image"], dtype=tf.float32)
+        input_image, ratio = prepare_image(image)
+
+        start_time = time.time()
+        detections = inference_model.predict(input_image)
+        end_time = time.time()
+        print("Iteration: {}, inference time: {} sec".format(i, (end_time - start_time)), flush=True)
+        total_time += (end_time - start_time)
+        total_sample += args.batch_size
+
+        # num_detections = detections.valid_detections[0]
+        # class_names = [
+        #     int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]
+        # ]
+        # visualize_detections(
+        #     image,
+        #     detections.nmsed_boxes[0][:num_detections] / ratio,
+        #     class_names,
+        #     detections.nmsed_scores[0][:num_detections],
+        # )
+        i += 1
+    if args.profile:
+        tf.profiler.experimental.stop()
+
+    latency = total_time / total_sample * 1000
+    throughput = total_sample / total_time
+    print("### Latency:: {:.2f} ms".format(latency))
+    print("### Throughput: {:.3f} samples/s".format(throughput))
