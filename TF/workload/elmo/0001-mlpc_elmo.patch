diff --git a/simple_elmo/elmo_helpers.py b/simple_elmo/elmo_helpers.py
index d2792af..58ddbd0 100644
--- a/simple_elmo/elmo_helpers.py
+++ b/simple_elmo/elmo_helpers.py
@@ -5,6 +5,7 @@ import json
 import logging
 import os
 import sys
+import time
 import zipfile
 import numpy as np
 import tensorflow as tf
@@ -40,6 +41,8 @@ class ElmoModel:
         self.init_state_tensors = None
         self.final_state_tensors = None
         self.init_state_values = None
+        self.num_warmup = None
+        self.num_iter = None
 
         # We do not use eager execution from TF 2.0
         tf.compat.v1.disable_eager_execution()
@@ -197,7 +200,7 @@ class ElmoModel:
 
         return "The model is now loaded."
 
-    def get_elmo_vectors(self, texts, warmup=True, layers="average"):
+    def get_elmo_vectors(self, texts, num_iter, num_warmup, warmup=True, layers="average", precision="float32"):
         """
         :param texts: list of sentences (lists of words)
         :param warmup: warm up the model before actual inference (by running it over the 1st batch)
@@ -208,6 +211,14 @@ class ElmoModel:
         """
         max_text_length = max([len(t) for t in texts])
 
+        config = tf.compat.v1.ConfigProto(allow_soft_placement=True)
+        if precision == 'bfloat16':
+            try:
+                from tensorflow.core.protobuf import rewriter_config_pb2
+                config.graph_options.rewrite_options.auto_mixed_precision_mkl = rewriter_config_pb2.RewriterConfig.ON
+            except:
+                print("WARNING: Auto mixed precision got FAILED.")
+
         # Creating the matrix which will eventually contain all embeddings from all batches:
         if layers == "all":
             final_vectors = np.zeros(
@@ -216,7 +227,7 @@ class ElmoModel:
         else:
             final_vectors = np.zeros((len(texts), max_text_length, self.vector_size))
 
-        with tf.compat.v1.Session() as sess:
+        with tf.compat.v1.Session(config=config) as sess:
             # Get an op to compute ELMo vectors (a function of the internal biLM layers)
             self.elmo_sentence_input = weight_layers(
                 "input", self.sentence_embeddings_op, use_layers=layers
@@ -226,10 +237,11 @@ class ElmoModel:
             sess.run(tf.compat.v1.global_variables_initializer())
 
             if warmup:
-                self.warmup(sess, texts)
+                self.warmup(sess, texts, num_warmup)
 
+            start = time.time()
             # Running batches:
-            chunk_counter = 0
+            chunk_counter = 1
             for chunk in divide_chunks(texts, self.batch_size):
                 # Converting sentences to character ids:
                 sentence_ids = self.batcher.batch_sentences(chunk)
@@ -248,7 +260,13 @@ class ElmoModel:
                 else:
                     final_vectors[first_row:last_row, : elmo_vectors.shape[1], :] = elmo_vectors
                 chunk_counter += 1
+                if chunk_counter > num_iter:
+                    break
 
+            end = time.time()
+            processing_time = end - start
+            throughput = chunk_counter / processing_time
+            self.logger.info(f"Throughput: {throughput} sentences/s")
             return final_vectors
 
     def get_elmo_vector_average(self, texts, warmup=True, layers="average"):
@@ -277,7 +295,7 @@ class ElmoModel:
             sess.run(tf.compat.v1.global_variables_initializer())
 
             if warmup:
-                self.warmup(sess, texts)
+                self.warmup(sess, texts, num_warmup)
 
             # Running batches:
             for chunk in divide_chunks(texts, self.batch_size):
@@ -440,7 +458,8 @@ class ElmoModel:
                 )
         return word_predictions
 
-    def warmup(self, sess, texts):
+    def warmup(self, sess, texts, num_warmup=10):
+        it = 1
         for chunk0 in divide_chunks(texts, self.batch_size):
             self.logger.info(f"Warming up ELMo on {len(chunk0)} sentences...")
             sentence_ids = self.batcher.batch_sentences(chunk0)
@@ -448,7 +467,9 @@ class ElmoModel:
                 self.elmo_sentence_input["weighted_op"],
                 feed_dict={self.sentence_character_ids: sentence_ids},
             )
-            break
+            it += 1
+            if it > num_warmup:
+                break
         self.logger.info("Warming up finished.")
 
 
diff --git a/simple_elmo/examples/get_elmo_vectors.py b/simple_elmo/examples/get_elmo_vectors.py
index 740bb53..b650e43 100755
--- a/simple_elmo/examples/get_elmo_vectors.py
+++ b/simple_elmo/examples/get_elmo_vectors.py
@@ -14,22 +14,33 @@ if __name__ == "__main__":
         "--input", "-i", help="Path to input text, one sentence per line", required=True
     )
     arg("--elmo", "-e", help="Path to ELMo model", required=True)
+    
+    arg('--num_iter', '-n', default=500, type=int,
+                   help='numbers of inference iteration (default: 500)')
+    arg("--batch", "-b", type=int, help="Max batch size", default=1)
+    arg("--precision", type=str, help="precision", default='float32')
+    arg('--num_warmup', default=10, type=int,
+                           help='numbers of warmup iteration, default is 10')
 
     args = parser.parse_args()
     data_path = args.input
+    max_batch_size = args.batch
+    num_warmup = args.num_warmup
+    num_iter = args.num_iter
+    precision = args.precision
 
     # Process only the first k sentences
-    max_sentences = 100
+    max_sentences = int(num_iter - num_warmup)
 
     raw_sentences = []
 
-    with open(data_path, "r") as f:
+    with open(data_path, "r", encoding='utf-8') as f:
         for line in f:
             res = line.strip()
             raw_sentences.append(res)
-            if len(raw_sentences) > max_sentences:
+            if len(raw_sentences) >= max_sentences:
                 break
-    sentences = [s.split()[:100] for s in raw_sentences]
+    sentences = [s.split()[:500] for s in raw_sentences]
 
     print("=====")
     print(f"{len(sentences)} sentences total")
@@ -37,17 +48,10 @@ if __name__ == "__main__":
 
     model = ElmoModel()
 
-    model.load(args.elmo)
+    model.load(args.elmo, max_batch_size=max_batch_size)
 
     # Actually producing ELMo embeddings for our data:
-    start = time.time()
-    elmo_vectors = model.get_elmo_vectors(sentences, layers="average")
-    end = time.time()
-
-    processing_time = int(end - start)
-
-    print(f"ELMo embeddings for your input are ready in {processing_time} seconds")
-    print(f"Tensor shape: {elmo_vectors.shape}")
+    elmo_vectors = model.get_elmo_vectors(sentences, num_iter, num_warmup, warmup=True, layers="average", precision=precision)
 
     # Due to batch processing, the above code produces for each sentence
     # the same number of token vectors, equal to the length of the longest sentence
