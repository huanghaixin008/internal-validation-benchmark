diff --git a/research/adversarial_text/evaluate.py b/research/adversarial_text/evaluate.py
index d7ea8c01..5c20326a 100644
--- a/research/adversarial_text/evaluate.py
+++ b/research/adversarial_text/evaluate.py
@@ -37,11 +37,13 @@ flags.DEFINE_string('eval_dir', '/tmp/text_eval',
                     'Directory where to write event logs.')
 flags.DEFINE_string('eval_data', 'test', 'Specify which dataset is used. '
                     '("train", "valid", "test") ')
-
 flags.DEFINE_string('checkpoint_dir', '/tmp/text_train',
                     'Directory where to read model checkpoints.')
+flags.DEFINE_string('precision', 'float32',
+                    'float32/bfloat16.')
 flags.DEFINE_integer('eval_interval_secs', 60, 'How often to run the eval.')
 flags.DEFINE_integer('num_examples', 32, 'Number of examples to run.')
+flags.DEFINE_integer('warm_up', 10, 'Number of examples to do warm up.')
 flags.DEFINE_bool('run_once', False, 'Whether to run eval only once.')
 
 
@@ -75,10 +77,16 @@ def run_eval(eval_ops, summary_writer, saver):
   Returns:
     dict<metric name, value>, with value being the average over all examples.
   """
+  config = tf.ConfigProto()
+  config.allow_soft_placement = True
+  if FLAGS.precision == 'bfloat16':
+      from tensorflow.core.protobuf import rewriter_config_pb2
+      config.graph_options.rewrite_options.auto_mixed_precision_mkl = rewriter_config_pb2.RewriterConfig.ON
+
   sv = tf.train.Supervisor(
-      logdir=FLAGS.eval_dir, saver=None, summary_op=None, summary_writer=None)
+    logdir=FLAGS.eval_dir, saver=None, summary_op=None, summary_writer=None)
   with sv.managed_session(
-      master=FLAGS.master, start_standard_services=False) as sess:
+    master=FLAGS.master, start_standard_services=False, config=config) as sess:
     if not restore_from_checkpoint(sess, saver):
       return
     sv.start_queue_runners(sess)
@@ -91,14 +99,21 @@ def run_eval(eval_ops, summary_writer, saver):
     # Run update ops
     num_batches = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))
     tf.logging.info('Running %d batches for evaluation.', num_batches)
+
     for i in range(num_batches):
-      if (i + 1) % 10 == 0:
+      if i == FLAGS.warm_up:
+        tic = time.time()
+      if (i + 1) % 100 == 0:
         tf.logging.info('Running batch %d/%d...', i + 1, num_batches)
-      if (i + 1) % 50 == 0:
+      if (i + 1) % 100 == 0:
         _log_values(sess, value_ops_dict)
       sess.run(update_ops)
 
-    _log_values(sess, value_ops_dict, summary_writer=summary_writer)
+    toc = time.time()
+    print("batch size: {}".format(FLAGS.batch_size))
+    print("total number: {} ".format(FLAGS.num_examples))
+    print("Throughput: {:.2f} fps".format((FLAGS.num_examples - FLAGS.warm_up) / (toc - tic)))
+    # _log_values(sess, value_ops_dict, summary_writer=summary_writer)
 
 
 def _log_values(sess, value_ops, summary_writer=None):
