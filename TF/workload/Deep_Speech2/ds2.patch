diff --git a/research/deep_speech/data/dataset.py b/research/deep_speech/data/dataset.py
index 7fdcaf5c..9bd91617 100644
--- a/research/deep_speech/data/dataset.py
+++ b/research/deep_speech/data/dataset.py
@@ -161,7 +161,7 @@ class DeepSpeechDataset(object):
     self.entries = _preprocess_data(self.config.data_path)
     # The generated spectrogram will have 161 feature bins.
     self.num_feature_bins = 161
-
+    self.size = len(self.entries)

 def batch_wise_dataset_shuffle(entries, epoch_index, sortagrad, batch_size):
   """Batch-wise shuffling of the data entries.
diff --git a/research/deep_speech/deep_speech.py b/research/deep_speech/deep_speech.py
index df14be36..365753dd 100644
--- a/research/deep_speech/deep_speech.py
+++ b/research/deep_speech/deep_speech.py
@@ -18,6 +18,7 @@ from __future__ import division
 from __future__ import print_function

 import os
+import time
 # pylint: disable=g-bad-import-order
 from absl import app as absl_app
 from absl import flags
@@ -67,7 +68,7 @@ def compute_length_after_conv(max_time_steps, ctc_time_steps, input_length):
       ctc_input_length, tf.cast(max_time_steps, dtype=tf.float32)), dtype=tf.int32)


-def evaluate_model(estimator, speech_labels, entries, input_fn_eval):
+def evaluate_model(estimator, speech_labels, entries, input_fn_eval, batch_size, num_iter, num_warmup):
   """Evaluate the model performance using WER anc CER as metrics.

   WER: Word Error Rate
@@ -105,8 +106,8 @@ def evaluate_model(estimator, speech_labels, entries, input_fn_eval):
         len(targets[i].split()))

   # Get mean value
-  total_cer /= num_of_examples
-  total_wer /= num_of_examples
+  #total_cer /= num_of_examples
+  #total_wer /= num_of_examples

   global_step = estimator.get_variable_value(tf.compat.v1.GraphKeys.GLOBAL_STEP)
   eval_results = {
@@ -226,11 +227,12 @@ def run_deep_speech(_):
   tf.compat.v1.set_random_seed(flags_obj.seed)
   # Data preprocessing
   tf.compat.v1.logging.info("Data preprocessing...")
-  train_speech_dataset = generate_dataset(flags_obj.train_data_dir)
+  if not flags_obj.only_dev:
+    train_speech_dataset = generate_dataset(flags_obj.train_data_dir)
   eval_speech_dataset = generate_dataset(flags_obj.eval_data_dir)

   # Number of label classes. Label string is "[a-z]' -"
-  num_classes = len(train_speech_dataset.speech_labels)
+  num_classes = len(eval_speech_dataset.speech_labels)

   # Use distribution strategy for multi-gpu training
   num_gpus = flags_core.get_num_gpus(flags_obj)
@@ -255,50 +257,73 @@ def run_deep_speech(_):
       "rnn_hidden_layers": flags_obj.rnn_hidden_layers,
       "rnn_type": flags_obj.rnn_type,
       "is_bidirectional": flags_obj.is_bidirectional,
-      "use_bias": flags_obj.use_bias
+      "use_bias": flags_obj.use_bias,
+      "eval_num_iter": flags_obj.eval_num_iter,
+      "eval_num_warmup": flags_obj.eval_num_warmup,
+      "precision": flags_obj.precision
   }

   per_replica_batch_size = per_device_batch_size(flags_obj.batch_size, num_gpus)
-
-  def input_fn_train():
-    return dataset.input_fn(
-        per_replica_batch_size, train_speech_dataset)
+  if not flags_obj.only_dev:
+    def input_fn_train():
+      return dataset.input_fn(
+          per_replica_batch_size, train_speech_dataset)

   def input_fn_eval():
     return dataset.input_fn(
         per_replica_batch_size, eval_speech_dataset)

-  total_training_cycle = (flags_obj.train_epochs //
-                          flags_obj.epochs_between_evals)
-  for cycle_index in range(total_training_cycle):
-    tf.compat.v1.logging.info("Starting a training cycle: %d/%d",
-                    cycle_index + 1, total_training_cycle)
-
-    # Perform batch_wise dataset shuffling
-    train_speech_dataset.entries = dataset.batch_wise_dataset_shuffle(
-        train_speech_dataset.entries, cycle_index, flags_obj.sortagrad,
-        flags_obj.batch_size)
-
-    estimator.train(input_fn=input_fn_train)
-
-    # Evaluation
-    tf.compat.v1.logging.info("Starting to evaluate...")
-
-    eval_results = evaluate_model(
-        estimator, eval_speech_dataset.speech_labels,
-        eval_speech_dataset.entries, input_fn_eval)
-
-    # Log the WER and CER results.
-    benchmark_logger.log_evaluation_result(eval_results)
-    tf.compat.v1.logging.info(
-        "Iteration {}: WER = {:.2f}, CER = {:.2f}".format(
-            cycle_index + 1, eval_results[_WER_KEY], eval_results[_CER_KEY]))
-
-    # If some evaluation threshold is met
-    if model_helpers.past_stop_threshold(
-        flags_obj.wer_threshold, eval_results[_WER_KEY]):
-      break

+  if not flags_obj.only_dev:
+    total_training_cycle = (flags_obj.train_epochs //
+                            flags_obj.epochs_between_evals)
+    for cycle_index in range(total_training_cycle):
+      tf.compat.v1.logging.info("Starting a training cycle: %d/%d",
+                      cycle_index + 1, total_training_cycle)
+
+      # Perform batch_wise dataset shuffling
+      train_speech_dataset.entries = dataset.batch_wise_dataset_shuffle(
+          train_speech_dataset.entries, cycle_index, flags_obj.sortagrad,
+          flags_obj.batch_size)
+      print("********** {}".format('starting to training'))
+      estimator.train(input_fn=input_fn_train)
+
+      # Evaluation
+      tf.compat.v1.logging.info("Starting to evaluate...")
+
+      eval_results = evaluate_model(
+          estimator, eval_speech_dataset.speech_labels,
+          eval_speech_dataset.entries, input_fn_eval, flags_obj.batch_size, flags_obj.eval_num_iter, flags_obj.eval_num_warmup)
+
+      # Log the WER and CER results.
+      #benchmark_logger.log_evaluation_result(eval_results)
+      tf.compat.v1.logging.info(
+          "Iteration {}: WER = {:.2f}, CER = {:.2f}".format(
+              cycle_index + 1, eval_results[_WER_KEY], eval_results[_CER_KEY]))
+
+      # If some evaluation threshold is met
+      if model_helpers.past_stop_threshold(
+          flags_obj.wer_threshold, eval_results[_WER_KEY]):
+        break
+
+  else:
+      # Evaluation
+      tf.compat.v1.logging.info("Starting to evaluate...")
+
+      tic = time.time()
+      eval_results = evaluate_model(
+          estimator, eval_speech_dataset.speech_labels,
+          eval_speech_dataset.entries, input_fn_eval, flags_obj.batch_size, flags_obj.eval_num_iter, flags_obj.eval_num_warmup)
+      toc = time.time()
+      print("***** During time: {}".format(toc-tic))
+      print("***** Total sample: {}".format(eval_speech_dataset.size))
+      print("***** Throughput: {} samples/s".format(eval_speech_dataset.size / (toc-tic)))
+
+      # Log the WER and CER results.
+      #benchmark_logger.log_evaluation_result(eval_results)
+      #tf.compat.v1.logging.info(
+      #    "Iteration {}: WER = {:.2f}, CER = {:.2f}".format(
+      #        cycle_index + 1, eval_results[_WER_KEY], eval_results[_CER_KEY]))

 def define_deep_speech_flags():
   """Add flags for run_deep_speech."""
@@ -334,6 +359,17 @@ def define_deep_speech_flags():
       name="seed", default=1,
       help=flags_core.help_wrap("The random seed."))

+  flags.DEFINE_integer(
+      name="eval_num_iter", default=-1,
+      help=flags_core.help_wrap("The eval iter."))
+  flags.DEFINE_integer(
+      name="eval_num_warmup", default=10,
+      help=flags_core.help_wrap("The eval warmup."))
+  flags.DEFINE_string(
+      name="precision",
+      default="float32",
+      help=flags_core.help_wrap("float32 or bfloat16."))
+
   flags.DEFINE_string(
       name="train_data_dir",
       default="/tmp/librispeech_data/test-clean/LibriSpeech/test-clean.csv",
@@ -389,6 +425,10 @@ def define_deep_speech_flags():
       case_sensitive=False,
       help=flags_core.help_wrap("Type of RNN cell."))

+  flags.DEFINE_bool(
+      name="only_dev", default=False,
+      help=flags_core.help_wrap("Use bias in the last fully-connected layer"))
+
   # Training related flags
   flags.DEFINE_float(
       name="learning_rate", default=5e-4,
@@ -403,7 +443,6 @@ def define_deep_speech_flags():
           "the desired wer_threshold is 0.23 which is the result achieved by "
           "MLPerf implementation."))

-
 def main(_):
   run_deep_speech(flags_obj)

