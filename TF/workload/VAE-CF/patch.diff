diff --git a/run.py b/run.py
index 2eb40a1..c55035e 100644
--- a/run.py
+++ b/run.py
@@ -17,16 +17,17 @@ from vae.metrics.ndcg import ndcg
               help='Enable Automatic Mixed Precision to speedup fp32 computation using tensor cores')
 @click.option('--dataset', default=vae.config.ML_20M, help='Dataset to use')
 @click.option('--gpu_number', default=0, help='Number of GPU used in training or validation')
-@click.option('--number_of_gpus', default=1, help='How many GPUs to use during training or validation')
-@click.option('--number_of_epochs', default=200, help='Number of epochs to train')
-@click.option('--batch_size_train', default=10000)
-@click.option('--batch_size_validation', default=10000, help='Used both for validation and testing')
+@click.option('--number_of_gpus', default=0, help='How many GPUs to use during training or validation')
+@click.option('--number_of_epochs', default=1, help='Number of epochs to train')
+@click.option('--batch_size_train', default=1)
+@click.option('--batch_size_validation', default=1, help='Used both for validation and testing')
 @click.option('--validation_step', default=5)
 @click.option('--warm_up_epochs', default=5, help='Number of epochs to omit during benchmark')
 @click.option('--total_anneal_steps', default=200000, help='Number of annealing steps')
 @click.option('--anneal_cap', default=0.2, help='Annealing cap')
 @click.option('--lam', default=1e-2, help='Regularization parameter')
 @click.option('--lr', default=1e-3, help='Learning rate')
+@click.option('--precision', default='float32', type=str, help='precision')
 def main(train,
          test,
          benchmark,
@@ -42,7 +43,8 @@ def main(train,
          total_anneal_steps,
          anneal_cap,
          lam,
-         lr):
+         lr,
+         precision):
     if not train and not test and not benchmark:
         print("Choose one or more:")
         for option in "train", "test", "benchmark":
@@ -98,7 +100,8 @@ def main(train,
                   batch_size_validation=batch_size_validation,
                   lam=lam,
                   lr=lr,
-                  device='/device:CPU')
+                  device="/cpu:0",
+                  precision=precision)
     elif number_of_gpus == 1:
         from vae.models.Mult_VAE_training import VAE
         vae = VAE(train_data,
@@ -109,7 +112,8 @@ def main(train,
                   batch_size_validation=batch_size_validation,
                   lam=lam,
                   lr=lr,
-                  device='/device:GPU:' + str(gpu_number))
+                  device='/device:GPU:' + str(gpu_number),
+                  precision=precision)
     else:
         batches_per_epoch = -(-number_of_train_users // batch_size_train)  # ceil div
         if (batches_per_epoch % number_of_gpus != 0):
@@ -123,7 +127,8 @@ def main(train,
                           batch_size_train=batch_size_train,
                           batch_size_validation=batch_size_validation,
                           lam=lam,
-                          lr=lr)
+                          lr=lr,
+                          precision=precision)

     metrics = {'ndcg@100': partial(ndcg, R=100),
                'recall@20': partial(recall, R=20),
@@ -137,11 +142,12 @@ def main(train,
                   batch_size_train=batch_size_train,
                   batch_size_validation=batch_size_validation,
                   metrics=metrics,
-                  validation_step=validation_step)
-        if number_of_gpus > 1:
-            print("Saving is not supported with horovod multigpu yet")
-        else:
-            vae.save()
+                  validation_step=validation_step,
+                  precision=precision)
+        # if number_of_gpus > 1:
+        #     print("Saving is not supported with horovod multigpu yet")
+        # else:
+        #     vae.save()

     if benchmark:
         vae.benchmark(n_epochs=number_of_epochs,
diff --git a/vae/load/preprocessing.py b/vae/load/preprocessing.py
index 529df84..870d7dc 100644
--- a/vae/load/preprocessing.py
+++ b/vae/load/preprocessing.py
@@ -71,11 +71,7 @@ def load_and_parse_ML_20M(path, threshold=4, seed=98765, **kwargs):
     test_data_true_file = os.path.join(CACHE_DIR, "test_data_true.npz")
     test_data_test_file = os.path.join(CACHE_DIR, "test_data_test.npz")

-    if (os.path.isfile(train_data_file)
-       and os.path.isfile(vad_data_true_file
-       and os.path.isfile((vad_data_test_file)
-       and os.path.isfile(test_data_true_file)
-       and os.path.isfile(test_data_test_file):
+    if (os.path.isfile(train_data_file) and os.path.isfile(vad_data_true_file) and os.path.isfile(vad_data_test_file) and os.path.isfile(test_data_true_file) and os.path.isfile(test_data_test_file)):

         LOG.info("Already processed, skipping.")
         return load_npz(train_data_file), \
@@ -110,7 +106,7 @@ def load_and_parse_ML_20M(path, threshold=4, seed=98765, **kwargs):
         # After doing this, some of the items will have less than min_uc users, but should only be a small proportion
         if min_uc > 0:
             usercount = get_count(tp, 'userId')
-            tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]
+            tp = tp[tp['userId'].isin(usercount.index[usercount["size"] >= min_uc])]

         # Update both usercount and itemcount after filtering
         usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')
@@ -160,9 +156,9 @@ def load_and_parse_ML_20M(path, threshold=4, seed=98765, **kwargs):
     vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]
     vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]

-    vad_plays_true, vad_plays_test = split_train_testst_proportion(vad_plays)
+    vad_plays_true, vad_plays_test = split_train_test_proportion(vad_plays)

-    test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]
+    test_plays = raw_data.loc[raw_data['userId'].isin(test_users)]
     test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]

     test_plays_true, test_plays_test = split_train_test_proportion(test_plays)
diff --git a/vae/models/Mult_VAE_training.py b/vae/models/Mult_VAE_training.py
index 5dc68db..07fbde0 100644
--- a/vae/models/Mult_VAE_training.py
+++ b/vae/models/Mult_VAE_training.py
@@ -29,7 +29,8 @@ class VAE:
                  lr=1e-3,
                  total_anneal_steps=200000,
                  anneal_cap=0.2,
-                 device='CPU'):
+                 device='CPU',
+                 precision='float32'):

         if not batch_size_train > 0:
             raise Exception("batch_size_train has to be positive")
@@ -43,9 +44,9 @@ class VAE:
             raise Exception("encoder_dims is mandatory")
         if decoder_dims is None:
             decoder_dims = encoder_dims[::-1]
-        for i in encoder_dims + decoder_dims + [batch_size_train, batch_size_validation]:
-            if i != round_8(i):
-                raise Exception("all dims and batch sizes should be divisible by 8")
+        # for i in encoder_dims + decoder_dims + [batch_size_train, batch_size_validation]:
+        #     if i != round_8(i):
+        #         raise Exception("all dims and batch sizes should be divisible by 8")

         self.metrics_history = None
         self.batch_size_train = batch_size_train
@@ -55,12 +56,13 @@ class VAE:
         self.total_anneal_steps = total_anneal_steps
         self.anneal_cap = anneal_cap
         self.device = device
+        self.precision = precision
         self.encoder_dims = encoder_dims
         self.decoder_dims = decoder_dims
         self._create_dataset(train_data,
                              batch_size_train,
                              encoder_dims)
-        self._setup_model()
+        self._setup_model(self.precision)

     def train(
             self,
@@ -72,6 +74,7 @@ class VAE:
             batch_size_validation: int,
             metrics: dict,  # Dict[str, matrix -> matrix -> float]
             validation_step: 10,
+            precision: 'float32',
     ):
         """
         Train the model
@@ -89,30 +92,59 @@ class VAE:
         for epoch in range(1, n_epochs + 1):

             self.log_which_epoch(epoch, n_epochs)
-            init_time = time.time()
-
-            for _ in range(self.n_batch_per_train):
+            # init_time = time.time()
+            for _ in range(200):
                 self.session.run(self.optimizer)

-            training_duration = time.time() - init_time
-            self.time_elapsed_training_history.append(training_duration)
-            LOG.info("Train time:\t{}".format(training_duration))
-
-            if epoch % validation_step == 0 or epoch == n_epochs:
-                init_time = time.time()
-                metrics_scores = self.test(validation_data_input,
-                                           validation_data_true,
-                                           metrics)
-
-                for name, score in metrics_scores.items():
-                    self.metrics_history[name].append(score)
-
-                validation_duration = time.time() - init_time
-                self.time_elapsed_validation_history.append(validation_duration)
-                LOG.info("Valid time:\t{}".format(validation_duration))
-                self.log_metrics(epoch, metrics_scores, n_epochs)
-
-        self.log_training_time()
+            # training_duration = time.time() - init_time
+            # self.time_elapsed_training_history.append(training_duration)
+            # LOG.info("Train time:\t{}".format(training_duration))
+            # from tensorflow.python.framework import graph_util
+            # constant_graph = graph_util.convert_variables_to_constants(self.session, self.session.graph_def, ['private_vae_graph/sequential_1/decoder_20056/BiasAdd'])
+            # # with tf.gfile.FastGFile('/home/jialew/tensorflow_enable/VAE_CF/VAE-CF/export_dir/vace.pb', mode='wb') as f:
+            # #         f.write(constant_graph.SerializeToString())
+            # tf.compat.v1.train.write_graph(constant_graph ,'/home/jialew/tensorflow_enable/VAE_CF/VAE-CF/export_dir/','vace.pbtxt',True)
+            # if epoch % validation_step == 0 or epoch == n_epochs:
+            #    init_time = time.time()
+
+            #    metrics_scores = self.test(validation_data_input,
+            #                               validation_data_true,
+            #                               metrics)
+
+             #   for name, score in metrics_scores.items():
+             #       self.metrics_history[name].append(score)
+
+              #  validation_duration = time.time() - init_time
+              #  self.time_elapsed_validation_history.append(validation_duration)
+              #  LOG.info("Valid time:\t{}".format(validation_duration))
+              #  self.log_metrics(epoch, metrics_scores, n_epochs)
+
+            total_time = 0.0
+            reps_done = 0
+            gen= self.batch_iterator_val(validation_data_input,
+                                            validation_data_true)
+            warm_up=0
+            for idxs, vals, X_true in gen():
+                if warm_up<10:
+                    pred_val = self.session.run(
+                        self.logits_validation,
+                        feed_dict={self.inputs_validation: (idxs, vals)})
+                else:
+                    start = time.time()
+                    pred_val = self.session.run(
+                        self.logits_validation, feed_dict={self.inputs_validation: (idxs, vals)})
+                    end = time.time()
+                    delta = end - start
+                    total_time += delta
+                    reps_done += 1
+                warm_up+=1
+            print(reps_done)
+            avg_time = total_time / reps_done
+            latency = avg_time * 1000
+            throughput = 1.0 / avg_time
+            print("Latency: {:.0f} ms".format(latency))
+            print("Throughput: {:.2f} fps".format(throughput))
+        # self.log_training_time()

     def test(
             self,
@@ -251,11 +283,17 @@ class VAE:
         tf.saved_model.loader.load(self.session,
                                    [tag_constants.SERVING],
                                    directory)
-    def _setup_model(self):
+    def _setup_model(self, precision):
         with tf.device(self.device):
             self._build_graph()
-            config = tf.ConfigProto()
+            config = tf.ConfigProto(allow_soft_placement=True)
             config.gpu_options.allow_growth = True
+            if precision == 'bfloat16':
+                try:
+                    from tensorflow.core.protobuf import rewriter_config_pb2
+                    config.graph_options.rewrite_options.auto_mixed_precision_mkl = rewriter_config_pb2.RewriterConfig.ON
+                except:
+                    print("WARNING: Auto mixed precision got FAILED.")
             self.session = tf.Session(config=config)
             init = tf.global_variables_initializer()
             self.session.run(init)
@@ -263,7 +301,7 @@ class VAE:
     def _build_graph(self):
         self.vae = _VAEGraph(self.encoder_dims, self.decoder_dims)

-        self.inputs_validation = tf.sparse.placeholder(
+        self.inputs_validation = tf.sparse.placeholder(name="input",
             dtype=tf.float32,
             shape=np.array([self.batch_size_validation, self.vae.input_dim], dtype=np.int32))
         self.inputs_query = tf.sparse.placeholder(
