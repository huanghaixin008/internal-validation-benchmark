diff --git a/recommendation/pytorch/ncf.py b/recommendation/pytorch/ncf.py
index d6cbd85..7965fc2 100644
--- a/recommendation/pytorch/ncf.py
+++ b/recommendation/pytorch/ncf.py
@@ -16,6 +16,7 @@ import tqdm
 import numpy as np
 import torch
 import torch.nn as nn
+import intel_pytorch_extension as ipex
 
 import utils
 from neumf import NeuMF
@@ -68,38 +69,55 @@ def parse_args():
                         help='pre-process data on cpu to save memory')
     parser.add_argument('--random_negatives', action='store_true',
                         help='do not check train negatives for existence in dataset')
+    parser.add_argument('--ipex', action='store_true',
+                        help='use ipex')
+    # parser.add_argument('--jit', action='store_true',
+    #                     help='use fusion jit model')
+    parser.add_argument('--evaluate', action='store_true',
+                        help='evaluate only')
+    parser.add_argument('--precision', type=str, default="float32",
+                        help='precision, float32, int8, bfloat16')
     return parser.parse_args()
 
 
 # TODO: val_epoch is not currently supported on cpu
 def val_epoch(model, x, y, dup_mask, real_indices, K, samples_per_user, num_user, output=None,
-              epoch=None, loss=None):
+              epoch=None, loss=None, device='cpu'):
 
     start = datetime.now()
     log_2 = math.log(2)
 
     model.eval()
-    hits = torch.tensor(0., device='cuda')
-    ndcg = torch.tensor(0., device='cuda')
+    hits = torch.tensor(0., device=device)
+    ndcg = torch.tensor(0., device=device)
+    batch_size = None
+    total_time = 0
 
     with torch.no_grad():
         for i, (u,n) in enumerate(zip(x,y)):
-            res = model(u.cuda().view(-1), n.cuda().view(-1), sigmoid=True).detach().view(-1,samples_per_user)
+            if batch_size is None:
+                batch_size = u.size()[0]
+            if i > 5:
+                start = time.time()
+            res = model(u.to(device).view(-1), n.to(device).view(-1), sigmoid=True).detach().view(-1,samples_per_user)
             # set duplicate results for the same item to -1 before topk
             res[dup_mask[i]] = -1
             out = torch.topk(res,K)[1]
             # topk in pytorch is stable(if not sort)
             # key(item):value(predicetion) pairs are ordered as original key(item) order
             # so we need the first position of real item(stored in real_indices) to check if it is in topk
-            ifzero = (out == real_indices[i].cuda().view(-1,1))
+            ifzero = (out == real_indices[i].to(device).view(-1,1))
             hits += ifzero.sum()
             ndcg += (log_2 / (torch.nonzero(ifzero)[:,1].view(-1).to(torch.float)+2).log_()).sum()
+            if i > 5:
+                total_time += time.time() - start
 
     mlperf_log.ncf_print(key=mlperf_log.EVAL_SIZE, value={"epoch": epoch, "value": num_user * samples_per_user})
     mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_USERS, value=num_user)
     mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_NEG, value=samples_per_user - 1)
 
     end = datetime.now()
+    print('inference Throughput: %0.3f samples/s' % (batch_size * (i - 5) / total_time))
 
     hits = hits.item()
     ndcg = ndcg.item()
@@ -121,6 +139,7 @@ def val_epoch(model, x, y, dup_mask, real_indices, K, samples_per_user, num_user
 def main():
 
     args = parse_args()
+    print(args)
     if args.seed is not None:
         print("Using seed = {}".format(args.seed))
         torch.manual_seed(args.seed)
@@ -133,7 +152,7 @@ def main():
     run_dir = "./run/neumf/{}".format(config['timestamp'])
     print("Saving config and results to {}".format(run_dir))
     if not os.path.exists(run_dir) and run_dir != '':
-        os.makedirs(run_dir)
+        os.makedirs(run_dir, exist_ok=True)
     utils.save_config(config, run_dir)
 
     # Check that GPUs are actually available
@@ -141,8 +160,16 @@ def main():
     # Check where to put data loader
     if use_cuda:
         dataloader_device = 'cpu' if args.cpu_dataloader else 'cuda'
+        device = 'cuda'
+    elif args.ipex:
+        dataloader_device = 'cpu'
+        device = ipex.DEVICE
+        if args.precision == "bfloat16":
+            # Automatically mix precision
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
     else:
         dataloader_device = 'cpu'
+        device = 'cpu'
 
     # more like load trigger timmer now
     mlperf_log.ncf_print(key=mlperf_log.PREPROC_HP_NUM_EVAL, value=args.valid_negative)
@@ -151,8 +178,9 @@ def main():
     mlperf_log.ncf_print(key=mlperf_log.INPUT_HP_SAMPLE_TRAIN_REPLACEMENT, value=True)
     mlperf_log.ncf_print(key=mlperf_log.INPUT_STEP_EVAL_NEG_GEN)
 
-    # sync worker before timing.
-    torch.cuda.synchronize()
+    if use_cuda:
+        # sync worker before timing.
+        torch.cuda.synchronize()
 
     #===========================================================================
     #== The clock starts on loading the preprocessed data. =====================
@@ -250,7 +278,8 @@ def main():
     real_indices = torch.cat(real_indices)
 
     # make pytorch memory behavior more consistent later
-    torch.cuda.empty_cache()
+    if use_cuda:
+        torch.cuda.empty_cache()
 
     mlperf_log.ncf_print(key=mlperf_log.INPUT_BATCH_SIZE, value=args.batch_size)
     mlperf_log.ncf_print(key=mlperf_log.INPUT_ORDER)  # we shuffled later with randperm
@@ -285,9 +314,13 @@ def main():
 
     if use_cuda:
         # Move model and loss to GPU
-        model = model.cuda()
+        # model = model.cuda()
         criterion = criterion.cuda()
 
+    model.to(device)
+    # if args.jit:
+    #     model = torch.jit.script(model)
+
     local_batch = args.batch_size
     traced_criterion = torch.jit.trace(criterion.forward, (torch.rand(local_batch,1),torch.rand(local_batch,1)))
 
@@ -304,9 +337,12 @@ def main():
     real_indices = real_indices.split(users_per_valid_batch)
 
     hr, ndcg = val_epoch(model, test_users, test_items, dup_mask, real_indices, args.topk, samples_per_user=samples_per_user,
-                         num_user=nb_users)
+                         num_user=nb_users, device=device)
     print('Initial HR@{K} = {hit_rate:.4f}, NDCG@{K} = {ndcg:.4f}'
           .format(K=args.topk, hit_rate=hr, ndcg=ndcg))
+    if args.evaluate:
+        return
+
     success = False
     mlperf_log.ncf_print(key=mlperf_log.TRAIN_LOOP)
     for epoch in range(args.epochs):
