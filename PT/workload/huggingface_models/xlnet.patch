diff --git a/examples/run_glue.py b/examples/run_glue.py
index 1f586b7..d1483bf 100644
--- a/examples/run_glue.py
+++ b/examples/run_glue.py
@@ -24,6 +24,7 @@ import os
 import random
 from dataclasses import dataclass, field
 from typing import Optional
+import time 
 
 import numpy as np
 import torch
@@ -43,6 +44,7 @@ from transformers import (
     get_linear_schedule_with_warmup,
 )
 from transformers import glue_compute_metrics as compute_metrics
+from transformers import bart_compute_metrics as bart_compute_metrics
 from transformers import glue_convert_examples_to_features as convert_examples_to_features
 from transformers import glue_output_modes as output_modes
 from transformers import glue_processors as processors
@@ -175,7 +177,7 @@ def train(args, train_dataset, model, tokenizer):
             model.train()
             batch = tuple(t.to(args.device) for t in batch)
             inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
-            if args.model_type != "distilbert":
+            if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
                 inputs["token_type_ids"] = (
                     batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
                 )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
@@ -280,6 +282,15 @@ def evaluate(args, model, tokenizer, prefix=""):
         # multi-gpu eval
         if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
             model = torch.nn.DataParallel(model)
+        if args.ipex:
+            # Import Extension
+            import intel_pytorch_extension as ipex
+            print("Running with IPEX...")
+            if args.precision == 'bfloat16':
+                # Automatically mix precision
+                ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+                print("Running with bfloat16...")
+            model = model.to(ipex.DEVICE)
 
         # Eval!
         logger.info("***** Running evaluation {} *****".format(prefix))
@@ -289,17 +300,54 @@ def evaluate(args, model, tokenizer, prefix=""):
         nb_eval_steps = 0
         preds = None
         out_label_ids = None
+        # for warmup
+        for num_iter, batch in enumerate(eval_dataloader):
+            print("*", end="")
+            model.eval()
+            with torch.no_grad():
+                inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
+                if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                    inputs["token_type_ids"] = (
+                        batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
+                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
+                if args.ipex:
+                    inputs = {key:value.to(ipex.DEVICE) for key,value in inputs.items()}
+                if args.ipex and args.jit:
+                    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['labels']))
+                    else:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['labels']))
+                outputs = model(**inputs)
+                tmp_eval_loss, logits = outputs[:2]
+                eval_loss += tmp_eval_loss.mean().item()
+
+            if num_iter > args.num_warmup_iters:
+                print("Complete %d iters warmup" % args.num_warmup_iters)
+                break
+
+        total_batch = 0
+        total_time = 0
         for batch in tqdm(eval_dataloader, desc="Evaluating"):
             model.eval()
             batch = tuple(t.to(args.device) for t in batch)
 
             with torch.no_grad():
                 inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
-                if args.model_type != "distilbert":
+                if args.model_type not in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
                     inputs["token_type_ids"] = (
                         batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
                     )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
+                if args.ipex:
+                    inputs = {key:value.to(ipex.DEVICE) for key,value in inputs.items()}
+                if args.ipex and args.jit:
+                    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['labels']))
+                    else:
+                        model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], inputs['labels']))
+
+                tic = time.time()
                 outputs = model(**inputs)
+                total_time += time.time() - tic
                 tmp_eval_loss, logits = outputs[:2]
 
                 eval_loss += tmp_eval_loss.mean().item()
@@ -310,13 +358,19 @@ def evaluate(args, model, tokenizer, prefix=""):
             else:
                 preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
                 out_label_ids = np.append(out_label_ids, inputs["labels"].detach().cpu().numpy(), axis=0)
+            total_batch += 1
+            if total_batch >= args.num_iters:
+                break
 
         eval_loss = eval_loss / nb_eval_steps
         if args.output_mode == "classification":
             preds = np.argmax(preds, axis=1)
         elif args.output_mode == "regression":
             preds = np.squeeze(preds)
-        result = compute_metrics(eval_task, preds, out_label_ids)
+        if args.model_type in ["bart"]:
+            result = bart_compute_metrics(eval_task, preds, out_label_ids)
+        else:
+            result = compute_metrics(eval_task, preds, out_label_ids)
         results.update(result)
 
         output_eval_file = os.path.join(eval_output_dir, prefix, "eval_results.txt")
@@ -326,6 +380,10 @@ def evaluate(args, model, tokenizer, prefix=""):
                 logger.info("  %s = %s", key, str(result[key]))
                 writer.write("%s = %s\n" % (key, str(result[key])))
 
+        data_sum = args.eval_batch_size * total_batch
+        print(" time cost %s\n inference latency: %ss\n inference Throughput: %s samples/s\n " 
+              %(total_time, total_time / data_sum, data_sum / total_time))
+
     return results
 
 
@@ -370,7 +428,10 @@ def load_and_cache_examples(args, task, tokenizer, evaluate=False):
     # Convert to Tensors and build dataset
     all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
     all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
-    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
+    if args.model_type in ["distilbert", "roberta", "xlm-roberta", "xlm", "bart"]:
+        all_token_type_ids = torch.tensor([0 for f in features], dtype=torch.long)
+    else:
+        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
     if output_mode == "classification":
         all_labels = torch.tensor([f.label for f in features], dtype=torch.long)
     elif output_mode == "regression":
@@ -428,6 +489,7 @@ def main():
     # For now, let's merge all the sets of args into one,
     # but soon, we'll keep distinct sets of args, with a cleaner separation of concerns.
     args = argparse.Namespace(**vars(model_args), **vars(dataprocessing_args), **vars(training_args))
+    print(args)
 
     if (
         os.path.exists(args.output_dir)
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index 01f5600..7bc7813 100755
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -149,7 +149,7 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 if is_sklearn_available():
-    from .data import glue_compute_metrics, xnli_compute_metrics
+    from .data import glue_compute_metrics, xnli_compute_metrics, bart_compute_metrics
 
 
 # Modeling
diff --git a/src/transformers/data/__init__.py b/src/transformers/data/__init__.py
index 8d5f6b8..3f2e88d 100644
--- a/src/transformers/data/__init__.py
+++ b/src/transformers/data/__init__.py
@@ -24,4 +24,4 @@ from .processors import (
 
 
 if is_sklearn_available():
-    from .metrics import glue_compute_metrics, xnli_compute_metrics
+    from .metrics import glue_compute_metrics, xnli_compute_metrics, bart_compute_metrics
diff --git a/src/transformers/data/metrics/__init__.py b/src/transformers/data/metrics/__init__.py
index 6c29c23..5751304 100644
--- a/src/transformers/data/metrics/__init__.py
+++ b/src/transformers/data/metrics/__init__.py
@@ -83,3 +83,10 @@ if _has_sklearn:
             return {"acc": simple_accuracy(preds, labels)}
         else:
             raise KeyError(task_name)
+
+    def bart_compute_metrics(task_name, preds, labels):
+        assert len(preds) == len(labels)
+        if task_name == "mrpc":
+            return {"acc": simple_accuracy(preds, labels)}
+        else:
+            raise KeyError(task_name)
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index b48486d..8a998d3 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -73,3 +73,12 @@ class TrainingArguments:
         },
     )
     local_rank: int = field(default=-1, metadata={"help": "For distributed training: local_rank"})
+    num_warmup_iters: int = field(default=10, metadata={"help": "Warmup steps for evaluation benchmarking."})
+    num_iters: int = field(default=500, metadata={"help": "total iters for evaluation benchmarking."})
+    profiling: bool = field(default=False, metadata={"help": "Doing profiling on cpu."})
+    ipex: bool = field(default=False, metadata={"help": "Use Intel IPEX."})
+    jit: bool = field(default=False, metadata={"help": "Use jit optimize to do optimization."})
+    precision: str = field(default='float32',
+        metadata={"help": "Precision: 'float32' or 'bfloat16'."}
+    )
+
