diff --git a/rnn_translator/pytorch/seq2seq/data/config.py b/rnn_translator/pytorch/seq2seq/data/config.py
index 0582e04..bde99bc 100644
--- a/rnn_translator/pytorch/seq2seq/data/config.py
+++ b/rnn_translator/pytorch/seq2seq/data/config.py
@@ -12,8 +12,8 @@ VOCAB_FNAME = 'vocab.bpe.32000'
 
 # paths to source and target training files, relative to the data directory, it
 # should point to BPE-encoded files, generated by subword-nmt/apply_bpe.py
-SRC_TRAIN_FNAME = 'train.tok.clean.bpe.32000.en'
-TGT_TRAIN_FNAME = 'train.tok.clean.bpe.32000.de'
+SRC_TRAIN_FNAME = 'newstest2013.tok.clean.bpe.32000.en'
+TGT_TRAIN_FNAME = 'newstest2013.tok.clean.bpe.32000.de'
 
 # paths to source and target validation files, relative to the data directory,
 # it should point to BPE-encoded files, generated by subword-nmt/apply_bpe.py
diff --git a/rnn_translator/pytorch/seq2seq/inference/inference.py b/rnn_translator/pytorch/seq2seq/inference/inference.py
index 5ec3a4b..62b2ef3 100644
--- a/rnn_translator/pytorch/seq2seq/inference/inference.py
+++ b/rnn_translator/pytorch/seq2seq/inference/inference.py
@@ -117,7 +117,7 @@ class Translator:
         self.model.eval()
         torch.cuda.empty_cache()
 
-        output = self.evaluate(epoch, iteration, summary)
+        output, perf = self.evaluate(epoch, iteration, summary)
         output = output[:len(self.loader.dataset)]
         output = self.loader.dataset.unsort(output)
 
@@ -142,7 +142,7 @@ class Translator:
             dist.broadcast(break_training, 0)
             dist.broadcast(test_bleu, 0)
 
-        return test_bleu[0].item(), break_training[0].item()
+        return test_bleu[0].item(), break_training[0].item(), perf
 
     def evaluate(self, epoch, iteration, summary):
         """
@@ -251,8 +251,9 @@ class Translator:
             log += f'Total decoder iterations: {int(iterations.sum)}'
             log = ''.join(log)
             logging.info(log)
+        perf = tot_tok_per_sec.avg
 
-        return output
+        return output, perf
 
     def run_detokenizer(self, eval_path):
         """
diff --git a/rnn_translator/pytorch/seq2seq/train/trainer.py b/rnn_translator/pytorch/seq2seq/train/trainer.py
index c7a2b75..a65e33d 100644
--- a/rnn_translator/pytorch/seq2seq/train/trainer.py
+++ b/rnn_translator/pytorch/seq2seq/train/trainer.py
@@ -7,7 +7,7 @@ import numpy as np
 import torch
 import torch.optim
 import torch.utils.data
-from apex.parallel import DistributedDataParallel as DDP
+# from apex.parallel import DistributedDataParallel as DDP
 from mlperf_compliance import mlperf_log
 
 from seq2seq.train.fp_optimizers import Fp16Optimizer
@@ -98,8 +98,8 @@ class Seq2SeqTrainer:
         if math == 'fp16':
             self.model = self.model.half()
 
-        if distributed:
-            self.model = DDP(self.model)
+        # if distributed:
+        #     self.model = DDP(self.model)
 
         if math == 'fp16':
             self.fp_optimizer = Fp16Optimizer(self.model, grad_clip)
diff --git a/rnn_translator/pytorch/train.py b/rnn_translator/pytorch/train.py
index a4906c9..b0a697b 100644
--- a/rnn_translator/pytorch/train.py
+++ b/rnn_translator/pytorch/train.py
@@ -22,6 +22,7 @@ from seq2seq.inference.inference import Translator
 from seq2seq.models.gnmt import GNMT
 from seq2seq.train.smoothing import LabelSmoothing
 from seq2seq.utils import gnmt_print
+import intel_pytorch_extension as ipex
 
 
 def parse_args():
@@ -44,7 +45,7 @@ def parse_args():
 
     # dataset
     dataset = parser.add_argument_group('dataset setup')
-    dataset.add_argument('--dataset-dir', default='data/wmt16_de_en',
+    dataset.add_argument('--dataset-dir', default='../data',
                          help='path to the directory with training/test data')
     dataset.add_argument('--max-size', default=None, type=int,
                          help='use at most MAX_SIZE elements from training \
@@ -83,7 +84,7 @@ def parse_args():
     general = parser.add_argument_group('general setup')
     general.add_argument('--math', default='fp32', choices=['fp16', 'fp32'],
                          help='arithmetic type')
-    general.add_argument('--seed', default=None, type=int,
+    general.add_argument('--seed', default=1, type=int,
                          help='master seed for random number generators, if \
                          "seed" is undefined then the master seed will be \
                          sampled from random.SystemRandom()')
@@ -92,14 +93,14 @@ def parse_args():
                     help='run validation and test after every epoch')
     exclusive_group(group=general, name='env', default=False,
                     help='print info about execution env')
-    exclusive_group(group=general, name='cuda', default=True,
+    exclusive_group(group=general, name='cuda', default=False,
                     help='enables cuda')
-    exclusive_group(group=general, name='cudnn', default=True,
+    exclusive_group(group=general, name='cudnn', default=False,
                     help='enables cudnn')
 
     # training
     training = parser.add_argument_group('training setup')
-    training.add_argument('--train-batch-size', default=128, type=int,
+    training.add_argument('--train-batch-size', default=64, type=int,
                           help='training batch size per worker')
     training.add_argument('--train-global-batch-size', default=None, type=int,
                           help='global training batch size, this argument \
@@ -114,7 +115,7 @@ def parse_args():
                           help='training iter size, training loop will \
                           accumulate gradients over N iterations and execute \
                           optimizer every N steps')
-    training.add_argument('--epochs', default=8, type=int,
+    training.add_argument('--epochs', default=1, type=int,
                           help='max number of training epochs')
 
     training.add_argument('--grad-clip', default=5.0, type=float,
@@ -176,7 +177,7 @@ def parse_args():
 
     # test
     test = parser.add_argument_group('test setup')
-    test.add_argument('--test-batch-size', default=128, type=int,
+    test.add_argument('--test-batch-size', default=64, type=int,
                       help='batch size for test')
     test.add_argument('--max-length-test', default=150, type=int,
                       help='maximum sequence length for test \
@@ -226,6 +227,17 @@ def parse_args():
                              help='global rank of the process, do not set!')
     distributed.add_argument('--local_rank', default=0, type=int,
                              help='local rank of the process, do not set!')
+    
+    # ipex
+    ipe = parser.add_argument_group('ipex setup')
+    ipe.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+    ipe.add_argument('--precision', type=str, default="float32",
+                    help='precision, float32, bfloat16')
+    ipe.add_argument('--jit', action='store_true', default=False,
+                    help='enable ipex jit fusionpath')
+    ipe.add_argument('--inference', action='store_true', default=False,
+                    help='Inference only')                
 
     args = parser.parse_args()
 
@@ -236,12 +248,12 @@ def parse_args():
     return args
 
 
-def build_criterion(vocab_size, padding_idx, smoothing):
+def build_criterion(vocab_size, padding_idx, smoothing, device):
     if smoothing == 0.:
         logging.info(f'Building CrossEntropyLoss')
         loss_weight = torch.ones(vocab_size)
         loss_weight[padding_idx] = 0
-        criterion = nn.CrossEntropyLoss(weight=loss_weight, size_average=False)
+        criterion = nn.CrossEntropyLoss(weight=loss_weight, size_average=False).to(device)
         gnmt_print(key=mlperf_log.MODEL_HP_LOSS_FN,
                    value='Cross Entropy', sync=False)
     else:
@@ -263,7 +275,14 @@ def main():
     mlperf_log.LOGGER.propagate = False
 
     args = parse_args()
-    device = utils.set_device(args.cuda, args.local_rank)
+    if args.cuda:
+        torch.cuda.set_device(args.local_rank)
+        device = torch.device('cuda')
+    elif args.ipex:
+        device = ipex.DEVICE
+    else:
+        device = torch.device('cpu')
+        
     distributed = utils.init_distributed(args.cuda)
     gnmt_print(key=mlperf_log.RUN_START, sync=True)
     args.rank = utils.get_rank()
@@ -360,8 +379,13 @@ def main():
     batch_first = model.batch_first
 
     # define loss function (criterion) and optimizer
-    criterion = build_criterion(vocab_size, config.PAD, args.smoothing)
+    criterion = build_criterion(vocab_size, config.PAD, args.smoothing, device)
 
+    model = model.to(device)
+    if args.jit:
+        print("running jit fusion path\n")
+        # model = torch.jit.script(model)
+    
     opt_config = {'optimizer': args.optimizer, 'lr': args.lr}
     opt_config.update(literal_eval(args.optimizer_extra))
     logging.info(f'Training optimizer config: {opt_config}')
@@ -464,6 +488,16 @@ def main():
     break_training = False
     test_bleu = None
     gnmt_print(key=mlperf_log.TRAIN_LOOP, sync=True)
+    t = 0
+    
+    if args.ipex:
+        if args.precision=="bfloat16":
+            conf = ipex.AmpConf(torch.bfloat16)
+            print("running bf16 evalation step\n")
+        else:
+            conf = ipex.AmpConf(None)
+            print("running fp32 evalation step\n")
+    
     for epoch in range(args.start_epoch, args.epochs):
         logging.info(f'Starting epoch {epoch}')
         gnmt_print(key=mlperf_log.TRAIN_EPOCH,
@@ -472,6 +506,22 @@ def main():
         train_loader.sampler.set_epoch(epoch)
 
         trainer.epoch = epoch
+        
+        if args.inference:
+            if args.ipex:
+                with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                    test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+            else:
+                test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+            perf_log = []
+            perf_log += [f'Performance: Epoch: {epoch}']
+            perf_log += [f'Inference: {inf_perf:.0f} Tok/s']
+            logging.info('\t'.join(perf_log))
+            logging.info(f'Finished epoch {epoch}')
+            continue
+        
         train_loss, train_perf = trainer.optimize(train_loader)
 
         # evaluate on validation set
@@ -488,8 +538,14 @@ def main():
 
         if args.eval:
             gnmt_print(key=mlperf_log.EVAL_START, value=epoch, sync=True)
-            test_bleu, break_training = translator.run(calc_bleu=True,
-                                                       epoch=epoch)
+            if args.ipex:
+                with ipex.AutoMixPrecision(conf, running_mode="inference"):
+                    test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+            else:
+                test_bleu, break_training, inf_perf = translator.run(calc_bleu=False,
+                                                           epoch=epoch)
+            
             gnmt_print(key=mlperf_log.EVAL_ACCURACY,
                        value={"epoch": epoch, "value": round(test_bleu, 2)},
                        sync=False)
@@ -509,6 +565,7 @@ def main():
         perf_log += [f'Training: {train_perf:.0f} Tok/s']
         if args.eval:
             perf_log += [f'Validation: {val_perf:.0f} Tok/s']
+            perf_log += [f'Throughput: {inf_perf:.0f} Tok/s']
 
         if args.rank == 0:
             logging.info('\t'.join(acc_log))
@@ -518,8 +575,8 @@ def main():
         if break_training:
             break
 
-    gnmt_print(key=mlperf_log.RUN_STOP,
-               value={"success": bool(break_training)}, sync=True)
+    # gnmt_print(key=mlperf_log.RUN_STOP,
+    #            value={"success": bool(break_training)}, sync=True)
     gnmt_print(key=mlperf_log.RUN_FINAL, sync=False)
 
 
