diff --git a/speech_recognition/pytorch/decoder.py b/speech_recognition/pytorch/decoder.py
index 890407c..72e485b 100644
--- a/speech_recognition/pytorch/decoder.py
+++ b/speech_recognition/pytorch/decoder.py
@@ -48,7 +48,7 @@ class Decoder(object):
         return strings
 
     def _convert_to_string(self, sequence, sizes):
-        return ''.join([self.int_to_char[sequence[i]] for i in range(sizes)])
+        return ''.join([self.int_to_char[sequence[i].item()] for i in range(sizes)])
 
     def process_strings(self, sequences, remove_repetitions=False):
         """
diff --git a/speech_recognition/pytorch/eval_model.py b/speech_recognition/pytorch/eval_model.py
index 0b75604..7db9c48 100644
--- a/speech_recognition/pytorch/eval_model.py
+++ b/speech_recognition/pytorch/eval_model.py
@@ -7,6 +7,7 @@ from warpctc_pytorch import CTCLoss
 import torch.nn.functional as F
 
 import sys
+import time
 ### Import Data Utils ###
 sys.path.append('../')
 
@@ -16,11 +17,13 @@ from decoder import GreedyDecoder
 from model import DeepSpeech, supported_rnns
 from params import cuda
 
-def eval_model(model, test_loader, decoder):
+def eval_model(model, test_loader, decoder, device="cpu", batch_time=None):
         start_iter = 0  # Reset start iteration for next epoch
         total_cer, total_wer = 0, 0
         model.eval()
         for i, (data) in enumerate(test_loader):  # test
+            if i >= 5 and batch_time is not None:
+                start = time.time()
             inputs, targets, input_percentages, target_sizes = data
 
             inputs = Variable(inputs, volatile=True)
@@ -32,8 +35,10 @@ def eval_model(model, test_loader, decoder):
                 split_targets.append(targets[offset:offset + size])
                 offset += size
 
-            if cuda:
-                inputs = inputs.cuda()
+            # if cuda:
+            #     inputs = inputs.cuda()
+
+            inputs = inputs.to(device)
 
             out = model(inputs)
             out = out.transpose(0, 1)  # TxNxH
@@ -42,6 +47,8 @@ def eval_model(model, test_loader, decoder):
 
             decoded_output = decoder.decode(out.data, sizes)
             target_strings = decoder.process_strings(decoder.convert_to_strings(split_targets))
+            if i >= 5 and batch_time is not None:
+                batch_time.update(time.time() - start)
             wer, cer = 0, 0
             for x in range(len(target_strings)):
                 wer += decoder.wer(decoded_output[x], target_strings[x]) / float(len(target_strings[x].split()))
@@ -49,7 +56,7 @@ def eval_model(model, test_loader, decoder):
             total_cer += cer
             total_wer += wer
 
-            if cuda:
+            if device == "cuda":
                 torch.cuda.synchronize()
             del out
         wer = total_wer / len(test_loader.dataset)
diff --git a/speech_recognition/pytorch/model.py b/speech_recognition/pytorch/model.py
index 0714e86..9967caa 100644
--- a/speech_recognition/pytorch/model.py
+++ b/speech_recognition/pytorch/model.py
@@ -140,12 +140,12 @@ class DeepSpeech(nn.Module):
         num_classes = len(self._labels)
 
         self.conv = nn.Sequential(
-            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2)),
+            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 5)),
             nn.BatchNorm2d(32),
-            nn.Hardtanh(0, 20, inplace=True),
-            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1)),
+            nn.Hardtanh(0.0, 20.0, inplace=True),
+            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(0, 5)),
             nn.BatchNorm2d(32),
-            nn.Hardtanh(0, 20, inplace=True)
+            nn.Hardtanh(0.0, 20.0, inplace=True)
         )
         # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1
         rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)
diff --git a/speech_recognition/pytorch/train.py b/speech_recognition/pytorch/train.py
index d9e7036..aea23bd 100644
--- a/speech_recognition/pytorch/train.py
+++ b/speech_recognition/pytorch/train.py
@@ -9,6 +9,7 @@ import sys
 import numpy as np
 
 import torch
+import intel_pytorch_extension as ipex
 from torch.autograd import Variable
 from warpctc_pytorch import CTCLoss
 
@@ -41,6 +42,18 @@ parser.add_argument('--seed', default=0xdeadbeef, type=int, help='Random Seed')
 parser.add_argument('--acc', default=23.0, type=float, help='Target WER')
 
 parser.add_argument('--start_epoch', default=-1, type=int, help='Number of epochs at which to start from')
+parser.add_argument('--batch_size', default=1, type=int, help='batch size to inference')
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+parser.add_argument('--precision', type=str, default="float32",
+                    help='precision, float32, bfloat16')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='enable ipex jit fusionpath')
+parser.add_argument('--cuda', action='store_true', default=False,
+                    help='use CUDA')
+parser.add_argument('--evaluate', action='store_true', default=False,
+                    help='evaluate only')
+
 
 def to_np(x):
     return x.data.cpu().numpy()
@@ -66,9 +79,18 @@ class AverageMeter(object):
 
 def main():
     args = parser.parse_args()
+    params.batch_size = args.batch_size
 
     torch.manual_seed(args.seed)
-    torch.cuda.manual_seed_all(args.seed)
+    if args.cuda:
+        torch.cuda.manual_seed_all(args.seed)
+        device = "cuda"
+    elif args.ipex:
+        params.cuda = False
+        device = ipex.DEVICE
+    else:
+        params.cuda = False
+        device = "cpu"
 
     if params.rnn_type == 'gru' and params.rnn_act_type != 'tanh':
       print("ERROR: GRU does not currently support activations other than tanh")
@@ -106,12 +128,13 @@ def main():
                       noise_prob=params.noise_prob,
                       noise_levels=(params.noise_min, params.noise_max))
 
-    train_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath=params.train_manifest, labels=labels,
-                                       normalize=True, augment=params.augment)
+    if not args.evaluate:
+        train_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath=params.train_manifest, labels=labels,
+                                           normalize=True, augment=params.augment)
+        train_loader = AudioDataLoader(train_dataset, batch_size=params.batch_size,
+                                       num_workers=1)
     test_dataset = SpectrogramDataset(audio_conf=audio_conf, manifest_filepath=params.val_manifest, labels=labels,
                                       normalize=True, augment=False)
-    train_loader = AudioDataLoader(train_dataset, batch_size=params.batch_size,
-                                   num_workers=1)
     test_loader = AudioDataLoader(test_dataset, batch_size=params.batch_size,
                                   num_workers=1)
 
@@ -133,6 +156,16 @@ def main():
                                 weight_decay = params.l2)
     decoder = GreedyDecoder(labels)
 
+    if args.evaluate:
+        model.to(device)
+        # if args.jit:
+        #     model = torch.jit.script(model)
+        #     print(model)
+        batch_time = AverageMeter()
+        wer, cer = eval_model(model, test_loader, decoder, device, batch_time)
+        print('inference Throughput: %0.3f samples/s' % (params.batch_size / batch_time.avg))
+        exit(0)
+
     if args.continue_from:
         print("Loading checkpoint model %s" % args.continue_from)
         package = torch.load(args.continue_from)
