diff --git a/pytorch/eval.py b/pytorch/eval.py
index eff3618..59709f3 100644
--- a/pytorch/eval.py
+++ b/pytorch/eval.py
@@ -2,9 +2,11 @@
 import argparse
 import time
 import math
-import os, sys
+import os
+import sys
 
 import torch
+import intel_pytorch_extension as ipex
 
 from data_utils import get_lm_corpus
 from mem_transformer import MemTransformerLM
@@ -37,10 +39,21 @@ parser.add_argument('--no_log', action='store_true',
                     help='do not log the eval result')
 parser.add_argument('--same_length', action='store_true',
                     help='set same length attention with masking')
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use intel pytorch extension')
+parser.add_argument('--precision', type=str, default="float32",
+                    help='precision, float32, bfloat16')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='enable ipex jit fusionpath')
 args = parser.parse_args()
 assert args.ext_len >= 0, 'extended context length must be non-negative'
 
-device = torch.device("cuda" if args.cuda else "cpu")
+if args.cuda:
+    device = torch.device("cuda")
+elif args.ipex:
+    device = ipex.DEVICE
+else:
+    device = torch.device("cpu")
 
 # Get logger
 logging = get_logger(os.path.join(args.work_dir, 'log.txt'),
@@ -51,9 +64,9 @@ corpus = get_lm_corpus(args.data, args.dataset)
 ntokens = len(corpus.vocab)
 
 va_iter = corpus.get_iterator('valid', args.batch_size, args.tgt_len,
-    device=device, ext_len=args.ext_len)
+                              device=device, ext_len=args.ext_len)
 te_iter = corpus.get_iterator('test', args.batch_size, args.tgt_len,
-    device=device, ext_len=args.ext_len)
+                              device=device, ext_len=args.ext_len)
 
 # Load the best saved model.
 with open(os.path.join(args.work_dir, 'model.pt'), 'rb') as f:
@@ -62,7 +75,7 @@ model.backward_compatible()
 model = model.to(device)
 
 logging('Evaluating with bsz {} tgt_len {} ext_len {} mem_len {} clamp_len {}'.format(
-       args.batch_size, args.tgt_len, args.ext_len, args.mem_len, args.clamp_len))
+    args.batch_size, args.tgt_len, args.ext_len, args.mem_len, args.clamp_len))
 
 model.reset_length(args.tgt_len, args.ext_len, args.mem_len)
 if args.clamp_len > 0:
@@ -70,27 +83,50 @@ if args.clamp_len > 0:
 if args.same_length:
     model.same_length = True
 
+if args.jit:
+    logging('running jit fusion path')
+
 ###############################################################################
 # Evaluation code
 ###############################################################################
-def evaluate(eval_iter):
+
+
+def evaluate(eval_iter, model):
     # Turn on evaluation mode which disables dropout.
     model.eval()
+    if args.ipex:
+        if args.precision == "bfloat16":
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+            print('Running with bfloat16...')
+        else:
+            print("running fp32 evalation step\n")
     total_len, total_loss = 0, 0.
     start_time = time.time()
     with torch.no_grad():
         mems = tuple()
         for idx, (data, target, seq_len) in enumerate(eval_iter):
-            ret = model(data, target, *mems)
-            loss, mems = ret[0], ret[1:]
-            loss = loss.mean()
-            total_loss += seq_len * loss.item()
-            total_len += seq_len
+            if args.ipex:
+                data = data.to(device=ipex.DEVICE)
+                target = target.to(device=ipex.DEVICE)
+                if args.jit:
+                    model = torch.jit.trace(model, data, target)
+                ret = model(data, target, *mems)
+                loss, mems = ret[0], ret[1:]
+                loss = loss.mean()
+                total_loss += seq_len * loss.item()
+                total_len += seq_len
+            else:
+                ret = model(data, target, *mems)
+                loss, mems = ret[0], ret[1:]
+                loss = loss.mean()
+                total_loss += seq_len * loss.item()
+                total_len += seq_len
         total_time = time.time() - start_time
-    logging('Time : {:.2f}s, {:.2f}ms/segment'.format(
-            total_time, 1000 * total_time / (idx+1)))
+    logging('Time : {:.2f}s\ninference Throughput: {:.2f} segments/s'.format(
+            total_time, (idx+1) * args.batch_size / total_time))
     return total_loss / total_len
 
+
 # Run on test data.
 if args.split == 'all':
     test_loss = evaluate(te_iter)
@@ -99,9 +135,10 @@ elif args.split == 'valid':
     valid_loss = evaluate(va_iter)
     test_loss = None
 elif args.split == 'test':
-    test_loss = evaluate(te_iter)
+    test_loss = evaluate(te_iter, model)
     valid_loss = None
 
+
 def format_log(loss, split):
     if args.dataset in ['enwik8', 'text8']:
         log_str = '| {0} loss {1:5.2f} | {0} bpc {2:9.5f} '.format(
@@ -111,6 +148,7 @@ def format_log(loss, split):
             split, loss, math.exp(loss))
     return log_str
 
+
 log_str = ''
 if valid_loss is not None:
     log_str += format_log(valid_loss, 'valid')
diff --git a/pytorch/mem_transformer.py b/pytorch/mem_transformer.py
index 45147df..18b1624 100644
--- a/pytorch/mem_transformer.py
+++ b/pytorch/mem_transformer.py
@@ -196,7 +196,8 @@ class RelMultiHeadAttn(nn.Module):
                                device=x.device, dtype=x.dtype)
         x_padded = torch.cat([zero_pad, x], dim=1)
 
-        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])
+        x_padded = x_padded.reshape(x.size(1) + 1, x.size(0), *x.size()[2:])
+        # x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])
 
         x = x_padded[1:].view_as(x)
 
diff --git a/pytorch/run_wt103_base.sh b/pytorch/run_wt103_base.sh
index 22c7550..281a4a7 100644
--- a/pytorch/run_wt103_base.sh
+++ b/pytorch/run_wt103_base.sh
@@ -3,7 +3,6 @@
 if [[ $1 == 'train' ]]; then
     echo 'Run training...'
     python train.py \
-        --cuda \
         --data ../data/wikitext-103/ \
         --dataset wt103 \
         --adaptive \
@@ -17,18 +16,14 @@ if [[ $1 == 'train' ]]; then
         --optim adam \
         --lr 0.00025 \
         --warmup_step 0 \
-        --max_step 200000 \
         --tgt_len 150 \
         --mem_len 150 \
         --eval_tgt_len 150 \
-        --batch_size 60 \
-        --multi_gpu \
-        --gpu0_bsz 4 \
+        --batch_size 20 \
         ${@:2}
 elif [[ $1 == 'eval' ]]; then
     echo 'Run evaluation...'
     python eval.py \
-        --cuda \
         --data ../data/wikitext-103/ \
         --dataset wt103 \
         --tgt_len 64 \
diff --git a/pytorch/utils/proj_adaptive_softmax.py b/pytorch/utils/proj_adaptive_softmax.py
index a0fbfeb..6576207 100644
--- a/pytorch/utils/proj_adaptive_softmax.py
+++ b/pytorch/utils/proj_adaptive_softmax.py
@@ -6,8 +6,8 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-CUDA_MAJOR = int(torch.version.cuda.split('.')[0])
-CUDA_MINOR = int(torch.version.cuda.split('.')[1])
+# CUDA_MAJOR = int(torch.version.cuda.split('.')[0])
+# CUDA_MINOR = int(torch.version.cuda.split('.')[1])
 
 class ProjectedAdaptiveLogSoftmax(nn.Module):
     def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,
