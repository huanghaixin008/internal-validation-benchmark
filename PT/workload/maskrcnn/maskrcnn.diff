diff --git a/object_detection/pytorch/maskrcnn_benchmark/data/build.py b/object_detection/pytorch/maskrcnn_benchmark/data/build.py
index d3a96c2..d00ec6b 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/data/build.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/data/build.py
@@ -125,7 +125,7 @@ def make_data_loader(cfg, is_train=True, is_distributed=False, start_iter=0):
         "of GPUs ({}) used.".format(images_per_batch, num_gpus)
         images_per_gpu = images_per_batch // num_gpus
         shuffle = False if not is_distributed else True
-        num_iters = None
+        num_iters = cfg.SOLVER.MAX_ITER if cfg.SOLVER.MAX_ITER > 0 else None
         start_iter = 0
 
     if images_per_gpu > 1:
diff --git a/object_detection/pytorch/maskrcnn_benchmark/data/datasets/coco.py b/object_detection/pytorch/maskrcnn_benchmark/data/datasets/coco.py
index f0c8c25..cbf132e 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/data/datasets/coco.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/data/datasets/coco.py
@@ -61,7 +61,7 @@ class COCODataset(torchvision.datasets.coco.CocoDetection):
             v: k for k, v in self.json_category_id_to_contiguous_id.items()
         }
         self.id_to_img_map = {k: v for k, v in enumerate(self.ids)}
-        self.transforms = transforms
+        self._transforms = transforms
 
     def __getitem__(self, idx):
         img, anno = super(COCODataset, self).__getitem__(idx)
@@ -90,8 +90,8 @@ class COCODataset(torchvision.datasets.coco.CocoDetection):
 
         target = target.clip_to_image(remove_empty=True)
 
-        if self.transforms is not None:
-            img, target = self.transforms(img, target)
+        if self._transforms is not None:
+            img, target = self._transforms(img, target)
 
         return img, target, idx
 
diff --git a/object_detection/pytorch/maskrcnn_benchmark/engine/inference.py b/object_detection/pytorch/maskrcnn_benchmark/engine/inference.py
index 29576a5..b345c62 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/engine/inference.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/engine/inference.py
@@ -11,21 +11,49 @@ from maskrcnn_benchmark.data.datasets.evaluation import evaluate
 from ..utils.comm import is_main_process
 from ..utils.comm import all_gather
 from ..utils.comm import synchronize
+from ..utils.timer import Timer, get_time_str
 
 
-def compute_on_dataset(model, data_loader, device):
+def compute_on_dataset(model, data_loader, device, log_path=None, timer=None):
     model.eval()
     results_dict = {}
     cpu_device = torch.device("cpu")
-    for i, batch in enumerate(tqdm(data_loader)):
-        images, targets, image_ids = batch
-        images = images.to(device)
-        with torch.no_grad():
-            output = model(images)
-            output = [o.to(cpu_device) for o in output]
-        results_dict.update(
-            {img_id: result for img_id, result in zip(image_ids, output)}
-        )
+    if log_path is not None:
+        for i, batch in enumerate(tqdm(data_loader)):
+            images, targets, image_ids = batch
+            images = images.to(device)
+            if i == 5:
+                with torch.autograd.profiler.profile() as prof:
+                    with torch.no_grad():
+                        output = model(images)
+                        output = [o.to(cpu_device) for o in output]
+                    results_dict.update(
+                        {img_id: result for img_id, result in zip(image_ids, output)}
+                    )
+                prof.export_chrome_trace(os.path.join(log_path, 'result_' + str(i) + '.json'))
+                break
+            with torch.no_grad():
+                output = model(images)
+                output = [o.to(cpu_device) for o in output]
+            results_dict.update(
+                {img_id: result for img_id, result in zip(image_ids, output)}
+            )
+    else:
+        for i, batch in enumerate(tqdm(data_loader)):
+            images, targets, image_ids = batch
+            images = images.to(device)
+            with torch.no_grad():
+                if timer:
+                    timer.tic()
+                output = model(images)
+                if timer:
+                    if device.type == 'cuda':
+                        torch.cuda.synchronize()
+                    timer.toc()
+                output = [o.to(cpu_device) for o in output]
+            results_dict.update(
+                {img_id: result for img_id, result in zip(image_ids, output)}
+            )
     return results_dict
 
 
@@ -61,6 +89,9 @@ def inference(
         expected_results=(),
         expected_results_sigma_tol=4,
         output_folder=None,
+        log_path=None,
+        warmup=0,
+        performance_only=False
 ):
     # convert to a torch.device for efficiency
     device = torch.device(device)
@@ -71,21 +102,41 @@ def inference(
     )
     logger = logging.getLogger("maskrcnn_benchmark.inference")
     dataset = data_loader.dataset
+    if hasattr(data_loader.batch_sampler, "batch_sampler"):
+        batch_size = data_loader.batch_sampler.batch_sampler.batch_size
+    else:
+        batch_size = data_loader.batch_sampler.batch_size
     logger.info("Start evaluation on {} dataset({} images).".format(dataset_name, len(dataset)))
-    start_time = time.time()
-    predictions = compute_on_dataset(model, data_loader, device)
+    total_timer = Timer()
+    inference_timer = Timer(warmup)
+    total_timer.tic()
+    predictions = compute_on_dataset(model, data_loader, device, log_path, inference_timer)
     # wait for all processes to complete before measuring the time
     synchronize()
-    total_time = time.time() - start_time
-    total_time_str = str(datetime.timedelta(seconds=total_time))
+    total_time = total_timer.toc(average=False)
+    total_time_str = get_time_str(total_time)
+    logger.info(
+        "Total Total time: {} ({} s / img per device, on {} devices)".format(
+            total_time_str,  total_time / batch_size * len(data_loader), num_devices
+        )
+    )
+    total_infer_time = get_time_str(inference_timer.total_time)
     logger.info(
-        "Total inference time: {} ({} s / img per device, on {} devices)".format(
-            total_time_str, total_time * num_devices / len(dataset), num_devices
+        "Model inference latency: {} ({} s / img per device, on {} devices)".format(
+            total_infer_time,
+            inference_timer.average_time / batch_size,
+            num_devices,
+        )
+    )
+    logger.info(
+        "Model inference Throughput: {} imgs / s per device, on {} devices".format(
+            batch_size / inference_timer.average_time,
+            num_devices,
         )
     )
 
     predictions = _accumulate_predictions_from_multiple_gpus(predictions)
-    if not is_main_process():
+    if not is_main_process() or performance_only:
         return
 
     if output_folder:
diff --git a/object_detection/pytorch/maskrcnn_benchmark/layers/roi_align.py b/object_detection/pytorch/maskrcnn_benchmark/layers/roi_align.py
index 170c8f1..0031aaa 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/layers/roi_align.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/layers/roi_align.py
@@ -55,9 +55,14 @@ class ROIAlign(nn.Module):
         self.sampling_ratio = sampling_ratio
 
     def forward(self, input, rois):
-        return roi_align(
-            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio
-        )
+        if input.device == torch.device("dpcpp"):
+            return roi_align(
+                input.to("cpu"), rois, self.output_size, self.spatial_scale, self.sampling_ratio
+            ).to("dpcpp")
+        else:
+            return roi_align(
+                input, rois, self.output_size, self.spatial_scale, self.sampling_ratio
+            )
 
     def __repr__(self):
         tmpstr = self.__class__.__name__ + "("
diff --git a/object_detection/pytorch/maskrcnn_benchmark/structures/boxlist_ops.py b/object_detection/pytorch/maskrcnn_benchmark/structures/boxlist_ops.py
index dc51212..c0e6f7f 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/structures/boxlist_ops.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/structures/boxlist_ops.py
@@ -24,7 +24,10 @@ def boxlist_nms(boxlist, nms_thresh, max_proposals=-1, score_field="scores"):
     boxlist = boxlist.convert("xyxy")
     boxes = boxlist.bbox
     score = boxlist.get_field(score_field)
-    keep = _box_nms(boxes, score, nms_thresh)
+    if boxes.device == torch.device("dpcpp"):
+        keep = _box_nms(boxes.to("cpu"), score.to("cpu"), nms_thresh).to("dpcpp")
+    else:
+        keep = _box_nms(boxes, score, nms_thresh)
     if max_proposals > 0:
         keep = keep[: max_proposals]
     boxlist = boxlist[keep]
diff --git a/object_detection/pytorch/maskrcnn_benchmark/structures/segmentation_mask.py b/object_detection/pytorch/maskrcnn_benchmark/structures/segmentation_mask.py
index ba1290b..621675c 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/structures/segmentation_mask.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/structures/segmentation_mask.py
@@ -195,7 +195,8 @@ class SegmentationMask(object):
         else:
             # advanced indexing on a single dimension
             selected_polygons = []
-            if isinstance(item, torch.Tensor) and item.dtype == torch.uint8:
+            if isinstance(item, torch.Tensor) and \
+                    (item.dtype == torch.uint8 or item.dtype == torch.bool):
                 item = item.nonzero()
                 item = item.squeeze(1) if item.numel() > 0 else item
                 item = item.tolist()
diff --git a/object_detection/pytorch/maskrcnn_benchmark/utils/model_zoo.py b/object_detection/pytorch/maskrcnn_benchmark/utils/model_zoo.py
index 7a0ebb3..94cf2f9 100644
--- a/object_detection/pytorch/maskrcnn_benchmark/utils/model_zoo.py
+++ b/object_detection/pytorch/maskrcnn_benchmark/utils/model_zoo.py
@@ -2,9 +2,12 @@
 import os
 import sys
 
-from torch.utils.model_zoo import _download_url_to_file
-from torch.utils.model_zoo import urlparse
-from torch.utils.model_zoo import HASH_REGEX
+# from torch.utils.model_zoo import _download_url_to_file
+from torch.hub import download_url_to_file
+# from torch.utils.model_zoo import urlparse
+from torch.hub import urlparse
+# from torch.utils.model_zoo import HASH_REGEX
+from torch.hub import HASH_REGEX
 
 from maskrcnn_benchmark.utils.comm import is_main_process
 from maskrcnn_benchmark.utils.comm import synchronize
@@ -51,6 +54,6 @@ def cache_url(url, model_dir=None, progress=True):
             # if the hash_prefix is less than 6 characters
             if len(hash_prefix) < 6:
                 hash_prefix = None
-        _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
+        download_url_to_file(url, cached_file, hash_prefix, progress=progress)
     synchronize()
     return cached_file
diff --git a/object_detection/pytorch/maskrcnn_benchmark/utils/timer.py b/object_detection/pytorch/maskrcnn_benchmark/utils/timer.py
new file mode 100644
index 0000000..3770d40
--- /dev/null
+++ b/object_detection/pytorch/maskrcnn_benchmark/utils/timer.py
@@ -0,0 +1,48 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+
+
+import time
+import datetime
+
+
+class Timer(object):
+    def __init__(self, warmup=0):
+        self.reset()
+        self.warmup = warmup
+
+    @property
+    def average_time(self):
+        return (self.total_time / (self.calls - self.warmup)) if self.calls > self.warmup else 0.0
+
+    def tic(self):
+        # using time.time instead of time.clock because time time.clock
+        # does not normalize for multithreading
+        self.start_time = time.time()
+
+    def toc(self, average=True):
+        self.add(time.time() - self.start_time)
+        if average:
+            return self.average_time
+        else:
+            return self.diff
+
+    def add(self, time_diff):
+        self.diff = time_diff
+        self.calls += 1
+        if self.calls > self.warmup:
+            self.total_time += self.diff
+
+    def reset(self):
+        self.total_time = 0.0
+        self.calls = 0
+        self.start_time = 0.0
+        self.diff = 0.0
+
+    def avg_time_str(self):
+        time_str = str(datetime.timedelta(seconds=self.average_time))
+        return time_str
+
+
+def get_time_str(time_diff):
+    time_str = str(datetime.timedelta(seconds=time_diff))
+    return time_str
\ No newline at end of file
diff --git a/object_detection/pytorch/tools/test_net.py b/object_detection/pytorch/tools/test_net.py
index d0acd28..8791ccd 100644
--- a/object_detection/pytorch/tools/test_net.py
+++ b/object_detection/pytorch/tools/test_net.py
@@ -33,6 +33,14 @@ def main():
         default=None,
         nargs=argparse.REMAINDER,
     )
+    parser.add_argument('--profile_log', type=str, default=None,
+                        help="folder to save profiling result, None means don't profile")
+    parser.add_argument('--precision', type=str, default='float32',
+                        help="precision, 'float32' or 'bfloat16'")
+    parser.add_argument('--warmup', type=int, default=5,
+                        help='num of warmup')
+    # parser.add_argument('--jit', action='store_true', default=False,
+    #                     help='convert model to script model')
 
     args = parser.parse_args()
 
@@ -55,6 +63,14 @@ def main():
     logger.info("Using {} GPUs".format(num_gpus))
     logger.info(cfg)
 
+    if cfg.MODEL.DEVICE == "dpcpp":
+        import intel_pytorch_extension as ipex
+        if args.precision == "bfloat16":
+            # Automatically mix precision
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+            print('Run model with bfloat16...')
+
+
     logger.info("Collecting env info (might take some time)")
     logger.info("\n" + collect_env_info())
 
@@ -65,6 +81,9 @@ def main():
     checkpointer = DetectronCheckpointer(cfg, model, save_dir=output_dir)
     _ = checkpointer.load(cfg.MODEL.WEIGHT)
 
+    # if args.jit:
+    #     model = torch.jit.script(model)
+
     iou_types = ("bbox",)
     if cfg.MODEL.MASK_ON:
         iou_types = iou_types + ("segm",)
@@ -89,6 +108,9 @@ def main():
             expected_results=cfg.TEST.EXPECTED_RESULTS,
             expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,
             output_folder=output_folder,
+            log_path=args.profile_log,
+            warmup=args.warmup,
+            performance_only=cfg.PER_EPOCH_EVAL
         )
         synchronize()
 
diff --git a/object_detection/run_and_time.sh b/object_detection/run_and_time.sh
index f852653..cdb39c8 100755
--- a/object_detection/run_and_time.sh
+++ b/object_detection/run_and_time.sh
@@ -1,11 +1,18 @@
 #!/bin/bash
 
-# Runs benchmark and reports time to convergence
+# Runs benchmark
 
 pushd pytorch
+BATCH_SIZE=1
+ITERATIONS=10
+PERFORMANCE_ONLY=True
 
-# Single GPU training
-time python tools/train_mlperf.py --config-file "configs/e2e_mask_rcnn_R_50_FPN_1x.yaml" \
-       SOLVER.IMS_PER_BATCH 2 TEST.IMS_PER_BATCH 1 SOLVER.MAX_ITER 720000 SOLVER.STEPS "(480000, 640000)" SOLVER.BASE_LR 0.0025
+# # Single GPU training
+# time python tools/train_mlperf.py --config-file "configs/e2e_mask_rcnn_R_50_FPN_1x.yaml" \
+#        SOLVER.IMS_PER_BATCH 2 TEST.IMS_PER_BATCH 1 SOLVER.MAX_ITER 720000 SOLVER.STEPS "(480000, 640000)" SOLVER.BASE_LR 0.0025
+
+time python tools/test_net.py --config-file "configs/e2e_mask_rcnn_R_50_FPN_1x.yaml" \
+        DATALOADER.NUM_WORKERS 1 SOLVER.MAX_ITER ${ITERATIONS} TEST.IMS_PER_BATCH $BATCH_SIZE MODEL.DEVICE dpcpp \
+        PER_EPOCH_EVAL $PERFORMANCE_ONLY
        
 popd
