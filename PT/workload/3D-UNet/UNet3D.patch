diff --git a/pytorch3dunet/predict.py b/pytorch3dunet/predict.py
index 36d06de..b715812 100755
--- a/pytorch3dunet/predict.py
+++ b/pytorch3dunet/predict.py
@@ -1,5 +1,6 @@
 import importlib
 import os
+import time
 
 import torch
 import torch.nn as nn
@@ -59,16 +60,34 @@ def main():
 
     logger.info(f"Sending the model to '{device}'")
     model = model.to(device)
-
     logger.info('Loading HDF5 datasets...')
-    for test_loader in get_test_loaders(config):
+    data_sum = 0
+    time_sum = 0
+    assert 'loaders' in config, 'Could not find data loaders configuration'
+    loaders_config = config['loaders']
+    batch_size = loaders_config.get('batch_size', 1)
+    if config['ipex']:
+        import intel_pytorch_extension as ipex
+        print('Running with IPEX...')
+        model = model.to(ipex.DEVICE)
+        if config['precision'] == 'bfloat16':
+            # Automatically mix precision
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+            print('Running with bfloat16...')
+
+    for index, test_loader in enumerate(get_test_loaders(config)):
         logger.info(f"Processing '{test_loader.dataset.file_path}'...")
 
         output_file = _get_output_file(test_loader.dataset)
 
         predictor = _get_predictor(model, test_loader, output_file, config)
         # run the model prediction on the entire dataset and save to the 'output_file' H5
-        predictor.predict()
+        iter_time, iter_data = predictor.predict()
+        data_sum += iter_data
+        time_sum += iter_time
+
+    print(" time cost %s\n inference latency: %.2fs\n inference Throughput: %.2f images/s\n " 
+            %(time_sum, time_sum /data_sum, data_sum / time_sum))
 
 
 if __name__ == '__main__':
diff --git a/pytorch3dunet/unet3d/config.py b/pytorch3dunet/unet3d/config.py
index b3c0683..3cc4deb 100644
--- a/pytorch3dunet/unet3d/config.py
+++ b/pytorch3dunet/unet3d/config.py
@@ -11,8 +11,16 @@ logger = utils.get_logger('ConfigLoader')
 def load_config():
     parser = argparse.ArgumentParser(description='UNet3D')
     parser.add_argument('--config', type=str, help='Path to the YAML config file', required=True)
+    parser.add_argument('--ipex', action='store_true', help='Use intel pytorch extension.')
+    parser.add_argument('--jit', action='store_true', help='enable jit optimization in intel pytorch extension.')
+    parser.add_argument('--profiling', action='store_true', help='Do profiling.')
+    parser.add_argument('--batch_size', type=int, default=1, help='input batch size.') 
+    parser.add_argument('--num_warmup', type=int, default=10, help='num of warmup, default is 10.')
+    parser.add_argument('--precision', type=str, default='float32', help='data type precision, default is float32.')
+
     args = parser.parse_args()
     config = _load_config_yaml(args.config)
+    print(config)
     # Get a device to train on
     device_str = config.get('device', None)
     if device_str is not None:
@@ -26,6 +34,12 @@ def load_config():
 
     device = torch.device(device_str)
     config['device'] = device
+    config['loaders']['batch_size'] = args.batch_size
+    config['ipex'] = args.ipex
+    config['num_warmup'] = args.num_warmup
+    config['jit'] = args.jit
+    config['profiling'] = args.profiling
+    config['precision'] = args.precision
     return config
 
 
diff --git a/pytorch3dunet/unet3d/predictor.py b/pytorch3dunet/unet3d/predictor.py
index 84c19ac..28bbf99 100644
--- a/pytorch3dunet/unet3d/predictor.py
+++ b/pytorch3dunet/unet3d/predictor.py
@@ -10,6 +10,11 @@ from pytorch3dunet.datasets.utils import SliceBuilder
 from pytorch3dunet.unet3d.utils import get_logger
 from pytorch3dunet.unet3d.utils import remove_halo
 
+try:
+    import intel_pytorch_extension as ipex
+except:
+    print("No module: intel_pytorch_extension")
+
 logger = get_logger('UNet3DPredictor')
 
 
@@ -89,64 +94,44 @@ class StandardPredictor(_AbstractPredictor):
         self._validate_halo(patch_halo, self.config['loaders']['test']['slice_builder'])
         logger.info(f'Using patch_halo: {patch_halo}')
 
-        # create destination H5 file
-        h5_output_file = h5py.File(self.output_file, 'w')
-        # allocate prediction and normalization arrays
-        logger.info('Allocating prediction and normalization arrays...')
-        prediction_maps, normalization_masks = self._allocate_prediction_maps(prediction_maps_shape,
-                                                                              output_heads, h5_output_file)
-
         # Sets the module in evaluation mode explicitly (necessary for batchnorm/dropout layers if present)
         self.model.eval()
         # Set the `testing=true` flag otherwise the final Softmax/Sigmoid won't be applied!
         self.model.testing = True
         # Run predictions on the entire input dataset
+        tic = 0
+        toc = 0
+        total_samples = 0
+
         with torch.no_grad():
-            for batch, indices in self.loader:
+            for index, (batch, indices) in enumerate(self.loader):
                 # send batch to device
-                batch = batch.to(device)
+                if self.config['ipex']:
+                    batch = batch.to(ipex.DEVICE)
+                # transfer to jit model at the first iter
+                if index==0 and self.config['jit']:
+                    self.model = torch.jit.trace(self.model, batch)
+
+                if index > self.config['num_warmup']:
+                    tic = time.time()
 
                 # forward pass
-                predictions = self.model(batch)
+                if self.config['profiling'] and index > self.config['num_warmup']:
+                    with torch.autograd.profiler.profile(use_cuda=False) as prof:
+                        predictions = self.model(batch)
+                else:
+                    predictions = self.model(batch)
 
                 # wrap predictions into a list if there is only one output head from the network
                 if output_heads == 1:
                     predictions = [predictions]
+                if index > self.config['num_warmup']:
+                    total_samples += batch.size()[0]
+            toc = time.time()
+        if self.config['profiling']:
+            print(prof.key_averages().table(sort_by="self_cpu_time_total"))
 
-                # for each output head
-                for prediction, prediction_map, normalization_mask in zip(predictions, prediction_maps,
-                                                                          normalization_masks):
-
-                    # convert to numpy array
-                    prediction = prediction.cpu().numpy()
-
-                    # for each batch sample
-                    for pred, index in zip(prediction, indices):
-                        # save patch index: (C,D,H,W)
-                        if prediction_channel is None:
-                            channel_slice = slice(0, out_channels)
-                        else:
-                            channel_slice = slice(0, 1)
-                        index = (channel_slice,) + index
-
-                        if prediction_channel is not None:
-                            # use only the 'prediction_channel'
-                            logger.info(f"Using channel '{prediction_channel}'...")
-                            pred = np.expand_dims(pred[prediction_channel], axis=0)
-
-                        logger.info(f'Saving predictions for slice:{index}...')
-
-                        # remove halo in order to avoid block artifacts in the output probability maps
-                        u_prediction, u_index = remove_halo(pred, index, volume_shape, patch_halo)
-                        # accumulate probabilities into the output prediction array
-                        prediction_map[u_index] += u_prediction
-                        # count voxel visits for normalization
-                        normalization_mask[u_index] += 1
-
-        # save results to
-        self._save_results(prediction_maps, normalization_masks, output_heads, h5_output_file, self.loader.dataset)
-        # close the output H5 file
-        h5_output_file.close()
+        return toc - tic, total_samples
 
     def _allocate_prediction_maps(self, output_shape, output_heads, output_file):
         # initialize the output prediction arrays
