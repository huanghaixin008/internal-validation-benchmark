diff --git a/pygcn/train.py b/pygcn/train.py
index dca2d47..f2cade7 100644
--- a/pygcn/train.py
+++ b/pygcn/train.py
@@ -29,9 +29,28 @@ parser.add_argument('--hidden', type=int, default=16,
                     help='Number of hidden units.')
 parser.add_argument('--dropout', type=float, default=0.5,
                     help='Dropout rate (1 - keep probability).')
+parser.add_argument('--evaluate', action='store_true', default=False,
+                    help='evaluation only')
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use ipex')
+# parser.add_argument('--jit', action='store_true', default=False,
+#                     help='use ipex')
+parser.add_argument('--precision', default="float32",
+                        help='precision, "float32" or "bfloat16"')
+parser.add_argument('--warmup', type=int, default=10,
+                    help='number of warmup')
+parser.add_argument('--max_iters', type=int, default=1000,
+                    help='max number of iterations to run')
 
 args = parser.parse_args()
 args.cuda = not args.no_cuda and torch.cuda.is_available()
+if args.ipex:
+    import intel_pytorch_extension as ipex
+    print("Running with IPEX...")
+    if args.precision == "bfloat16":
+        # Automatically mix precision
+        ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+        print('Running with bfloat16...')
 
 np.random.seed(args.seed)
 torch.manual_seed(args.seed)
@@ -49,7 +68,14 @@ model = GCN(nfeat=features.shape[1],
 optimizer = optim.Adam(model.parameters(),
                        lr=args.lr, weight_decay=args.weight_decay)
 
-if args.cuda:
+if args.ipex:
+    features = features.to(ipex.DEVICE)
+    adj = adj.to(ipex.DEVICE)
+    labels = labels.to(ipex.DEVICE)
+    idx_train = idx_train.to(ipex.DEVICE)
+    idx_val = idx_val.to(ipex.DEVICE)
+    idx_test = idx_test.to(ipex.DEVICE)
+elif args.cuda:
     model.cuda()
     features = features.cuda()
     adj = adj.cuda()
@@ -85,22 +111,36 @@ def train(epoch):
           'time: {:.4f}s'.format(time.time() - t))
 
 
-def test():
+def test(model):
     model.eval()
-    output = model(features, adj)
+    if args.ipex:
+        model.to(ipex.DEVICE)
+        # if args.jit:
+        #     model = torch.jit.trace(model, (features, adj))
+    
+    for i in range(args.max_iters + args.warmup):
+        if i == args.warmup - 1:
+            start = time.time()
+        output = model(features, adj)
+    total_time = time.time() - start
     loss_test = F.nll_loss(output[idx_test], labels[idx_test])
     acc_test = accuracy(output[idx_test], labels[idx_test])
     print("Test set results:",
           "loss= {:.4f}".format(loss_test.item()),
           "accuracy= {:.4f}".format(acc_test.item()))
+    latency = total_time / args.max_iters * 1000
+    perf = args.max_iters / total_time
+    print('inference latency: %0.3f ms' % latency)
+    print('inference Throughput: %0.3f samples/s' % perf)
 
 
 # Train model
-t_total = time.time()
-for epoch in range(args.epochs):
-    train(epoch)
-print("Optimization Finished!")
-print("Total time elapsed: {:.4f}s".format(time.time() - t_total))
+if not args.evaluate:
+    t_total = time.time()
+    for epoch in range(args.epochs):
+        train(epoch)
+    print("Optimization Finished!")
+    print("Total time elapsed: {:.4f}s".format(time.time() - t_total))
 
 # Testing
-test()
+test(model)
